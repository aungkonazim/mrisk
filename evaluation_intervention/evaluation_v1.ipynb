{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# %matplotlib notebook\n",
    "import pickle\n",
    "import os\n",
    "from scipy.signal import savgol_filter\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "from joblib import Parallel, delayed\n",
    "from copy import deepcopy\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def f1Bias_scorer_CV(probs, y, ret_bias=True):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "    f1,bias = 0.0,.5\n",
    "    min_recall = .7\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0) and recall[i]>min_recall:\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    return f1\n",
    "\n",
    "\n",
    "def cluster_specific_result_collection(result_phenotypes):\n",
    "    for key in result_phenotypes.keys():\n",
    "        average_int = float(result_phenotypes[key][1])/result_phenotypes[key][0]\n",
    "        captured_lapse = float(result_phenotypes[key][2])/result_phenotypes[key][3]\n",
    "        mean_gap = np.mean(result_phenotypes[key][4])\n",
    "        result_phenotypes[key][5].append(average_int)\n",
    "        result_phenotypes[key][6].append(captured_lapse)\n",
    "        result_phenotypes[key][7].append(mean_gap)\n",
    "    return result_phenotypes\n",
    "\n",
    "def smooth_y_pred(y_time_pid,y_pred_pid):\n",
    "    df = pd.DataFrame({'time':y_time_pid,'prob':y_pred_pid}).sort_values('time').reset_index(drop=True)\n",
    "    y_pred_pid = df['prob'].rolling(window=10).mean().fillna(0).values\n",
    "    return y_pred_pid\n",
    "\n",
    "def get_lapse_times(puff_data,par):\n",
    "    return sorted(np.array([np.datetime64(datetime.fromtimestamp(lapse_time/1000)) for lapse_time in puff_data[puff_data['Participant']==int(par)]['Lapse'].values]))[:1]\n",
    "            \n",
    "\n",
    "def get_int_times_count(y_time_pid,y_pred_pid,timescap,bias):\n",
    "    ipd = 0\n",
    "    int_times = []\n",
    "    prev_time = np.min(y_time_pid)\n",
    "    for p,ys in enumerate(y_pred_pid):\n",
    "        timedelta = y_time_pid[p] - prev_time\n",
    "        mins = timedelta.astype('timedelta64[m]').astype(np.int32)\n",
    "        if mins>=timescap and ys>bias:\n",
    "            ipd+=1\n",
    "            int_times.append(y_time_pid[p])\n",
    "            prev_time = y_time_pid[p]\n",
    "    return np.array(int_times), ipd\n",
    "\n",
    "\n",
    "def ascertain_lapse(lapse_times, \n",
    "                    int_times, \n",
    "                    lapse_captured, \n",
    "                    cluster_dict, \n",
    "                    par, \n",
    "                    gap_s, \n",
    "                    result_phenotypes,\n",
    "                    phenotype_specific = True\n",
    "                    ):\n",
    "    \n",
    "    for lapse_time in lapse_times:\n",
    "        int_times_lapse = sorted(np.array([a for a in int_times if a<lapse_time]))\n",
    "        if len(int_times_lapse)==0:\n",
    "            continue\n",
    "        gap_lapse_ = lapse_time- np.max(int_times_lapse)\n",
    "        gap_lapse_ = gap_lapse_.astype('timedelta64[m]').astype(np.int32)\n",
    "        if abs(gap_lapse_)<gap_th:\n",
    "            lapse_captured+=1\n",
    "            if phenotype_specific:\n",
    "                result_phenotypes[cluster_dict[par]][2]+=1\n",
    "        if gap_lapse_<500:\n",
    "            gap_s.append(abs(gap_lapse_))\n",
    "            if phenotype_specific:\n",
    "                result_phenotypes[cluster_dict[par]][4].append(abs(gap_lapse_))\n",
    "    return result_phenotypes, gap_s, lapse_captured\n",
    "\n",
    "def get_final_step(final_intpday,final_recall,final_gap,result_phenotypes,iteration,x,all_data,name='Overall',phenotype_specific=True):\n",
    "    result_final = get_smoothed_result(final_intpday,final_recall,final_gap)\n",
    "    result_final = get_interpolated_data(result_final,x,name=name,iteration=iteration)\n",
    "    all_data.extend(result_final)\n",
    "    if phenotype_specific:\n",
    "        for key in result_phenotypes.keys():\n",
    "            result_this_cluster = get_smoothed_result(result_phenotypes[key][5],\n",
    "                                                            result_phenotypes[key][6],\n",
    "                                                            result_phenotypes[key][7])\n",
    "            result_final_cluster = get_interpolated_data(result_this_cluster,x,name='Cluster {}'.format(key),iteration=iteration)\n",
    "            all_data.extend(result_final_cluster)\n",
    "    return all_data\n",
    "\n",
    "def get_results(iteration):\n",
    "    all_data = []\n",
    "    final_intpday,final_recall,final_gap = [],[],[]\n",
    "    pred_index = indexes_pred\n",
    "    result_phenotypes = deepcopy(result_phenotypes_final)\n",
    "    if episode_presence==0:\n",
    "        min_,max_ = .2,.5\n",
    "    elif episode_presence==2:\n",
    "        min_,max_ = .2,.5\n",
    "    else:\n",
    "        min_,max_ = .2,.5\n",
    "        \n",
    "    for bias in np.arange(.05,.7,.005):\n",
    "        timescap = 60\n",
    "        part_days, total_ipd, lapse_captured, total_lapsers,gap_s = 0,0,0,0,[]\n",
    "        for key in result_phenotypes.keys():\n",
    "            for ii in range(4):\n",
    "                result_phenotypes[key][ii] = 0\n",
    "            result_phenotypes[key][4] = []\n",
    "        for i,par in enumerate(pars):\n",
    "            a = np.where(y_pid==par)[0]\n",
    "            lapse_times = get_lapse_times(puff_data,par)\n",
    "            total_lapsers+=len(lapse_times)\n",
    "            result_phenotypes[cluster_dict[par]][3]+=len(lapse_times)\n",
    "            \n",
    "            y_pred_pid = y_pred[a][:,pred_index]\n",
    "            biases = np.array(bias_dict[par])[pred_index]\n",
    "            for ii in range(y_pred_pid.shape[1]):\n",
    "                y_pred_pid[:,ii] = np.array(y_pred_pid[:,ii]>=np.percentile(y_pred_pid[:,ii],percentile),dtype=int)\n",
    "            y_pred_pid = np.mean(y_pred_pid,axis=1)\n",
    "            \n",
    "            y_test_pid,y_time_pid = y_test[a],y_time[a]\n",
    "            y_time_pid = np.array([y+np.timedelta64(time_gap_delta, 'm') for y in y_time_pid])\n",
    "            y_pred_pid = y_pred_pid[y_time_pid.argsort()]\n",
    "            y_test_pid = y_test_pid[y_time_pid.argsort()]\n",
    "            y_time_pid = np.array(sorted(y_time_pid))\n",
    "            y_day_pid = np.array([str(a)[:10] for a in y_time_pid])\n",
    "            if smoothing:\n",
    "                y_pred_pid = smooth_y_pred(y_time_pid,y_pred_pid)\n",
    "            part_days+=len(np.unique(y_day_pid))\n",
    "            result_phenotypes[cluster_dict[par]][0]+=len(np.unique(y_day_pid))\n",
    "            \n",
    "            int_times, ipd =  get_int_times_count(y_time_pid,y_pred_pid,timescap,bias)    \n",
    "            total_ipd+=ipd\n",
    "            result_phenotypes[cluster_dict[par]][1]+=ipd\n",
    "            \n",
    "            if len(int_times)>0:\n",
    "                result_phenotypes, gap_s, lapse_captured = ascertain_lapse(lapse_times, \n",
    "                                                                            int_times, \n",
    "                                                                            lapse_captured, \n",
    "                                                                            cluster_dict, \n",
    "                                                                            par, \n",
    "                                                                            gap_s, \n",
    "                                                                            result_phenotypes)\n",
    "            \n",
    "        final_day_th = float(total_ipd)/part_days\n",
    "        final_intpday.append(final_day_th)\n",
    "        final_gap.append(np.median(gap_s))\n",
    "        final_recall.append(float(lapse_captured)/total_lapsers)\n",
    "        result_phenotypes = cluster_specific_result_collection(result_phenotypes=result_phenotypes)\n",
    "        \n",
    "    all_data = get_final_step(final_intpday,final_recall,final_gap,result_phenotypes,iteration,x,all_data,name='Overall')\n",
    "    return all_data\n",
    "\n",
    "def get_df(df):\n",
    "    if df.shape[0]==0:\n",
    "        return df\n",
    "    df = df.sort_values('time').reset_index(drop=True)\n",
    "    timescap = df['timescap'].values[0]//2\n",
    "    indexes = np.arange(timescap)\n",
    "    index = np.random.choice(indexes,1)[0]\n",
    "    if index>df.shape[0]-1:\n",
    "        return df\n",
    "    df['pred'].at[index] = 1\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_baseline_result(iteration):\n",
    "    all_data = []\n",
    "    final_intpday = []\n",
    "    final_recall = []\n",
    "    final_gap = []\n",
    "    result_phenotypes = deepcopy(result_phenotypes_final)\n",
    "    for timescap in list(np.arange(10,240,10)):\n",
    "        part_days = 0\n",
    "        total_ipd = 0\n",
    "        lapse_captured = 0\n",
    "        gap_s = []\n",
    "        total_lapsers = 0\n",
    "        for key in result_phenotypes.keys():\n",
    "            result_phenotypes[key][0] = 0\n",
    "            result_phenotypes[key][1] = 0\n",
    "            result_phenotypes[key][2] = 0\n",
    "            result_phenotypes[key][3] = 0\n",
    "            result_phenotypes[key][4] = []\n",
    "        \n",
    "        for i,par in enumerate(pars):\n",
    "            a = np.where(y_pid==par)\n",
    "            if len(a[0])==0:\n",
    "                continue\n",
    "            lapse_times = get_lapse_times(puff_data,par)\n",
    "            total_lapsers+=len(lapse_times)\n",
    "            result_phenotypes[cluster_dict[par]][3]+=len(lapse_times)\n",
    "            \n",
    "            y_time_pid = y_time[a]\n",
    "            y_time_pid = np.array([y+np.timedelta64(time_gap_delta, 'm') for y in y_time_pid])\n",
    "            y_time_pid = np.array(sorted(y_time_pid))\n",
    "            y_day_pid = np.array([str(a)[:10] for a in y_time_pid])\n",
    "            total_minutes = 0\n",
    "            for day in np.unique(y_day_pid):\n",
    "                y_time_pid1 = sorted(y_time_pid[y_day_pid==day])\n",
    "                total_minutes+= (y_time_pid1[-1]-y_time_pid1[0]).astype('timedelta64[m]').astype(np.int32)\n",
    "            ipd = math.ceil(total_minutes/timescap)\n",
    "            udays = len(np.unique(y_day_pid))\n",
    "            part_days+=udays\n",
    "            result_phenotypes[cluster_dict[par]][0]+=udays\n",
    "            \n",
    "            df_pid = pd.DataFrame({'time':y_time_pid,\n",
    "                                    'pred':[0]*len(y_time_pid)})\n",
    "            df_pid['timescap'] = timescap\n",
    "            df_all = df_pid.groupby(pd.Grouper(key='time',freq=str(60*timescap)+'S')).apply(get_df)\n",
    "            int_times = df_all[df_all.pred==1]['time'].values\n",
    "            \n",
    "            if len(int_times)>0:\n",
    "                result_phenotypes, gap_s, lapse_captured = ascertain_lapse(lapse_times, \n",
    "                                                                            int_times, \n",
    "                                                                            lapse_captured, \n",
    "                                                                            cluster_dict, \n",
    "                                                                            par, \n",
    "                                                                            gap_s, \n",
    "                                                                            result_phenotypes)\n",
    "            \n",
    "            total_ipd+=ipd\n",
    "            result_phenotypes[cluster_dict[par]][1]+=ipd\n",
    "            \n",
    "        final_day_th = float(total_ipd)/part_days\n",
    "        final_intpday.append(final_day_th)\n",
    "        final_gap.append(np.mean(gap_s))\n",
    "        final_recall.append(float(lapse_captured)/total_lapsers)\n",
    "        result_phenotypes = cluster_specific_result_collection(result_phenotypes=result_phenotypes)\n",
    "    all_data = get_final_step(final_intpday,final_recall,final_gap,result_phenotypes,iteration,x,all_data,name='Overall')\n",
    "    return all_data\n",
    "\n",
    "def return_important_vars(episode_presence,n_lag=15,n_cluster = 2,obs = 30):\n",
    "    data_file = '../data/episode_encoded_lagged_data_with_episode/episode_encoded_'+'lagged_'+str(n_lag)+'_obs_{}'.format(obs)+'_windows_with_episode_cluster_check_{}'.format(n_cluster)\n",
    "    if episode_presence==0:\n",
    "        # episode_label = 'only'\n",
    "        # filename = './data/output_final_all/result_'+str(episode_label)+'_episode_lag_{}_obs_{}_triplet_loss_cluster___pos_to_neg_none_v111{}{}.p'.format(n_lag,obs,n_cluster,.2)        \n",
    "        filename = '../data/output_final_all/result_only_episode_lag_{}_obs_{}_triplet_loss_cluster_{}_revised_loss_alpha_{}_softmax_{}_loss_{}_rare_{}_positive_{}_percentile_{}.p'.format(n_lag,obs,n_cluster,.2,50,10,1,.2,80)\n",
    "        filename = '../data/output_final_all/result_only_episode_lag_15_obs_30_triplet_loss_cluster_4_revised_loss_alpha_0.3_softmax_10_loss_50_rare_1_positive_0.2_percentile_80.p'\n",
    "        filename = '../data/final_output/model_name_ddhi_episode_False_cross_entropy_weight_10_loss_weight_50_is_triplet_False_ratio_0.2_n_cluster_4_lag_15_obs_30_iters_50.p'\n",
    "        \n",
    "        # filename = './data/output_final_all/distance_lag_15_obs_30_triplet_loss_cluster_4_revised_loss_alpha_0.351_softmax_10_loss_50_rare_1_positive_0.2_percentile_80.p'\n",
    "    elif episode_presence==1:\n",
    "        filename = '../data/output_final_all/distance_lag_15_obs_30_triplet_loss_cluster_4_triplet_loss_alpha_0.2_softmax_1_loss_1_rare_1_positive_0.2_percentile_80.p'\n",
    "        filename = '../data/final_output/model_name_ddhi_episode_False_cross_entropy_weight_10_loss_weight_50_is_triplet_True_ratio_0.2_n_cluster_4_lag_15_obs_30.p'\n",
    "    elif episode_presence==3:\n",
    "        filename = '../data/output_final_all/no_episode_distance_lag_15_obs_30_triplet_loss_cluster_4_triplet_loss_alpha_0.3_softmax_1_loss_1_rare_1_positive_0.2_percentile_80.p' \n",
    "        filename = '../data/final_output/model_name_dres_episode_False_cross_entropy_weight_10_loss_weight_50_is_triplet_True_ratio_0.2_n_cluster_4_lag_15_obs_30.p'\n",
    "    elif episode_presence==5:\n",
    "         filename = '../data/final_output/model_name_ddhi_episode_False_cross_entropy_weight_10_loss_weight_50_is_triplet_False_ratio_0.2_n_cluster_58_lag_15_obs_30_iters_50.p'\n",
    "        # filename = '../data/final_output/model_name_ddhi_episode_False_cross_entropy_weight_10_loss_weight_0_is_triplet_False_ratio_0.2_n_cluster_4_lag_15_obs_30_iters_50.p'\n",
    "    elif episode_presence==6:\n",
    "        filename = '../data/final_output/model_name_ddhi_episode_False_cross_entropy_weight_10_loss_weight_50_is_triplet_False_ratio_0.2_n_cluster_2_lag_15_obs_30_iters_50.p'\n",
    "        # filename = '../data/final_output/model_name_dres_episode_False_cross_entropy_weight_10_loss_weight_0_is_triplet_False_ratio_0.2_n_cluster_4_lag_15_obs_30_iters_50.p'\n",
    "    elif episode_presence == 7:\n",
    "        filename = '../data/final_output/model_name_ddhi_episode_False_cross_entropy_weight_10_loss_weight_50_is_triplet_False_ratio_0.2_n_cluster_6_lag_15_obs_30_iters_50.p'\n",
    "    elif episode_presence == 8:\n",
    "        filename = '../data/final_output/model_name_ddhi_episode_False_cross_entropy_weight_10_loss_weight_50_is_triplet_False_ratio_0.2_n_cluster_8_lag_15_obs_30_iters_50.p'\n",
    "    \n",
    "    else:\n",
    "        # episode_label = 'no'\n",
    "        filename = '../data/output_final_all/result_no_episode_lag_{}_obs_{}_triplet_loss_cluster_{}_revised_loss_alpha_{}_softmax_{}_loss_{}_rare_{}_positive_{}_percentile_{}.p'.format(n_lag,obs,n_cluster,.2,10,50,1,.2,80)      \n",
    "        filename = '../data/output_final_all/result_no_episode_lag_15_obs_30_triplet_loss_cluster_4_revised_loss_alpha_0.3_softmax_10_loss_50_rare_1_positive_0.2_percentile_80.p'  \n",
    "        # filename = './data/output_final_all/no_episode_distance_lag_15_obs_30_triplet_loss_cluster_4_revised_loss_alpha_0.35_softmax_10_loss_50_rare_1_positive_0.2_percentile_80.p'    \n",
    "    data_eval = pickle.load(open(filename,'rb'))\n",
    "    file1 = '../data/mRisk/mRisk_Lapse_postquit.csv'\n",
    "    puff_data = pd.read_csv(file1)\n",
    "    data_feature_label = pickle.load(open(data_file,'rb'))\n",
    "    data_cluster = pickle.load(open(data_file,'rb'))\n",
    "    temp = data_cluster.groupby(['user','cluster_label']).count().index.values\n",
    "    users = np.array([a[0] for a in temp])\n",
    "    labels = np.array([a[1] for a in temp])\n",
    "    cluster_dict = {}\n",
    "    for i,a in enumerate(users):\n",
    "        cluster_dict[a] = labels[i]\n",
    "    y_time,y_pred,y_test,y_pid,bias_dict,val_results = data_eval[0],np.array(data_eval[1]),data_eval[2],data_eval[3],data_eval[4],data_eval[5]\n",
    "    y_time,y_pred,y_test,y_pid,pars = np.array(y_time),np.array(y_pred).reshape(len(y_test),-1),np.array(y_test),np.array(y_pid),np.unique(y_pid)\n",
    "    lapse_captured_dict = {}\n",
    "    total_lapsers = 0\n",
    "    result_phenotypes_final = {}\n",
    "    for label in np.unique(list(cluster_dict.values())):\n",
    "        result_phenotypes_final[label] = [0,0,0,0,[],[],[],[]]\n",
    "    return y_pred, y_test, y_time, y_pid, lapse_captured_dict, result_phenotypes_final, cluster_dict, puff_data, data_feature_label, total_lapsers, pars, bias_dict\n",
    "    \n",
    "\n",
    "\n",
    "smoothing = False            \n",
    "n_cluster = 4          \n",
    "time_gap_delta = 20\n",
    "gap_th = 60\n",
    "# inc = ['3002', '3004', '3005', '3006', '3007', '3009', '3013', '3014', '3015', '3022', '3024', '3025', '3029', '3031', '3033',\n",
    "#  '3036', '3038', '3041', '3045', '3048', '3050', '3053', '3076', '3077', '3079', '3086', '3088', '3091', '3095', '3099',\n",
    "#  '3101', '3102', '3122', '3125', '3126', '3128', '3133', '3135', '3137', '3138', '3139', '3143', '3145', '3148', '3152',\n",
    "#  '3153', '3158', '3160', '3164', '3165', '3166', '3168']\n",
    "n_lag = 15\n",
    "n_iters = 1\n",
    "n_sample = 5\n",
    "x = np.array(list(np.arange(2,8.5,.5)))\n",
    "df_all = []\n",
    "done_ = False\n",
    "data_dict = {}\n",
    "for episode_presence in [0,5,6,7,8]:\n",
    "    y_pred, y_test, y_time, y_pid, lapse_captured_dict, result_phenotypes_final, cluster_dict, puff_data, data_feature_label, total_lapsers, pars, bias_dict = return_important_vars(episode_presence=episode_presence,n_lag=n_lag,n_cluster=n_cluster)\n",
    "    indexes_pred = np.arange(y_pred.shape[1])\n",
    "    for percentile in [80]:\n",
    "        for bias in [.4]:\n",
    "            if episode_presence in [0,1,2,3,5,6,7,8]:\n",
    "                n_iters = 1\n",
    "                # if episode_presence==0:\n",
    "                #     bias = .4\n",
    "                # else:\n",
    "                #     bias = .4\n",
    "                print(bias,percentile)\n",
    "                all_data_v1 = [get_results(iteration) for iteration in np.arange(n_iters)]\n",
    "                # all_data_v1 = [get_results(iteration) for iteration in np.arange(n_iters)]\n",
    "            if episode_presence ==4 and not done_:\n",
    "                done_ = True\n",
    "                n_iters = 2\n",
    "                all_data_v1 = Parallel(n_jobs=-1,verbose=2)(delayed(get_baseline_result)(iteration) for iteration in np.arange(n_iters))\n",
    "                # all_data_v1 = [get_baseline_result(iteration) for iteration in np.arange(n_iters)]\n",
    "            all_data = []\n",
    "            for a in all_data_v1:\n",
    "                all_data.extend(a)\n",
    "            df2 = pd.DataFrame(all_data,columns=['Interventions per day','IHR','TPI','Subtype','Iteration'])\n",
    "            if episode_presence==0:\n",
    "                df2['Encoding'] = ['DDHI - 4']*df2.shape[0]\n",
    "                data_dict['DDHI'] = [y_pred, y_test, y_time, y_pid]\n",
    "                # df2['Encoding'] = ['Proposed Event Stream Encoding plus Continuous Features']*df2.shape[0]\n",
    "            elif episode_presence==1:\n",
    "                df2['Encoding'] = ['DDHI - CE + Triplet loss']*df2.shape[0]\n",
    "                # df2['Encoding'] = ['Continuous & Event Stream Computed Features']*df2.shape[0]\n",
    "            elif episode_presence==2:\n",
    "                df2['Encoding'] = ['DRES - CE + RP loss']*df2.shape[0]\n",
    "                data_dict['DRES'] = [y_pred, y_test, y_time, y_pid]\n",
    "            elif episode_presence==3:\n",
    "                df2['Encoding'] = ['DRES - CE + Triplet loss']*df2.shape[0]\n",
    "            elif episode_presence==5:\n",
    "                df2['Encoding'] = ['DDHI - 58']*df2.shape[0]\n",
    "            elif episode_presence==6:\n",
    "                df2['Encoding'] = ['DDHI - 2']*df2.shape[0]\n",
    "            elif episode_presence==7:\n",
    "                df2['Encoding'] = ['DDHI - 6']*df2.shape[0]\n",
    "            elif episode_presence==8:\n",
    "                df2['Encoding'] = ['DDHI - 8']*df2.shape[0]            \n",
    "                # df2['Encoding'] = ['Proposed Event Stream Encoding Only']*df2.shape[0]\n",
    "            elif episode_presence==4:\n",
    "                df2['Encoding'] = ['Random Baseline']*df2.shape[0]\n",
    "            df2['Encoding1'] = episode_presence\n",
    "            df2['Bias'] = bias\n",
    "            df2['percentile'] = percentile\n",
    "            df_all.append(df2)\n",
    "df=pd.concat(df_all)\n",
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(np.unique(y_pid))\n",
    "# import seaborn as sns\n",
    "# df_final = df[df.Subtype.isin(['Overall'])]\n",
    "# for bias in df_final.Bias.unique():\n",
    "#     for percentile in df_final.percentile.unique():\n",
    "#         df_this = df_final[(df_final.Bias==bias) & (df_final.percentile==percentile)]\n",
    "#         print(bias,percentile)\n",
    "#         plt.figure(figsize=(10,8))\n",
    "#         sns.barplot(x='Interventions per day',y='IHR',hue='Encoding',data=df_this)\n",
    "#         plt.grid()\n",
    "#         plt.show()\n",
    "# df_final['Interventions per day']+=1\n",
    "df_final = df1\n",
    "df_result = df_final[df_final.Subtype.isin(['Overall'])]\n",
    "tpi_table1 = pd.pivot_table(df_result,columns='Interventions per day',values='IHR',index='Encoding',aggfunc='mean')\n",
    "tpi_table1.to_csv('ihr1.csv')\n",
    "tpi_table = pd.pivot_table(df_result,columns='Interventions per day',values='TPI',index='Encoding',aggfunc='mean')\n",
    "tpi_table.to_csv('tpi1.csv')\n",
    "tpi_table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_result.groupby(['Encoding','Subtype']).mean()\n",
    "tpi_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# df_final = df[df.Subtype.isin(['Overall'])]\n",
    "# for bias in df_final.Bias.unique():\n",
    "#     for percentile in df_final.percentile.unique():\n",
    "#         df_this = df_final[(df_final.Bias==bias) & (df_final.percentile==percentile)]\n",
    "#         print(bias,percentile)\n",
    "#         plt.figure(figsize=(10,8))\n",
    "#         sns.barplot(x='Interventions per day',y='IHR',hue='Encoding',data=df_this)\n",
    "#         plt.grid()\n",
    "#         plt.show()\n",
    "# df_final['Interventions per day']+=1\n",
    "df_final = df1\n",
    "df_result = df_final[df_final.Subtype.isin(['Overall'])]\n",
    "tpi_table1 = pd.pivot_table(df_result,columns='Interventions per day',values='IHR',index='Encoding',aggfunc='mean')\n",
    "# tpi_table1.to_csv('ihr.csv')\n",
    "tpi_table = pd.pivot_table(df_result,columns='Interventions per day',values='TPI',index='Encoding',aggfunc='mean')\n",
    "# tpi_table.to_csv('tpi.csv')\n",
    "tpi_table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_final = df1\n",
    "perf = 'IHR'\n",
    "# df_final\n",
    "import seaborn as sns\n",
    "import seaborn as sns\n",
    "colors = ['tab:blue',\"orange\", \"green\",'tab:red']\n",
    "# Set your custom color palette\n",
    "sns.set_palette(sns.color_palette(colors))\n",
    "\n",
    "# df_final = df\n",
    "# df_final = df_final[df_final['Interventions per day'].isin(np.arange(3,10))]\n",
    "plt.rcParams.update({'font.size':25})\n",
    "fig,ax = plt.subplots(1,1,figsize=(15,10),sharey=True)\n",
    "ax = [ax]\n",
    "sns.barplot(x='Interventions per day',y=perf,hue='Encoding',data=df_final[df_final.Subtype.isin(['Overall'])],ci=None,ax=ax[0])\n",
    "plt.legend([],[])\n",
    "# ax[0].grid()\n",
    "# ax[0].set_ylabel('Intervention Hit Rate(IHR)',color='r')\n",
    "# ax[0].legend([],[], frameon=False)\n",
    "ax[0].tick_params(axis='y', colors='k')\n",
    "ax[0].tick_params(axis='x', rotation=90)\n",
    "# ax[0].set_ylim([0,.99])\n",
    "# ax1=ax.twinx()\n",
    "# sns.barplot(y='Interventions per day',x='TPI',orient='h',hue='Encoding',data=df_final[df_final.Subtype.isin(['Overall'])],ci=None,ax=ax[1])\n",
    "if perf=='IHR':\n",
    "    ax[0].set_ylabel('Intervention Hit Rate (IHR)',color='k')\n",
    "    # ax[0].set_ylim([0,.99])\n",
    "else:\n",
    "    ax[0].set_ylabel('Temporal Precision of Interventions (TPI),\\n minutes',color='g')\n",
    "# ax[1].tick_params(axis='y', colors='g')\n",
    "# plt.xticks(rotation=90)\n",
    "# # ax[1].grid()\n",
    "# ax[1].legend(loc=(-1.3,1.01),ncol=2)\n",
    "# # plt.tight_layout()\n",
    "# # plt.savefig('./data/images/deep_model.png')\n",
    "# # plt.tight_layout()\n",
    "# ax[0].grid()\n",
    "plt.legend(loc=(.5,0), numpoints=1, ncol=1,frameon=True,fancybox=True,framealpha=.9)\n",
    "# plt.title('Model Performance -'+str(perf))\n",
    "# ax[1].grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig('./data/images/'+str(perf)+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "colors = [\"orange\", \"green\",'tab:red']\n",
    "# Set your custom color palette\n",
    "sns.set_palette(sns.color_palette(colors))\n",
    "# And then, from here onwards, it's exactly\n",
    "plt.rcParams.update({'font.size':25})\n",
    "df2 = df1[df1['Interventions per day'].isin(np.arange(3,8))]\n",
    "n_clusters = np.unique(list(cluster_dict.values())).shape[0]\n",
    "fig,ax = plt.subplots(2,n_clusters-2,figsize=(15,10),sharey=False)\n",
    "ax = np.array(ax).reshape(-1)\n",
    "for i in range(len(ax)):\n",
    "    sns.barplot(x='Interventions per day',y='IHR',hue='Encoding',data=df2[df2.Subtype.isin(['Cluster {}'.format(i)])],ci=None,ax=ax[i])\n",
    "    # if i!=0:\n",
    "    ax[i].legend([],[], frameon=False)\n",
    "    # else:\n",
    "    #     ax[i].legend(ncol=1,loc=(-.9,.3),frameon=False)\n",
    "    ax[i].set_xlabel('Int. per day')\n",
    "    ax[i].set_title('Cluster {}'.format(i))\n",
    "    # ax[i].set_xticks([0,1,2,3])\n",
    "    # ax[i].set_xticklabels([3,4,5,6])\n",
    "    \n",
    "    # ax[i].set_ylim([-1,8])\n",
    "# plt.legend(ncol=3)\n",
    "# plt.xlabel('Interventions per day')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./data/images/cluster_wise_perf.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_final = df1\n",
    "\n",
    "perf = 'IHR'\n",
    "# df_final\n",
    "import seaborn as sns\n",
    "import seaborn as sns\n",
    "colors = ['tab:blue',\"orange\", \"green\",'tab:red']\n",
    "# Set your custom color palette\n",
    "sns.set_palette(sns.color_palette(colors))\n",
    "\n",
    "# df_final = df\n",
    "# df_final = df_final[df_final['Interventions per day'].isin(np.arange(3,10))]\n",
    "plt.rcParams.update({'font.size':25})\n",
    "fig,ax = plt.subplots(1,1,figsize=(15,10),sharey=True)\n",
    "ax = [ax]\n",
    "sns.barplot(x='Interventions per day',y=perf,hue='Encoding',data=df_final[df_final.Subtype.isin(['Overall'])],ci=None,ax=ax[0])\n",
    "plt.legend([],[])\n",
    "# ax[0].grid()\n",
    "# ax[0].set_ylabel('Intervention Hit Rate(IHR)',color='r')\n",
    "# ax[0].legend([],[], frameon=False)\n",
    "ax[0].tick_params(axis='y', colors='k')\n",
    "ax[0].tick_params(axis='x', rotation=90)\n",
    "# ax[0].set_ylim([0,.99])\n",
    "# ax1=ax.twinx()\n",
    "# sns.barplot(y='Interventions per day',x='TPI',orient='h',hue='Encoding',data=df_final[df_final.Subtype.isin(['Overall'])],ci=None,ax=ax[1])\n",
    "if perf=='IHR':\n",
    "    ax[0].set_ylabel('Intervention Hit Rate (IHR)',color='k')\n",
    "    # ax[0].set_ylim([0,.99])\n",
    "else:\n",
    "    ax[0].set_ylabel('Temporal Precision of Interventions (TPI),\\n minutes',color='g')\n",
    "# ax[1].tick_params(axis='y', colors='g')\n",
    "# plt.xticks(rotation=90)\n",
    "# # ax[1].grid()\n",
    "# ax[1].legend(loc=(-1.3,1.01),ncol=2)\n",
    "# # plt.tight_layout()\n",
    "# # plt.savefig('./data/images/deep_model.png')\n",
    "# # plt.tight_layout()\n",
    "# ax[0].grid()\n",
    "plt.legend(loc=(.5,0), numpoints=1, ncol=1,frameon=True,fancybox=True,framealpha=.9)\n",
    "# plt.title('Model Performance -'+str(perf))\n",
    "# ax[1].grid()\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./data/images/'+str(perf)+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df_final[df_final.Subtype.isin(['Overall'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.groupby('Encoding').mean()[['IHR','TPI']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpi_table = pd.pivot_table(df_result,columns='Interventions per day',values='IHR',index='Encoding',aggfunc='median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpi_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpi_table.to_csv('tpi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(df,open('./data/temp_v2.p','wb'))\n",
    "# df[df.Subtype.isin(['Overall'])].groupby(['Bias','percentile','Subtype','Encoding'],as_index=False).mean()[['IHR']+['Bias','percentile','Subtype','Encoding']]\n",
    "df_final = df[df.Subtype.isin(['Overall'])]\n",
    "for bias in df_final.Bias.unique():\n",
    "    for percentile in df_final.percentile.unique():\n",
    "        df_this = df_final[(df_final.Bias==bias) & (df_final.percentile==percentile)]\n",
    "        print(bias,percentile)\n",
    "        plt.figure(figsize=(10,8))\n",
    "        sns.lineplot(x='Interventions per day',y='IHR',hue='Encoding',data=df_this)\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "# df_this[df_this.Encoding1==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('../data/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "extra_dfs = []\n",
    "for k in np.arange(n_cluster):\n",
    "    df_cluster = df[df.Subtype.isin(['Cluster {}'.format(k)])]\n",
    "    all_encodings = []\n",
    "    for encoding1 in df_cluster.Encoding1.unique():\n",
    "        if encoding1!=3:\n",
    "            all_encodings.append(df_cluster[df_cluster.Encoding1.isin([encoding1])])\n",
    "    results = []\n",
    "    for dff in all_encodings:\n",
    "        results.append(np.mean(dff['IHR'].values))\n",
    "    results = np.array(results)\n",
    "    df_this_cluster = all_encodings[results.argmax()]\n",
    "    df_this_cluster['Subtype'] = ['Overall']*df_this_cluster.shape[0]\n",
    "    df_this_cluster['Encoding'] = ['Phenotype Specific Model']*df_this_cluster.shape[0]\n",
    "    df_this_cluster['Encoding1'] = 10\n",
    "    extra_dfs.append(df_this_cluster)\n",
    "\n",
    "\n",
    "# extra_df = pd.concat(extra_dfs)\n",
    "# extra_df = extra_df.groupby(['Subtype','Encoding','Encoding1','Interventions per day'],as_index=False).mean()\n",
    "# extra_dfs.append(df)\n",
    "\n",
    "extra_dfs.append(df)\n",
    "df_final = pd.concat(extra_dfs)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_final = df1\n",
    "\n",
    "perf = 'IHR'\n",
    "# df_final\n",
    "import seaborn as sns\n",
    "# df_final = df\n",
    "# df_final = df_final[df_final['Interventions per day'].isin(np.arange(3,10))]\n",
    "plt.rcParams.update({'font.size':25})\n",
    "fig,ax = plt.subplots(1,1,figsize=(15,10),sharey=True)\n",
    "ax = [ax]\n",
    "sns.barplot(x='Interventions per day',y=perf,hue='Encoding',data=df_final[df_final.Subtype.isin(['Overall'])],ci=None,ax=ax[0])\n",
    "plt.legend(ncol=2)\n",
    "# ax[0].grid()\n",
    "# ax[0].set_ylabel('Intervention Hit Rate(IHR)',color='r')\n",
    "# ax[0].legend([],[], frameon=False)\n",
    "ax[0].tick_params(axis='y', colors='red')\n",
    "ax[0].tick_params(axis='x', rotation=90)\n",
    "# ax[0].set_ylim([0,.99])\n",
    "# ax1=ax.twinx()\n",
    "# sns.barplot(y='Interventions per day',x='TPI',orient='h',hue='Encoding',data=df_final[df_final.Subtype.isin(['Overall'])],ci=None,ax=ax[1])\n",
    "if perf=='IHR':\n",
    "    ax[0].set_ylabel('Intervention Hit Rate (IHR)',color='r')\n",
    "    ax[0].set_ylim([0,.99])\n",
    "else:\n",
    "    ax[0].set_ylabel('Temporal Precision of Interventions (TPI),\\n minutes',color='g')\n",
    "# ax[1].tick_params(axis='y', colors='g')\n",
    "# plt.xticks(rotation=90)\n",
    "# # ax[1].grid()\n",
    "# ax[1].legend(loc=(-1.3,1.01),ncol=2)\n",
    "# # plt.tight_layout()\n",
    "# # plt.savefig('./data/images/deep_model.png')\n",
    "# # plt.tight_layout()\n",
    "ax[0].grid()\n",
    "\n",
    "plt.title('Model Performance -'+str(perf))\n",
    "# ax[1].grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig('./data/images/'+str(perf)+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final.Subtype.unique()\n",
    "n_clusters = np.unique(list(cluster_dict.values())).shape[0]\n",
    "n_cluster,n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(df_final[df_final.Subtype.isin(['Overall'])],values='IHR',columns='Interventions per day',index='Encoding',aggfunc='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[df_final.Subtype.isin(['Overall'])].groupby('Encoding',as_index=False).mean()[['Encoding','IHR','TPI']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "# df  = pickle.load(open('./data/saved_result/ihr_tpi_dataframe.p','rb'))\n",
    "df  = df_final\n",
    "plt.rcParams.update({'font.size':10})\n",
    "fig,ax = plt.subplots(1,2,figsize=(10,5))\n",
    "# ax = np.array(ax).reshape((2,2))\n",
    "sns.barplot(y='Interventions per day',x='IHR',hue='Encoding',data=df[df.Subtype.isin(['Cluster 0'])],ci=None,ax=ax[0])\n",
    "# ax[0,0].set_ylim([0,1])\n",
    "# ax[0,0].set_xlabel('')\n",
    "ax[0].set_title('Cluster 0',color='g')\n",
    "# ax[0,0].legend([],[])\n",
    "sns.barplot(y='Interventions per day',x='IHR',hue='Encoding',data=df[df.Subtype.isin(['Cluster 1'])],ci=None,ax=ax[1])\n",
    "# ax[0,1].set_ylim([0,1])\n",
    "# ax[0,1].set_xlabel('')\n",
    "ax[1].set_title('Cluster 1',color='g')\n",
    "# ax[0,1].legend([],[])\n",
    "# sns.pointplot(x='Interventions per day',y='IHR',hue='Encoding',data=df[df.Subtype.isin(['Cluster 2'])],ci=None,ax=ax[1,0])\n",
    "# ax[1,0].set_ylim([0,1])\n",
    "# ax[1,0].set_title('Cluster 2',color='g')\n",
    "# ax[1,0].legend([],[])\n",
    "# sns.pointplot(x='Interventions per day',y='IHR',hue='Encoding',data=df[df.Subtype.isin(['Cluster 3'])],ci=None,ax=ax[1,1])\n",
    "# ax[1,1].set_ylim([0,1])\n",
    "# ax[1,1].set_title('Cluster 3',color='g')\n",
    "# ax[1,1].legend(loc=(-1.5,2.3),ncol=2)\n",
    "# ax[0,0].tick_params(axis='x', rotation=90)\n",
    "# ax[0,1].tick_params(axis='x', rotation=90)\n",
    "# ax[1,0].tick_params(axis='x', rotation=90)\n",
    "# ax[1,1].tick_params(axis='x', rotation=90)\n",
    "\n",
    "# plt.show()\n",
    "# plt.figure(figsize=(20,10))\n",
    "# sns.pointplot(x='Interventions per day',y='TPI',hue='Subtype',data=df[df.Encoding.isin(['Proposed Episode Encoding']) & ~df.Subtype.isin(['Overall'])],ci='sd',ax=ax[1])\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('./data/images/cluster_performance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = './data/episode_encoded_lagged_data/episode_encoded_'+'lagged_{}_windows_standardized_phenotype_with_episode_cluster_check'.format(n_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(puff_data.groupby('Participant').count()[\"Lapse\"].values)\n",
    "plt.xlabel('Number of Lapses Per Participant')\n",
    "plt.ylabel(\"No. of Participants\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "puff_data.Participant.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pickle.load(open(data_file,'rb'))\n",
    "cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = tf.unique_with_counts([1,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_subtype = pd.pivot_table(df[df['Interventions per day']<=5],columns='Encoding',index='Subtype',values='IHR',aggfunc='mean').reset_index(level=0)\n",
    "# phenotype_specific = {}\n",
    "# for subtype in df_subtype.Subtype.unique():\n",
    "#     if subtype=='Overall':\n",
    "#         continue\n",
    "#     cluster = int(subtype.split(\" \")[-1])\n",
    "#     ddhi = df_subtype[df_subtype.Subtype==subtype]['DDHI'].values[0]\n",
    "#     dres = df_subtype[df_subtype.Subtype==subtype]['DRES'].values[0]\n",
    "#     if ddhi>=dres:\n",
    "#         phenotype_specific[cluster] = [data_dict['DDHI'],'DDHI']\n",
    "#     else:\n",
    "#         print('Some')\n",
    "#         phenotype_specific[cluster] = [data_dict['DRES'],'DRES']\n",
    "# n_iters = 1\n",
    "# all_data_v1 = [get_max_results(iteration) for iteration in np.arange(n_iters)]\n",
    "# all_data = []\n",
    "# for a in all_data_v1:\n",
    "#     all_data.extend(a)\n",
    "# df2 = pd.DataFrame(all_data,columns=['Interventions per day','IHR','TPI','Subtype','Iteration'])\n",
    "# df2['Encoding'] = ['Phenotype Specific Model']*df2.shape[0]\n",
    "# df2['Bias'] = bias\n",
    "# df2['percentile'] = percentile\n",
    "# df = pd.concat([df2,df])\n",
    "# df1 = df.copy()\n",
    "\n",
    "def get_max_results(iteration):\n",
    "    all_data = []\n",
    "    final_intpday,final_recall,final_gap = [],[],[]\n",
    "    result_phenotypes = {}\n",
    "    for bias in np.arange(.2,.52,.01):\n",
    "        timescap = 60\n",
    "        part_days, total_ipd, lapse_captured, total_lapsers,gap_s = 0,0,0,0,[]\n",
    "        for i,par in enumerate(pars):\n",
    "            model_name = phenotype_specific[cluster_dict[par]][1]\n",
    "            # if model_name=='DRES' and bias<.23:\n",
    "            #     continue\n",
    "            y_pred, y_test, y_time, y_pid = phenotype_specific[cluster_dict[par]][0]\n",
    "            a = np.where(y_pid==par)[0]\n",
    "            lapse_times = get_lapse_times(puff_data,par)\n",
    "            total_lapsers+=len(lapse_times)\n",
    "            y_pred_pid = y_pred[a]\n",
    "            for ii in range(y_pred_pid.shape[1]):\n",
    "                y_pred_pid[:,ii] = np.array(y_pred_pid[:,ii]>=np.percentile(y_pred_pid[:,ii],percentile),dtype=int)\n",
    "            y_pred_pid = np.mean(y_pred_pid,axis=1) \n",
    "            y_test_pid,y_time_pid = y_test[a],y_time[a]\n",
    "            y_time_pid = np.array([y+np.timedelta64(time_gap_delta, 'm') for y in y_time_pid])\n",
    "            y_pred_pid = y_pred_pid[y_time_pid.argsort()]\n",
    "            y_test_pid = y_test_pid[y_time_pid.argsort()]\n",
    "            y_time_pid = np.array(sorted(y_time_pid))\n",
    "            y_day_pid = np.array([str(a)[:10] for a in y_time_pid])\n",
    "            if smoothing:\n",
    "                y_pred_pid = smooth_y_pred(y_time_pid,y_pred_pid)\n",
    "            part_days+=len(np.unique(y_day_pid))\n",
    "            int_times, ipd =  get_int_times_count(y_time_pid,y_pred_pid,timescap,bias)    \n",
    "            total_ipd+=ipd\n",
    "            if len(int_times)>0:\n",
    "                result_phenotypes, gap_s, lapse_captured = ascertain_lapse(lapse_times, \n",
    "                                                                            int_times, \n",
    "                                                                            lapse_captured, \n",
    "                                                                            cluster_dict, \n",
    "                                                                            par, \n",
    "                                                                            gap_s, \n",
    "                                                                            result_phenotypes,\n",
    "                                                                            phenotype_specific=False)\n",
    "            \n",
    "        final_day_th = float(total_ipd)/part_days\n",
    "        final_intpday.append(final_day_th)\n",
    "        final_gap.append(np.median(gap_s))\n",
    "        final_recall.append(float(lapse_captured)/total_lapsers)\n",
    "    all_data = get_final_step(final_intpday,final_recall,final_gap,result_phenotypes,iteration,x,all_data,name='Overall',phenotype_specific=False)\n",
    "    return all_data"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51b244ab9aca612e739a0539ae1af887c58db9e180d786deb0ab1761def69c1f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('test1': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
