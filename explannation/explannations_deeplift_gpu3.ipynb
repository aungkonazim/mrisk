{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azim/miniconda3/envs/test1/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:38: UserWarning: You are currently using a nightly version of TensorFlow (2.5.0-dev20210310). \n",
      "TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not. \n",
      "If you encounter a bug, do not file an issue on GitHub.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow_addons.losses import metric_learning\n",
    "import tensorflow_probability as tfp\n",
    "from keras import backend as K\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_features = []\n",
    "feature_name = ['p90','p95','p80','p5','p10','p20','range_90_to_80','range_20_to_10','range_90_to_10',\n",
    "                'range_80_to_20','median','iqr','skew']\n",
    "name = 'current_window_stress'\n",
    "all_features += [name+'_'+a for a in feature_name]\n",
    "# print(len(set(all_features)),len(all_features))\n",
    "name = 'current_window_stress_diff'\n",
    "all_features += [name+'_'+a for a in feature_name]\n",
    "name = 'daily_stress'\n",
    "all_features += [name+'_'+a for a in feature_name]\n",
    "all_features += ['percentage_of_stress', 'maximum_duration_of_current_stress_episode','average_duration_of_current_stress_episode',\n",
    "               'maximum_density_of_current_stress_episode', 'average_density_of_current_stress_episode', 'average_deviation_to_daily_mean_current',\n",
    "               'no_stress_till_now', 'time_since_last_stress', 'duration_of_last_stress_episode', 'average_duration_of_before_stress_episode',\n",
    "               'density_of_last_stress_episode', 'average_density_of_before_stress_episode', 'deviation_to_daily_mean_of_last_stress_episode',\n",
    "               'percentage_of_stress_before']\n",
    "all_features+= ['percentage_of_active_', 'maximum_duration_of_current_activity_episode', 'average_duration_of_current_window_episode',\n",
    "                 'no_activity_till_now', 'time_since_last_activity', 'duration_of_last_activity_episode', 'average_duration_of_before_activity_episode',\n",
    "                 'percentage_of_active_before']\n",
    "all_features += ['time_since_last_visit_smoking_spot','duration_of_stay_in_smoking_spot','fraction_of_minutes_spent_in_smoking_spots_out_of_observed']\n",
    "all_features += ['Gender','Age']\n",
    "all_features += ['is_smoking','spread','distance_to_nearest_spot',\n",
    "                'time_spent_in_transition','time_spent_in_smoking_spot']\n",
    "all_features += ['Time of day = {}'.format(a) for a in np.arange(24)]\n",
    "feature_names = all_features\n",
    "\n",
    "\n",
    "def get_X_y_groups(n_lag=10):\n",
    "    data = pickle.load(open(filepath_file.format(n_lag),'rb'))\n",
    "\n",
    "    X_feature = np.concatenate(data.feature_final.values)\n",
    "    X_static =  np.concatenate(data.static_features.values)\n",
    "\n",
    "    X_stress_episode = np.concatenate(data.stress_episode.values)\n",
    "    X_quit_episode = np.concatenate(data.quit_episode.values)\n",
    "    X_activity_episode = np.concatenate(data.activity_episode.values)\n",
    "    X_smoking_episode = np.concatenate(data.smoking_episode.values)\n",
    "\n",
    "    y_time = data['time'].values\n",
    "    y = data['label'].values\n",
    "    groups = data['user'].values\n",
    "    \n",
    "    return X_feature,X_static,X_stress_episode,X_quit_episode,X_activity_episode,X_smoking_episode,y_time,y,groups\n",
    "\n",
    "def get_train_test_indexes(groups,n_groups_split = 10,n_val_groups = 10):\n",
    "    groups_unique = np.unique(groups)\n",
    "    groups_split = np.array_split(groups_unique,n_groups_split)\n",
    "    indexes = []\n",
    "    for this_groups in groups_split:\n",
    "        train_groups = np.array([a for a in groups_unique if a not in this_groups])\n",
    "        val_groups = np.random.choice(train_groups,n_val_groups)\n",
    "        train_groups = np.array([a for a in groups_unique if a not in list(this_groups)+list(val_groups)])\n",
    "        test_groups = this_groups\n",
    "        train_index,test_index = np.array([i for i,a in enumerate(groups) \n",
    "                                           if a in train_groups]),np.array([i for i,a in enumerate(groups) \n",
    "                                                                               if a in test_groups])\n",
    "        val_index = np.array([i for i,a in enumerate(groups) \n",
    "                                           if a in val_groups])\n",
    "        indexes.append([train_index,test_index,val_index])\n",
    "    return indexes\n",
    "\n",
    "\n",
    "def get_train_test_indexes(groups,n_groups_split = 10,n_val_groups = 10):\n",
    "    groups_unique = np.unique(groups)\n",
    "    groups_split = np.array_split(groups_unique,n_groups_split)\n",
    "    indexes = []\n",
    "    for this_groups in groups_split:\n",
    "        train_groups = np.array([a for a in groups_unique if a not in this_groups])\n",
    "        val_groups = np.random.choice(train_groups,n_val_groups)\n",
    "        train_groups = np.array([a for a in groups_unique if a not in list(this_groups)+list(val_groups)])\n",
    "        test_groups = this_groups\n",
    "        train_index,test_index = np.array([i for i,a in enumerate(groups) \n",
    "                                           if a in train_groups]),np.array([i for i,a in enumerate(groups) \n",
    "                                                                               if a in test_groups])\n",
    "        val_index = np.array([i for i,a in enumerate(groups) \n",
    "                                           if a in val_groups])\n",
    "        indexes.append([train_index,test_index,val_index])\n",
    "    return indexes\n",
    "\n",
    "class CenterLoss(tf.keras.losses.Loss):\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha=0.9,\n",
    "        update_centers=True,\n",
    "        p = 80,\n",
    "        name=\"center_loss\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.update_centers = update_centers\n",
    "        self.p = p\n",
    "        \n",
    "    def consistency_loss(self,labels,precise_embeddings):\n",
    "        \n",
    "        lshape = tf.shape(labels)\n",
    "        labels = tf.reshape(labels, [lshape[0], 1])\n",
    "        mask_for_equal = tf.math.equal(tf.cast(labels,tf.bool),tf.transpose(tf.cast(labels,tf.bool)))\n",
    "        only_positive = tf.repeat(tf.transpose(tf.reshape(tf.cast(labels,tf.bool),[lshape[0],1])),lshape[0],axis=0)\n",
    "        mask_only_positive = tf.math.multiply(tf.cast(mask_for_equal,tf.float32),tf.cast(only_positive,tf.float32))\n",
    "        \n",
    "        pdist_matrix = metric_learning.pairwise_distance(\n",
    "                    precise_embeddings, squared=False\n",
    "                )\n",
    "        positive_only_dist = tf.boolean_mask(pdist_matrix, tf.cast(mask_only_positive,tf.bool))\n",
    "        no_of_positives = tf.cast(tf.reduce_sum(labels),tf.int32)\n",
    "        positive_only_dist = tf.reshape(positive_only_dist,[no_of_positives,no_of_positives])\n",
    "        positive_only_dist = tf.reduce_mean(positive_only_dist,axis=1)\n",
    "        \n",
    "        distance_95 = tf.reshape(positive_only_dist,(-1,))\n",
    "        # if max_vs_percentile:\n",
    "        #     return tf.reduce_max(distance_95),pdist_matrix\n",
    "        # return tfp.stats.percentile(distance_95,self.p),pdist_matrix\n",
    "        samples1 = tf.cast(no_of_positives, tf.float32) #batch size\n",
    "        p = (100. - self.p)/100.  #100% will return 0 elements, 0% will return all elements\n",
    "        samples = tf.cast(tf.math.floor(p * samples1), tf.int32)\n",
    "        values, indices = tf.math.top_k(distance_95, samples)\n",
    "        positive_dist_95th = tf.reduce_min(values)\n",
    "        return positive_dist_95th, pdist_matrix\n",
    "    \n",
    "    \n",
    "    def compute_rare_loss_v1(self,labels,pdist_matrix,positive_dist_95th):\n",
    "        lshape = tf.shape(labels)\n",
    "        labels = tf.reshape(labels, [lshape[0], 1])\n",
    "        mask_for_equal = tf.math.equal(tf.cast(labels,tf.bool),tf.transpose(tf.cast(labels,tf.bool)))\n",
    "        only_negative = tf.cast(tf.repeat(tf.transpose(tf.reshape(tf.cast(1-labels,tf.bool),[lshape[0],1])),lshape[0],axis=0),tf.int32)\n",
    "        mask_negative_to_positive = tf.math.logical_not(tf.cast(tf.math.add(tf.cast(mask_for_equal,tf.int32),only_negative),tf.bool))\n",
    "        no_of_positives = tf.cast(tf.reduce_sum(labels),tf.int32)\n",
    "        no_of_negatives = lshape[0] - no_of_positives\n",
    "        negative_to_positive_distance = tf.reshape(tf.boolean_mask(pdist_matrix,mask_negative_to_positive),[no_of_negatives,no_of_positives])\n",
    "        average_neg_to_pos_distance = tf.reduce_mean(negative_to_positive_distance,axis=1)\n",
    "        less_distance_mask = tf.reduce_sum(tf.where(average_neg_to_pos_distance<=positive_dist_95th,1,0))\n",
    "        return tf.cast(less_distance_mask,tf.float32)/(tf.cast(no_of_negatives,tf.float32)+K.epsilon())\n",
    "    def call(self, sparse_labels, prelogits):\n",
    "        sparse_labels = tf.reshape(sparse_labels, (-1,))\n",
    "        distance_95th, pdist_matrix = self.consistency_loss(sparse_labels,prelogits)\n",
    "        rare_loss = self.compute_rare_loss_v1(sparse_labels,pdist_matrix,distance_95th)\n",
    "        return tf.square(rare_loss-ratio_of_pos_to_neg) + distance_95th*.2\n",
    "\n",
    "def get_model():\n",
    "    n_t,n_f = train_feature.shape[1],train_feature.shape[2]\n",
    "    print(n_t,n_f)\n",
    "    x_input = Input(shape=(n_t,n_f))\n",
    "    x_feature = Conv1D(100,1,activation='linear')(x_input)\n",
    "    x_feature = Conv1D(100,1,activation='tanh')(x_feature)\n",
    "    # x_feature = Dropout(.2)(x_feature)\n",
    "    x_feature = LSTM(10,activation='tanh',return_sequences=False)(x_feature)\n",
    "    x_feature = Dropout(.2)(x_feature)\n",
    "    x_feature = Flatten()(x_feature)\n",
    "    x_feature = Dense(10,activation='relu')(x_feature)\n",
    "    merged_all = x_feature\n",
    "    merged1 = Dense(10,activation='relu',name='normalize2')(merged_all)\n",
    "    merged1 = Lambda(lambda x: K.l2_normalize(x,axis=1),name='normalize3')(merged1)\n",
    "    merged = Dense(5,activation='relu',name='normalize')(merged1)\n",
    "    output = Dense(1,activation='sigmoid',name='softmax')(merged)\n",
    "    model = tf.keras.models.Model(inputs=[x_input], outputs=[output,merged1])\n",
    "    myloss1 = CenterLoss()\n",
    "    model.compile(\n",
    "        loss={'softmax':tf.losses.BinaryCrossentropy(),'normalize3':tfa.losses.TripletSemiHardLoss()},\n",
    "        loss_weights = {'softmax':softmax_weight,'normalize3':revised_loss_weight},\n",
    "        metrics={'softmax':['acc']},\n",
    "        optimizer='adam'\n",
    "        )\n",
    "    return model\n",
    "    \n",
    "def f1Bias_scorer_CV(probs, y, ret_bias=True):\n",
    "    precision, recall, thresholds = precision_recall_curve(y, probs)\n",
    "\n",
    "    f1,bias = 0.0,.5\n",
    "    min_recall = .8\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0) and thresholds[i]>0.2:\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "def set_gpu():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "    # Restrict TensorFlow to only use the first GPU\n",
    "        try:\n",
    "            tf.config.experimental.set_visible_devices(gpus[gpu_id], 'GPU')\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "        except RuntimeError as e:\n",
    "            # Visible devices must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "            \n",
    "            \n",
    "import shap\n",
    "\n",
    "# we use the first 100 training examples as our background dataset to integrate over\n",
    "def get_random_samples(X_feature_train,y_train):\n",
    "    train_positive = X_feature_train[y_train==1]\n",
    "    train_negative = X_feature_train[y_train==0]\n",
    "    train_negative = train_negative[np.random.choice(np.arange(train_negative.shape[0]),min(train_positive.shape[0]*2,train_negative.shape[0]))]\n",
    "    X_feature_train = np.concatenate([train_positive,train_negative])\n",
    "    return X_feature_train\n",
    "\n",
    "n_lag = 15\n",
    "obs = 30\n",
    "n_cluster = 4\n",
    "n_iters = 7\n",
    "softmax_weight = 10\n",
    "revised_loss_weight = 50\n",
    "batch_size = 200\n",
    "n_epochs = 1000\n",
    "n_step = 20\n",
    "ratio_of_pos_to_neg = .35\n",
    "gpu_id = 2\n",
    "set_gpu()\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "filepath_file = './data/episode_encoded_lagged_data_without_episode/episode_encoded_'+'lagged_'+str(n_lag)+'_obs_{}'.format(obs)+'_windows_with_episode_cluster_check_{}'.format(n_cluster)\n",
    "X_feature,X_static,X_stress_episode,X_quit_episode,X_activity_episode,X_smoking_episode,y_time,y,groups = get_X_y_groups(n_lag)\n",
    "X_feature = np.nan_to_num(X_feature)\n",
    "indexes = get_train_test_indexes(groups,n_groups_split = 20,n_val_groups=10)\n",
    "explannation_data = []\n",
    "for kk,yyyy in enumerate(indexes):\n",
    "    train_index,test_index,val_index = yyyy\n",
    "    X_feature_train,X_feature_test = X_feature[train_index],X_feature[test_index]\n",
    "    X_static_train,X_static_test = X_static[train_index],X_static[test_index]\n",
    "    X_stress_episode_train,X_stress_episode_test = X_stress_episode[train_index], X_stress_episode[test_index]\n",
    "    X_quit_episode_train,X_quit_episode_test = X_quit_episode[train_index], X_quit_episode[test_index]\n",
    "    X_activity_episode_train,X_activity_episode_test = X_activity_episode[train_index], X_activity_episode[test_index]\n",
    "    X_smoking_episode_train,X_smoking_episode_test = X_smoking_episode[train_index], X_smoking_episode[test_index]\n",
    "    y_train,y_test,groups_train,groups_test,time_train,time_test = y[train_index],y[test_index],groups[train_index],groups[test_index],y_time[train_index],y_time[test_index]\n",
    "    \n",
    "    X_feature_val,X_static_val,X_stress_episode_val,X_quit_episode_val,\\\n",
    "    X_activity_episode_val,X_smoking_episode_val,y_val,groups_val,time_val = X_feature[val_index],X_static[val_index],X_stress_episode[val_index],X_quit_episode[val_index],\\\n",
    "                                                                            X_activity_episode[val_index],X_smoking_episode[val_index],y[val_index],groups[val_index],y_time[val_index]\n",
    "    \n",
    "    X_feature_train,val_feature,\\\n",
    "    X_static_train,val_static,\\\n",
    "    X_stress_episode_train,val_stress,\\\n",
    "    X_smoking_episode_train,val_smoking,\\\n",
    "    X_quit_episode_train,val_quit,\\\n",
    "    X_activity_episode_train,val_activity, \\\n",
    "    y_train,val_y = train_test_split(\n",
    "                                    X_feature_train,\n",
    "                                    X_static_train,\n",
    "                                    X_stress_episode_train,\n",
    "                                    X_smoking_episode_train,\n",
    "                                    X_quit_episode_train,\n",
    "                                    X_activity_episode_train,\n",
    "                                    y_train,\n",
    "                                    test_size=.05,\n",
    "                                    stratify=y_train\n",
    "                                    )\n",
    "    val_feature = np.concatenate([val_feature,X_feature_val])\n",
    "    val_static = np.concatenate([val_static,X_static_val])\n",
    "    val_stress = np.concatenate([val_stress,X_stress_episode_val])\n",
    "    val_activity = np.concatenate([val_activity,X_activity_episode_val])\n",
    "    val_smoking = np.concatenate([val_smoking,X_smoking_episode_val])\n",
    "    val_quit = np.concatenate([val_quit,X_quit_episode_val])\n",
    "    val_y = np.array(list(val_y)+list(y_val))\n",
    "    \n",
    "    positive_train_index = np.where(y_train==1)[0]\n",
    "    negative_train_index = np.where(y_train==0)[0]\n",
    "    len_positive = len(positive_train_index)\n",
    "    all_models = []\n",
    "    for iter in range(n_iters):\n",
    "        indexes_sampled = np.array(list(positive_train_index)+list(np.random.choice(negative_train_index,len_positive,replace=False)))\n",
    "        train_feature = np.float64(X_feature_train[indexes_sampled])\n",
    "        train_static = X_static_train[indexes_sampled]\n",
    "        train_stress = X_stress_episode_train[indexes_sampled]\n",
    "        train_quit = X_quit_episode_train[indexes_sampled]\n",
    "        train_activity = X_activity_episode_train[indexes_sampled]\n",
    "        train_smoking = X_smoking_episode_train[indexes_sampled]\n",
    "        train_y = np.float64(y_train[indexes_sampled])\n",
    "        model = get_model()\n",
    "        # model.summary()\n",
    "        validation_results = [0]\n",
    "        counter_ = 0\n",
    "        best_filepath = None\n",
    "        for iiii in np.arange(0,n_epochs,n_step):\n",
    "            filepath = './models/lag_'+str(n_lag)+'_iter_'+str(n_iters)+'_temp_with_episode_revised_loss_2_80_.2{}{}{}{}.hdf5'.format(softmax_weight,revised_loss_weight,ratio_of_pos_to_neg,iiii)\n",
    "            checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min',save_weights_only=False)\n",
    "            es = EarlyStopping(monitor='loss', mode='min', verbose=1,patience=60)\n",
    "            callbacks_list = [es,checkpoint]\n",
    "            model.fit(train_feature,[train_y,train_y],\n",
    "                      epochs=n_step, batch_size=batch_size,\n",
    "                      verbose=0,callbacks=callbacks_list,shuffle=True)\n",
    "            if os.path.isfile(filepath):\n",
    "                model.load_weights(filepath)\n",
    "            val_y_pred = model.predict(val_feature)[0]\n",
    "            val_f1_now = f1Bias_scorer_CV(val_y_pred,val_y)[0]\n",
    "            # print('<->'*20)\n",
    "            from sklearn.metrics import roc_auc_score\n",
    "            print('Validation Result now',val_f1_now,'AUC',roc_auc_score(val_y,val_y_pred),'step',iiii,'stagnant',counter_)\n",
    "            # print('<->'*20)\n",
    "            if val_f1_now>=max(validation_results):\n",
    "                best_filepath = filepath\n",
    "                counter_ = 0\n",
    "            else:\n",
    "                counter_ += n_step\n",
    "                if counter_>200:\n",
    "                    break\n",
    "            validation_results.append(val_f1_now)\n",
    "        if os.path.isfile(best_filepath):\n",
    "            print()\n",
    "            print(best_filepath[-15:])\n",
    "            model.load_weights(best_filepath)\n",
    "        model_this_iter = tf.keras.models.Model(model.input,model.get_layer('softmax').output)\n",
    "        all_models.append(model_this_iter)\n",
    "        print(len(all_models))\n",
    "    model_input = all_models[0].input\n",
    "    model_outputs = [model(model_input) for model in all_models]\n",
    "    ensemble_output = tf.keras.layers.Average()(model_outputs)\n",
    "    ensemble_model = tf.keras.Model(inputs=model_input, outputs=ensemble_output)\n",
    "    train_data = get_random_samples(X_feature_train,y_train)\n",
    "    test_data = get_random_samples(X_feature_test,y_test)\n",
    "    explainer = shap.DeepExplainer(ensemble_model,train_data)\n",
    "    shap_values = explainer.shap_values(test_data,check_additivity=False)\n",
    "    plt.figure()\n",
    "    shap.summary_plot([shap_values[0].mean(axis=1)], test_data.mean(axis=1),feature_names=feature_names)\n",
    "    plt.show()\n",
    "    explannation_data.append([shap_values,test_data])\n",
    "    pickle.dump(explannation_data,open('./data/explannation_data_v1.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = all_models[0].input\n",
    "model_outputs = [model(model_input) for model in all_models]\n",
    "ensemble_output = tf.keras.layers.Average()(model_outputs)\n",
    "ensemble_model = tf.keras.Model(inputs=model_input, outputs=ensemble_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model.predict(X_feature_test).shape,X_feature_test.shape,y_test.shape,X_feature_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# we use the first 100 training examples as our background dataset to integrate over\n",
    "def get_random_samples(X_feature_train,y_train):\n",
    "    train_positive = X_feature_train[y_train==1]\n",
    "    train_negative = X_feature_train[y_train==0]\n",
    "    train_negative = train_negative[np.random.choice(np.arange(train_negative.shape[0]),train_positive.shape[0])]\n",
    "    X_feature_train = np.concatenate([train_positive,train_negative])\n",
    "    return X_feature_train\n",
    "train_data = get_random_samples(X_feature_train,y_train)\n",
    "test_data = get_random_samples(X_feature_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.DeepExplainer(ensemble_model,X_feature_train)\n",
    "shap_values = explainer.shap_values(test_data,check_additivity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap_values[0].sum(axis=1).\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?shap.summary_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0].sum(axis=1)[0], val_feature.mean(axis=1)[0],feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?shap.summary_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_feature_test[:10].mean(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51b244ab9aca612e739a0539ae1af887c58db9e180d786deb0ab1761def69c1f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('test1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
