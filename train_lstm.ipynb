{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime,timedelta\n",
    "from scipy.stats import iqr,skew,kurtosis,mode\n",
    "from joblib import Parallel,delayed\n",
    "import zipfile\n",
    "import shutil\n",
    "from sklearn.feature_selection import SelectKBest,f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score,r2_score,classification_report\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,RandomForestRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV,StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "seed = 100\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from sklearn.model_selection import train_test_split,LeavePGroupsOut\n",
    "from tensorflow.keras.layers import Conv1D,BatchNormalization,Dropout,InputLayer,MaxPooling1D,Flatten,Dense,Input,Activation,GRU,Bidirectional,LSTM\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow_addons as tfa\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y_groups(use_standardization,n_lag):\n",
    "    feature_names = ['Time of Day','Gender', 'Age', 'current_window_stress_p90', 'current_window_stress_p95', 'current_window_stress_p80',\n",
    "     'current_window_stress_p5', 'current_window_stress_p10', 'current_window_stress_p20', 'current_window_stress_range_90_to_80',\n",
    "     'current_window_stress_range_20_to_10', 'current_window_stress_range_90_to_10', 'current_window_stress_range_80_to_20', 'current_window_stress_median',\n",
    "     'current_window_stress_iqr', 'current_window_stress_skew', 'current_window_stress_diff_p90', 'current_window_stress_diff_p95',\n",
    "     'current_window_stress_diff_p80', 'current_window_stress_diff_p5', 'current_window_stress_diff_p10', 'current_window_stress_diff_p20',\n",
    "     'current_window_stress_diff_range_90_to_80', 'current_window_stress_diff_range_20_to_10', 'current_window_stress_diff_range_90_to_10', 'current_window_stress_diff_range_80_to_20',\n",
    "     'current_window_stress_diff_median', 'current_window_stress_diff_iqr', 'current_window_stress_diff_skew', 'time_since_last_visit_smoking_spot',\n",
    "     'duration_of_stay_in_smoking_spot', 'fraction_of_minutes_spent_in_smoking_spots_out_of_observed', 'daily_stress_p90', 'daily_stress_p95',\n",
    "     'daily_stress_p80', 'daily_stress_p5', 'daily_stress_p10', 'daily_stress_p20', 'daily_stress_range_90_to_80', 'daily_stress_range_20_to_10',\n",
    "     'daily_stress_range_90_to_10', 'daily_stress_range_80_to_20', 'daily_stress_median', 'daily_stress_iqr', 'daily_stress_skew', 'percentage_of_stress', 'maximum_duration_of_current_stress_episode',\n",
    "     'average_duration_of_current_stress_episode', 'maximum_density_of_current_stress_episode', 'average_density_of_current_stress_episode', 'average_deviation_to_daily_mean_current',\n",
    "     'no_stress_till_now', 'time_since_last_stress', 'duration_of_last_stress_episode', 'average_duration_of_before_stress_episode', 'density_of_last_stress_episode',\n",
    "     'average_density_of_before_stress_episode', 'deviation_to_daily_mean_of_last_stress_episode', 'percentage_of_stress_before', 'percentage_of_active_',\n",
    "     'maximum_duration_of_current_activity_episode', 'average_duration_of_current_window_episode', 'no_activity_till_now', 'time_since_last_activity',\n",
    "     'duration_of_last_activity_episode', 'average_duration_of_before_activity_episode', 'percentage_of_active_before', 'is_smoking', 'spread', 'distance_to_nearest_spot', 'time_spent_in_transition',\n",
    "     'time_spent_in_smoking_spot']\n",
    "    if use_standardization:\n",
    "        data  = pickle.load(open('./data/lagged_data/obs_30_prediction_60/lagged_'+str(n_lag)+'_windows.p','rb'))\n",
    "    else:\n",
    "        data  = pickle.load(open('./data/lagged_data/obs_30_prediction_60/lagged_'+str(n_lag)+'_windows'+'_standardized_new'+'.p','rb'))\n",
    "    n_t = data.iloc[0]['features'].shape[1]\n",
    "    n_f = data.iloc[0]['features'].shape[2]\n",
    "    y_time = data['time'].values\n",
    "    X = np.concatenate(list(data['features']))\n",
    "    n = X.shape[0]\n",
    "    X1 = X\n",
    "    y = data['label'].values\n",
    "    y = np.int64(np.array(y))\n",
    "    y[y>0] = 1\n",
    "    y[y<1] = -1\n",
    "    groups = data['user'].values\n",
    "    X_time = data['Time of Day'].values.reshape(-1,1)\n",
    "    X_gender = data.Gender.values.reshape(-1,1)\n",
    "    time_oh = preprocessing.OneHotEncoder().fit(X_time)\n",
    "    gender_oh = preprocessing.OneHotEncoder().fit(X_gender)\n",
    "    X_time = time_oh.transform(X_time).todense()\n",
    "    X_gender = gender_oh.transform(X_gender).todense()\n",
    "    if use_standardization:\n",
    "        X1_all,y_all,groups_all,y_time_all,X_gender_all,X_time_all = [],[],[],[],[],[]\n",
    "        for g in np.unique(groups):\n",
    "            X1_all.append(X1[groups==g])\n",
    "            y_all.extend(list(y[groups==g]))\n",
    "            groups_all.extend(list(groups[groups==g]))\n",
    "            y_time_all.extend(list(y_time[groups==g]))\n",
    "            X_gender_all.append(preprocessing.StandardScaler().fit_transform(X_gender[groups==g]))\n",
    "            X_time_all.append(preprocessing.StandardScaler().fit_transform(X_time[groups==g]))\n",
    "        X1,y,groups,y_time,X_gender,X_time =  np.concatenate(X1_all),np.array(y_all),np.array(groups_all), \\\n",
    "        np.array(y_time_all),np.concatenate(X_gender_all),np.concatenate(X_time_all)\n",
    "\n",
    "    X_time = np.concatenate([np.expand_dims(X_time,axis=1)]*n_t,axis=1)\n",
    "    X_gender = np.concatenate([np.expand_dims(X_gender,axis=1)]*n_t,axis=1)\n",
    "    feature_names_temp = feature_names[2:]\n",
    "    time_feature_columns = [feature_names[0]+'=='+a for a in time_oh.categories_[0]]\n",
    "    gender_feature_columns = [feature_names[1]+'=='+a for a in gender_oh.categories_[0]]\n",
    "    feature_names_final = time_feature_columns+gender_feature_columns+feature_names_temp\n",
    "    X = np.concatenate([X1,X_time,X_gender],axis=2)\n",
    "    return X,y,groups,feature_names_final,y_time\n",
    "\n",
    "def get_train_test_indexes(groups,n_groups_split = 10):\n",
    "    groups_unique = np.unique(groups)\n",
    "    groups_split = np.array_split(groups_unique,n_groups_split)\n",
    "    indexes = []\n",
    "    for this_groups in groups_split:\n",
    "        this_groups = set(this_groups)\n",
    "        train_index,test_index = np.array([i for i,a in enumerate(groups) \n",
    "                                           if a not in this_groups]),np.array([i for i,a in enumerate(groups) \n",
    "                                                                               if a in this_groups])\n",
    "        indexes.append([train_index,test_index])\n",
    "    return indexes\n",
    "\n",
    "\n",
    "\n",
    "def get_model(X_train):\n",
    "    n_t,n_f = X_train.shape[1],X_train.shape[2]\n",
    "#     a = tf.placeholder(dtype=tf.float32, shape=(None, n_t,n_f))\n",
    "    model = Sequential()\n",
    "#     model.add(Input(shape=(n_t,n_f)))\n",
    "#     model.add(Conv1D(128,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "#     model.add(Conv1D(64,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "#     model.add(LSTM(80,activation='tanh',return_sequences=True,input_shape=(n_t,n_f)))\n",
    "#     model.add(LSTM(20,activation='relu',return_sequences=True))\n",
    "    model.add(LSTM(20,activation='tanh',return_sequences=False,input_shape=(n_t,n_f)))\n",
    "    model.add(Dropout(.3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10,activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss=tfa.losses.focal_loss.SigmoidFocalCrossEntropy(),metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_standardization = True\n",
    "n_lag = 10\n",
    "n_groups = 20\n",
    "X,y,groups,feature_names,y_time = get_X_y_groups(use_standardization,n_lag)\n",
    "y[y<0] = 0\n",
    "y = np.float32(y)\n",
    "indexes = get_train_test_indexes(groups,n_groups_split = 10)\n",
    "final_y_time = []\n",
    "final_probs = []\n",
    "final_y = []\n",
    "final_groups = []\n",
    "for train_index,test_index in indexes:\n",
    "    X_train,X_test,y_train,y_test,groups_train,groups_test,time_train,time_test = X[train_index],X[test_index], \\\n",
    "    y[train_index],y[test_index],groups[train_index],groups[test_index],y_time[train_index],y_time[test_index]\n",
    "    positive_train,negative_train,pos_train_y,neg_train_y = X_train[y_train==1],X_train[y_train==0],y_train[y_train==1],y_train[y_train==0]\n",
    "    len_positive = len(positive_train)\n",
    "    index = np.arange(len(negative_train))\n",
    "    n_iters = 10\n",
    "    test_preds = []\n",
    "    for i,n_iter in enumerate(range(n_iters)):\n",
    "        indexes_sampled = np.random.choice(index,len_positive*3)\n",
    "        negative_train_sampled = negative_train[indexes_sampled]\n",
    "        train_x = np.concatenate([positive_train,negative_train_sampled])\n",
    "        train_y = np.array([1]*len(positive_train) + [0]*len(negative_train_sampled))\n",
    "        model = get_model(train_x)\n",
    "        filepath = './models/temp_model.hdf5'\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='min',save_weights_only=False)\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0,patience=40)\n",
    "        callbacks_list = [es,checkpoint]\n",
    "        train_x,val_x,train_y,val_y = train_test_split(train_x,train_y,test_size=.1,stratify=train_y)\n",
    "        train_y = tf.cast(train_y, tf.float32)\n",
    "        train_x = tf.cast(train_x, tf.float32)\n",
    "        val_y = tf.cast(val_y, tf.float32)\n",
    "        val_x = tf.cast(val_x, tf.float32)\n",
    "        history = model.fit(train_x,train_y,validation_data=(val_x,val_y), epochs=200, batch_size=30,\n",
    "                            verbose=0,callbacks=callbacks_list,shuffle=True)\n",
    "        model.load_weights(filepath)\n",
    "        test_preds.append(model.predict(X_test))\n",
    "    y_test_pred = list(np.concatenate(test_preds,axis=1).mean(axis=1))\n",
    "    final_y_time.extend(list(time_test))\n",
    "    final_probs.extend(list(y_test_pred))\n",
    "    final_y.extend(list(y_test))\n",
    "    final_groups.extend(list(groups_test))\n",
    "    print(len(np.unique(final_groups)))\n",
    "final_y_time,final_probs,final_y,final_groups = np.array(final_y_time),np.array(final_probs),np.array(final_y),np.array(final_groups)\n",
    "pickle.dump([final_y_time,final_probs,final_y,final_groups],open('./data/output/all_features_single_LSTM.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8489,), (8489,), (8489,), (8489,))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(final_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "filepath = './models/'+'-'.join([str(n_lag),str(n_groups)])+'-'+str(uuid.uuid4())+'.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min',save_weights_only=False)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=0,patience=40)\n",
    "callbacks_list = [es,checkpoint]\n",
    "train_x,val_x,train_y,val_y = train_test_split(X_train,y_train,test_size=.2,stratify=y_train)\n",
    "history = model.fit(train_x,train_y,validation_data=(val_x,val_y), epochs=100, batch_size=100,verbose=1,callbacks=callbacks_list,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc,roc_auc_score,f1_score\n",
    "roc_auc_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
