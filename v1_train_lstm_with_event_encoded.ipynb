{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azim/miniconda3/envs/test1/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:38: UserWarning: You are currently using a nightly version of TensorFlow (2.5.0-dev20210310). \n",
      "TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not. \n",
      "If you encounter a bug, do not file an issue on GitHub.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime,timedelta\n",
    "from scipy.stats import iqr,skew,kurtosis,mode\n",
    "from joblib import Parallel,delayed\n",
    "import zipfile\n",
    "import shutil\n",
    "from sklearn.feature_selection import SelectKBest,f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score,r2_score,classification_report\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,RandomForestRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV,StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "seed = 100\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from sklearn.model_selection import train_test_split,LeavePGroupsOut\n",
    "from keras.backend import expand_dims, repeat_elements\n",
    "from keras.layers import Lambda\n",
    "from tensorflow.keras.layers import Conv1D,BatchNormalization,Dropout,InputLayer,MaxPooling1D,Flatten,RepeatVector,Dense,Input,Activation,GRU,Bidirectional,LSTM\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow_addons as tfa\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y_groups(n_lag=10):\n",
    "    data = pickle.load(open('./data/episode_encoded_lagged_data/episode_encoded_lagged_'+str(n_lag)+'_windows_standardized.p','rb'))\n",
    "\n",
    "    X_feature = np.concatenate(data.feature_final.values)\n",
    "    X_static =  np.concatenate(data.static_features.values)\n",
    "\n",
    "    X_stress_episode = np.concatenate(data.stress_episode.values)\n",
    "    X_quit_episode = np.concatenate(data.quit_episode.values)\n",
    "    X_activity_episode = np.concatenate(data.activity_episode.values)\n",
    "    X_smoking_episode = np.concatenate(data.smoking_episode.values)\n",
    "\n",
    "    y_time = data['time'].values\n",
    "    y = data['label'].values\n",
    "    groups = data['user'].values\n",
    "    \n",
    "    return X_feature,X_static,X_stress_episode,X_quit_episode,X_activity_episode,X_smoking_episode,y_time,y,groups\n",
    "\n",
    "\n",
    "def get_train_test_indexes(groups,n_groups_split = 10,n_val_groups = 5):\n",
    "    groups_unique = np.unique(groups)\n",
    "    groups_split = np.array_split(groups_unique,n_groups_split)\n",
    "    indexes = []\n",
    "    for this_groups in groups_split:\n",
    "        train_groups = np.array([a for a in groups_unique if a not in this_groups])\n",
    "        val_groups = np.random.choice(train_groups,n_val_groups)\n",
    "        train_groups = np.array([a for a in groups_unique if a not in list(this_groups)+list(val_groups)])\n",
    "        test_groups = this_groups\n",
    "        train_index,test_index = np.array([i for i,a in enumerate(groups) \n",
    "                                           if a in train_groups]),np.array([i for i,a in enumerate(groups) \n",
    "                                                                               if a in test_groups])\n",
    "        val_index = np.array([i for i,a in enumerate(groups) \n",
    "                                           if a in val_groups])\n",
    "        indexes.append([train_index,test_index,val_index])\n",
    "    return indexes\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "def f1Bias_scorer_CV(probs, y, ret_bias=True):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "\n",
    "    f1,bias = 0.0,.5\n",
    "    min_recall = .7\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0) and recall[i]>min_recall:\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    n_t,n_f = train_feature.shape[1],train_feature.shape[2]\n",
    "#     print(n_t,n_f)\n",
    "    x_input = Input(shape=(n_t,n_f))\n",
    "    x_feature = Conv1D(100,1,activation='linear')(x_input)\n",
    "    x_feature = Conv1D(100,1,activation='tanh')(x_feature)\n",
    "    x_feature = Dropout(.2)(x_feature)\n",
    "    x_feature = LSTM(20,activation='tanh',return_sequences=False)(x_feature)\n",
    "    x_feature = Dropout(.3)(x_feature)\n",
    "    x_feature = Flatten()(x_feature)\n",
    "    x_feature = Dense(10,activation='relu')(x_feature)\n",
    "    # x_final = Dense(1,activation='sigmoid')(x_feature)\n",
    "\n",
    "    n_sf = train_static.shape[1]\n",
    "    x_input_static = Input(shape=(n_sf))\n",
    "    x_static = Dense(100,activation='relu')(x_input_static)\n",
    "    x_static = Dense(10,activation='relu')(x_static)\n",
    "    n_timesteps = train_stress.shape[-2]\n",
    "    n_episodes_stress,n_episodes_quit,n_episodes_activity,n_episodes_smoking = train_stress.shape[1],train_quit.shape[1],train_activity.shape[1],train_smoking.shape[1]\n",
    "    x_alpha_stress = Dense(1,activation='sigmoid')(x_static)\n",
    "    x_alpha_stress = RepeatVector(n_timesteps)(x_alpha_stress)\n",
    "    x_alpha_stress = Lambda(lambda x: repeat_elements(expand_dims(x, axis=1), n_episodes_stress, 1))(x_alpha_stress)\n",
    "    x_alpha_quit = Dense(1,activation='sigmoid')(x_static)\n",
    "    x_alpha_quit = RepeatVector(n_timesteps)(x_alpha_quit)\n",
    "    x_alpha_quit = Lambda(lambda x: repeat_elements(expand_dims(x, axis=1), n_episodes_quit, 1))(x_alpha_quit)\n",
    "    x_alpha_activity = Dense(1,activation='sigmoid')(x_static)\n",
    "    x_alpha_activity = RepeatVector(n_timesteps)(x_alpha_activity)\n",
    "    x_alpha_activity = Lambda(lambda x: repeat_elements(expand_dims(x, axis=1), n_episodes_activity, 1))(x_alpha_activity)\n",
    "    x_alpha_smoking = Dense(1,activation='sigmoid')(x_static)\n",
    "    x_alpha_smoking = RepeatVector(n_timesteps)(x_alpha_smoking)\n",
    "    x_alpha_smoking = Lambda(lambda x: repeat_elements(expand_dims(x, axis=1), n_episodes_smoking, 1))(x_alpha_smoking)\n",
    "\n",
    "    n_dim = 3\n",
    "    x_stress = Input(shape=(n_episodes_stress,n_timesteps,n_dim))\n",
    "    stress_alpha_time = tf.math.multiply(x_alpha_stress[:,:,:,0]*-1,x_stress[:,:,:,0])\n",
    "    stress_alpha_time_exp = tf.math.exp(stress_alpha_time)\n",
    "\n",
    "    x_stress_amplitude = x_stress[:,:,:,1]\n",
    "    stress_amplitude_coeff = Dense(1,activation='sigmoid')(x_feature)\n",
    "    stress_amplitude_coeff = Lambda(lambda x: expand_dims(x, axis=2))(stress_amplitude_coeff)\n",
    "    print(stress_amplitude_coeff.shape,'coeff',x_stress_amplitude.shape,'amplitude')\n",
    "    x_stress_amplitude = tf.math.multiply(x_stress_amplitude,stress_amplitude_coeff)\n",
    "#     print(x_stress_amplitude.shape,'final')\n",
    "    \n",
    "    x_stress_duration = x_stress[:,:,:,2]\n",
    "    stress_duration_coeff = Dense(1,activation='sigmoid')(x_feature)\n",
    "    stress_duration_coeff = Lambda(lambda x: expand_dims(x, axis=2))(stress_duration_coeff)\n",
    "    x_stress_duration = tf.math.multiply(x_stress_duration,stress_duration_coeff)\n",
    "    \n",
    "    x_stress_all = tf.math.add(x_stress_amplitude,x_stress_duration)\n",
    "    \n",
    "#     print(x_stress_all.shape,stress_alpha_time_exp.shape)\n",
    "    stress_alpha_time_exp_amplitude = tf.math.multiply(stress_alpha_time_exp,x_stress_all)\n",
    "    \n",
    "#     print(stress_alpha_time_exp_amplitude.shape)\n",
    "    stress_final = tf.math.reduce_sum(stress_alpha_time_exp_amplitude,axis=1)\n",
    "#     print(stress_final.shape)\n",
    "    stress_final = Lambda(lambda x: expand_dims(x, axis=2))(stress_final)\n",
    "#     print(stress_final.shape)\n",
    "    stress_final = LSTM(10,activation='tanh',return_sequences=True)(stress_final)\n",
    "    \n",
    "    \n",
    "    \n",
    "    x_quit = Input(shape=(n_episodes_quit,n_timesteps,n_dim))\n",
    "#     quit_alpha_time = tf.math.multiply(x_alpha_quit[:,:,:,0]*-1,x_quit[:,:,:,0])\n",
    "#     quit_alpha_time_exp = tf.math.exp(quit_alpha_time)\n",
    "#     quit_alpha_time_exp_amplitude = tf.math.multiply(quit_alpha_time_exp,x_quit[:,:,:,1])\n",
    "#     quit_final = tf.math.reduce_sum(quit_alpha_time_exp_amplitude,axis=1)\n",
    "#     quit_final = Lambda(lambda x: expand_dims(x, axis=2))(quit_final)\n",
    "#     quit_final = LSTM(5,activation='tanh',return_sequences=True)(quit_final)\n",
    "\n",
    "    \n",
    "    \n",
    "    x_smoking = Input(shape=(n_episodes_smoking,n_timesteps,n_dim))\n",
    "    smoking_alpha_time = tf.math.multiply(x_alpha_smoking[:,:,:,0]*-1,x_smoking[:,:,:,0])\n",
    "    smoking_alpha_time_exp = tf.math.exp(smoking_alpha_time)\n",
    "    \n",
    "    x_smoking_amplitude = x_smoking[:,:,:,1]\n",
    "    smoking_amplitude_coeff = Dense(1,activation='sigmoid')(x_feature)\n",
    "    smoking_amplitude_coeff = Lambda(lambda x: expand_dims(x, axis=2))(smoking_amplitude_coeff)\n",
    "    x_smoking_amplitude = tf.math.multiply(x_smoking_amplitude,smoking_amplitude_coeff)\n",
    "    \n",
    "    x_smoking_duration = x_smoking[:,:,:,2]\n",
    "    smoking_duration_coeff = Dense(1,activation='sigmoid')(x_feature)\n",
    "    smoking_duration_coeff = Lambda(lambda x: expand_dims(x, axis=2))(smoking_duration_coeff)\n",
    "    x_smoking_duration = tf.math.multiply(x_smoking_duration,smoking_duration_coeff)\n",
    "    \n",
    "    x_smoking_all = tf.math.add(x_smoking_amplitude,x_smoking_duration)\n",
    "    smoking_alpha_time_exp_amplitude = tf.math.multiply(smoking_alpha_time_exp,x_smoking_all)\n",
    "    smoking_final = tf.math.reduce_sum(smoking_alpha_time_exp_amplitude,axis=1)\n",
    "    smoking_final = Lambda(lambda x: expand_dims(x, axis=2))(smoking_final)\n",
    "    smoking_final = LSTM(10,activation='tanh',return_sequences=True)(smoking_final)\n",
    "\n",
    "    \n",
    "    \n",
    "    x_activity = Input(shape=(n_episodes_activity,n_timesteps,n_dim))\n",
    "    activity_alpha_time = tf.math.multiply(x_alpha_activity[:,:,:,0]*-1,x_activity[:,:,:,0])\n",
    "    activity_alpha_time_exp = tf.math.exp(activity_alpha_time)\n",
    "    \n",
    "    x_activity_amplitude = x_activity[:,:,:,1]\n",
    "    activity_amplitude_coeff = Dense(1,activation='sigmoid')(x_feature)\n",
    "    activity_amplitude_coeff = Lambda(lambda x: expand_dims(x, axis=2))(activity_amplitude_coeff)\n",
    "    x_activity_amplitude = tf.math.multiply(x_activity_amplitude,activity_amplitude_coeff)\n",
    "    \n",
    "    x_activity_duration = x_activity[:,:,:,2]\n",
    "    activity_duration_coeff = Dense(1,activation='sigmoid')(x_feature)\n",
    "    activity_duration_coeff = Lambda(lambda x: expand_dims(x, axis=2))(activity_duration_coeff)\n",
    "#     print(activity_duration_coeff)\n",
    "    x_activity_duration = tf.math.multiply(x_activity_duration,activity_duration_coeff)\n",
    "    \n",
    "    x_activity_all = tf.math.add(x_activity_amplitude,x_activity_duration)\n",
    "    activity_alpha_time_exp_amplitude = tf.math.multiply(activity_alpha_time_exp,x_activity_all)\n",
    "    activity_final = tf.math.reduce_sum(activity_alpha_time_exp_amplitude,axis=1)\n",
    "    activity_final = Lambda(lambda x: expand_dims(x, axis=2))(activity_final)\n",
    "    activity_final = LSTM(10,activation='tanh',return_sequences=True)(activity_final)\n",
    "    \n",
    "    \n",
    "    x_episode = tf.concat([activity_final,stress_final,smoking_final],2)\n",
    "    x_episode = Conv1D(100,10,activation='relu')(x_episode)\n",
    "    x_episode = Conv1D(100,10,activation='tanh')(x_episode)\n",
    "    x_episode = LSTM(100,activation='tanh',return_sequences=True)(x_episode)\n",
    "    x_episode = LSTM(20,activation='tanh',return_sequences=False)(x_episode)\n",
    "    x_episode = Dropout(.3)(x_episode)\n",
    "    x_episode = Flatten()(x_episode)\n",
    "    x_episode = Dense(10,activation='relu')(x_episode)\n",
    "\n",
    "    merged = tf.concat([x_feature,x_episode],1)\n",
    "\n",
    "    merged = Dense(10,activation='relu')(merged)\n",
    "    output = Dense(1,activation='sigmoid')(merged)\n",
    "    # output = Activation('softmax',name='softmax')(output)\n",
    "    model = Model(inputs=[x_input,x_input_static,x_stress,x_activity,x_smoking,x_quit], outputs=[output])\n",
    "    model.compile(loss=myloss(.1),metrics=['acc',tfa.metrics.F1Score(num_classes=2,average='micro',threshold=.5)])\n",
    "    return model\n",
    "# tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "def myloss(alpha=.2):\n",
    "    \n",
    "    def custom_loss(true,pred):\n",
    "        m = tf.keras.losses.BinaryCrossentropy()\n",
    "        return f1_loss(true,pred)+m(true,pred)*alpha\n",
    "    \n",
    "    \n",
    "    def f1_weighted(true, pred): #shapes (batch, 4)\n",
    "\n",
    "        #for metrics include these two lines, for loss, don't include them\n",
    "        #these are meant to round 'pred' to exactly zeros and ones\n",
    "        #predLabels = K.argmax(pred, axis=-1)\n",
    "        #pred = K.one_hot(predLabels, 4) \n",
    "\n",
    "\n",
    "        ground_positives = K.sum(true, axis=0) + K.epsilon()       # = TP + FN\n",
    "        pred_positives = K.sum(pred, axis=0) + K.epsilon()         # = TP + FP\n",
    "        true_positives = K.sum(true * pred, axis=0) + K.epsilon()  # = TP\n",
    "            #all with shape (4,)\n",
    "\n",
    "        precision = true_positives / pred_positives \n",
    "        recall = true_positives / ground_positives\n",
    "            #both = 1 if ground_positives == 0 or pred_positives == 0\n",
    "            #shape (4,)\n",
    "\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "            #still with shape (4,)\n",
    "        # print(f1)\n",
    "        # weighted_f1 = f1 * ground_positives / K.sum(ground_positives) \n",
    "        weighted_f1 = K.sum(f1)\n",
    "\n",
    "\n",
    "        return 1 - weighted_f1\n",
    "\n",
    "    def f1_loss(y_true, y_pred):\n",
    "    \n",
    "        tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "        tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "        fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "        fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "        p = tp / (tp + fp + K.epsilon())\n",
    "        r = tp / (tp + fn + K.epsilon())\n",
    "        fpr = fp/(fp+tn+K.epsilon())\n",
    "        f1 = 2*p*r / (p+r+K.epsilon())\n",
    "        # f1 = tf.where(tf(f1), tf.zeros_like(f1), f1)\n",
    "        return 1-K.mean(f1)\n",
    "    return custom_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1330 (17685,)\n",
      "29260\n",
      "(None, 1, 1) coeff (None, 5, 30) amplitude\n",
      "Epoch 1/300\n",
      "147/147 [==============================] - 16s 55ms/step - loss: 0.7998 - acc: 0.8376 - f1_score: 0.2826 - val_loss: 1.0355 - val_acc: 0.9029 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to ./models/lag_15_iter_0_split_0_episode_model_v9_new_validation_set_no_smoking_sparse_cross_entropy_10-100_f1_no_fpr.hdf5\n",
      "Epoch 2/300\n",
      "147/147 [==============================] - 7s 44ms/step - loss: 0.3866 - acc: 0.9514 - f1_score: 0.6833 - val_loss: 1.0658 - val_acc: 0.8988 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "Epoch 3/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.2541 - acc: 0.9651 - f1_score: 0.7826 - val_loss: 1.0748 - val_acc: 0.8589 - val_f1_score: 0.0542\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.00000 to 0.05424, saving model to ./models/lag_15_iter_0_split_0_episode_model_v9_new_validation_set_no_smoking_sparse_cross_entropy_10-100_f1_no_fpr.hdf5\n",
      "Epoch 4/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.2002 - acc: 0.9737 - f1_score: 0.8387 - val_loss: 1.1161 - val_acc: 0.8584 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.05424\n",
      "Epoch 5/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.1782 - acc: 0.9757 - f1_score: 0.8485 - val_loss: 1.0540 - val_acc: 0.8938 - val_f1_score: 0.0789\n",
      "\n",
      "Epoch 00005: val_f1_score improved from 0.05424 to 0.07895, saving model to ./models/lag_15_iter_0_split_0_episode_model_v9_new_validation_set_no_smoking_sparse_cross_entropy_10-100_f1_no_fpr.hdf5\n",
      "Epoch 6/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.1504 - acc: 0.9789 - f1_score: 0.8711 - val_loss: 1.0181 - val_acc: 0.9105 - val_f1_score: 0.1531\n",
      "\n",
      "Epoch 00006: val_f1_score improved from 0.07895 to 0.15311, saving model to ./models/lag_15_iter_0_split_0_episode_model_v9_new_validation_set_no_smoking_sparse_cross_entropy_10-100_f1_no_fpr.hdf5\n",
      "Epoch 7/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.1237 - acc: 0.9820 - f1_score: 0.8953 - val_loss: 1.0297 - val_acc: 0.8968 - val_f1_score: 0.1639\n",
      "\n",
      "Epoch 00007: val_f1_score improved from 0.15311 to 0.16393, saving model to ./models/lag_15_iter_0_split_0_episode_model_v9_new_validation_set_no_smoking_sparse_cross_entropy_10-100_f1_no_fpr.hdf5\n",
      "Epoch 8/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0834 - acc: 0.9874 - f1_score: 0.9307 - val_loss: 1.1327 - val_acc: 0.8442 - val_f1_score: 0.0833\n",
      "\n",
      "Epoch 00008: val_f1_score did not improve from 0.16393\n",
      "Epoch 9/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0657 - acc: 0.9900 - f1_score: 0.9437 - val_loss: 1.0413 - val_acc: 0.9069 - val_f1_score: 0.1786\n",
      "\n",
      "Epoch 00009: val_f1_score improved from 0.16393 to 0.17857, saving model to ./models/lag_15_iter_0_split_0_episode_model_v9_new_validation_set_no_smoking_sparse_cross_entropy_10-100_f1_no_fpr.hdf5\n",
      "Epoch 10/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0572 - acc: 0.9914 - f1_score: 0.9517 - val_loss: 1.0703 - val_acc: 0.9054 - val_f1_score: 0.1221\n",
      "\n",
      "Epoch 00010: val_f1_score did not improve from 0.17857\n",
      "Epoch 11/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0454 - acc: 0.9931 - f1_score: 0.9616 - val_loss: 1.0513 - val_acc: 0.8998 - val_f1_score: 0.1538\n",
      "\n",
      "Epoch 00011: val_f1_score did not improve from 0.17857\n",
      "Epoch 12/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0519 - acc: 0.9924 - f1_score: 0.9582 - val_loss: 1.0764 - val_acc: 0.8938 - val_f1_score: 0.1393\n",
      "\n",
      "Epoch 00012: val_f1_score did not improve from 0.17857\n",
      "Epoch 13/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0460 - acc: 0.9934 - f1_score: 0.9624 - val_loss: 1.0367 - val_acc: 0.9176 - val_f1_score: 0.1466\n",
      "\n",
      "Epoch 00013: val_f1_score did not improve from 0.17857\n",
      "Epoch 14/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0445 - acc: 0.9929 - f1_score: 0.9615 - val_loss: 1.0895 - val_acc: 0.8741 - val_f1_score: 0.0945\n",
      "\n",
      "Epoch 00014: val_f1_score did not improve from 0.17857\n",
      "Epoch 15/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0320 - acc: 0.9952 - f1_score: 0.9728 - val_loss: 1.0304 - val_acc: 0.9110 - val_f1_score: 0.2000\n",
      "\n",
      "Epoch 00015: val_f1_score improved from 0.17857 to 0.20000, saving model to ./models/lag_15_iter_0_split_0_episode_model_v9_new_validation_set_no_smoking_sparse_cross_entropy_10-100_f1_no_fpr.hdf5\n",
      "Epoch 16/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0361 - acc: 0.9944 - f1_score: 0.9698 - val_loss: 1.1026 - val_acc: 0.8918 - val_f1_score: 0.0776\n",
      "\n",
      "Epoch 00016: val_f1_score did not improve from 0.20000\n",
      "Epoch 17/300\n",
      "147/147 [==============================] - 6s 44ms/step - loss: 0.0410 - acc: 0.9939 - f1_score: 0.9664 - val_loss: 1.0752 - val_acc: 0.9272 - val_f1_score: 0.0649\n",
      "\n",
      "Epoch 00017: val_f1_score did not improve from 0.20000\n",
      "Epoch 18/300\n",
      "147/147 [==============================] - 6s 39ms/step - loss: 0.0311 - acc: 0.9952 - f1_score: 0.9737 - val_loss: 1.0801 - val_acc: 0.9105 - val_f1_score: 0.1281\n",
      "\n",
      "Epoch 00018: val_f1_score did not improve from 0.20000\n",
      "Epoch 19/300\n",
      "147/147 [==============================] - 5s 37ms/step - loss: 0.0372 - acc: 0.9941 - f1_score: 0.9670 - val_loss: 1.0736 - val_acc: 0.8938 - val_f1_score: 0.1250\n",
      "\n",
      "Epoch 00019: val_f1_score did not improve from 0.20000\n",
      "Epoch 20/300\n",
      "147/147 [==============================] - 6s 44ms/step - loss: 0.0230 - acc: 0.9963 - f1_score: 0.9797 - val_loss: 1.0688 - val_acc: 0.8988 - val_f1_score: 0.1379\n",
      "\n",
      "Epoch 00020: val_f1_score did not improve from 0.20000\n",
      "Epoch 21/300\n",
      "147/147 [==============================] - 6s 42ms/step - loss: 0.0265 - acc: 0.9959 - f1_score: 0.9781 - val_loss: 1.0765 - val_acc: 0.8816 - val_f1_score: 0.1643\n",
      "\n",
      "Epoch 00021: val_f1_score did not improve from 0.20000\n",
      "Epoch 22/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0292 - acc: 0.9958 - f1_score: 0.9760 - val_loss: 1.0120 - val_acc: 0.9246 - val_f1_score: 0.2359\n",
      "\n",
      "Epoch 00022: val_f1_score improved from 0.20000 to 0.23590, saving model to ./models/lag_15_iter_0_split_0_episode_model_v9_new_validation_set_no_smoking_sparse_cross_entropy_10-100_f1_no_fpr.hdf5\n",
      "Epoch 23/300\n",
      "147/147 [==============================] - 6s 44ms/step - loss: 0.0258 - acc: 0.9962 - f1_score: 0.9787 - val_loss: 1.0525 - val_acc: 0.8948 - val_f1_score: 0.1938\n",
      "\n",
      "Epoch 00023: val_f1_score did not improve from 0.23590\n",
      "Epoch 24/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0265 - acc: 0.9962 - f1_score: 0.9788 - val_loss: 1.0500 - val_acc: 0.9221 - val_f1_score: 0.1348\n",
      "\n",
      "Epoch 00024: val_f1_score did not improve from 0.23590\n",
      "Epoch 25/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0249 - acc: 0.9956 - f1_score: 0.9765 - val_loss: 1.0346 - val_acc: 0.9095 - val_f1_score: 0.1900\n",
      "\n",
      "Epoch 00025: val_f1_score did not improve from 0.23590\n",
      "Epoch 26/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0244 - acc: 0.9964 - f1_score: 0.9800 - val_loss: 1.0553 - val_acc: 0.9059 - val_f1_score: 0.1842\n",
      "\n",
      "Epoch 00026: val_f1_score did not improve from 0.23590\n",
      "Epoch 27/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0209 - acc: 0.9966 - f1_score: 0.9819 - val_loss: 1.0387 - val_acc: 0.9282 - val_f1_score: 0.1932\n",
      "\n",
      "Epoch 00027: val_f1_score did not improve from 0.23590\n",
      "Epoch 28/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0252 - acc: 0.9960 - f1_score: 0.9778 - val_loss: 1.0751 - val_acc: 0.9140 - val_f1_score: 0.1053\n",
      "\n",
      "Epoch 00028: val_f1_score did not improve from 0.23590\n",
      "Epoch 29/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0243 - acc: 0.9963 - f1_score: 0.9794 - val_loss: 1.0566 - val_acc: 0.9115 - val_f1_score: 0.1784\n",
      "\n",
      "Epoch 00029: val_f1_score did not improve from 0.23590\n",
      "Epoch 30/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0208 - acc: 0.9968 - f1_score: 0.9820 - val_loss: 1.0714 - val_acc: 0.8862 - val_f1_score: 0.1636\n",
      "\n",
      "Epoch 00030: val_f1_score did not improve from 0.23590\n",
      "Epoch 31/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0174 - acc: 0.9972 - f1_score: 0.9850 - val_loss: 1.1032 - val_acc: 0.8710 - val_f1_score: 0.1414\n",
      "\n",
      "Epoch 00031: val_f1_score did not improve from 0.23590\n",
      "Epoch 32/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0224 - acc: 0.9964 - f1_score: 0.9804 - val_loss: 1.1314 - val_acc: 0.8498 - val_f1_score: 0.1341\n",
      "\n",
      "Epoch 00032: val_f1_score did not improve from 0.23590\n",
      "Epoch 33/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0209 - acc: 0.9967 - f1_score: 0.9818 - val_loss: 1.0280 - val_acc: 0.9029 - val_f1_score: 0.1933\n",
      "\n",
      "Epoch 00033: val_f1_score did not improve from 0.23590\n",
      "Epoch 34/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0210 - acc: 0.9967 - f1_score: 0.9816 - val_loss: 1.0953 - val_acc: 0.8816 - val_f1_score: 0.1702\n",
      "\n",
      "Epoch 00034: val_f1_score did not improve from 0.23590\n",
      "Epoch 35/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0194 - acc: 0.9971 - f1_score: 0.9840 - val_loss: 1.0570 - val_acc: 0.9064 - val_f1_score: 0.1704\n",
      "\n",
      "Epoch 00035: val_f1_score did not improve from 0.23590\n",
      "Epoch 36/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0169 - acc: 0.9973 - f1_score: 0.9849 - val_loss: 1.0286 - val_acc: 0.9201 - val_f1_score: 0.2178\n",
      "\n",
      "Epoch 00036: val_f1_score did not improve from 0.23590\n",
      "Epoch 37/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0144 - acc: 0.9980 - f1_score: 0.9891 - val_loss: 1.0100 - val_acc: 0.9287 - val_f1_score: 0.2378\n",
      "\n",
      "Epoch 00037: val_f1_score improved from 0.23590 to 0.23784, saving model to ./models/lag_15_iter_0_split_0_episode_model_v9_new_validation_set_no_smoking_sparse_cross_entropy_10-100_f1_no_fpr.hdf5\n",
      "Epoch 38/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0179 - acc: 0.9972 - f1_score: 0.9843 - val_loss: 1.0590 - val_acc: 0.8872 - val_f1_score: 0.1771\n",
      "\n",
      "Epoch 00038: val_f1_score did not improve from 0.23784\n",
      "Epoch 39/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0210 - acc: 0.9965 - f1_score: 0.9813 - val_loss: 1.0595 - val_acc: 0.8902 - val_f1_score: 0.1811\n",
      "\n",
      "Epoch 00039: val_f1_score did not improve from 0.23784\n",
      "Epoch 40/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0207 - acc: 0.9970 - f1_score: 0.9837 - val_loss: 1.1130 - val_acc: 0.8776 - val_f1_score: 0.1232\n",
      "\n",
      "Epoch 00040: val_f1_score did not improve from 0.23784\n",
      "Epoch 41/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0134 - acc: 0.9981 - f1_score: 0.9892 - val_loss: 1.0758 - val_acc: 0.8933 - val_f1_score: 0.1790\n",
      "\n",
      "Epoch 00041: val_f1_score did not improve from 0.23784\n",
      "Epoch 42/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0146 - acc: 0.9977 - f1_score: 0.9874 - val_loss: 1.0503 - val_acc: 0.9100 - val_f1_score: 0.2124\n",
      "\n",
      "Epoch 00042: val_f1_score did not improve from 0.23784\n",
      "Epoch 43/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0181 - acc: 0.9972 - f1_score: 0.9844 - val_loss: 1.1143 - val_acc: 0.8670 - val_f1_score: 0.1204\n",
      "\n",
      "Epoch 00043: val_f1_score did not improve from 0.23784\n",
      "Epoch 44/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0219 - acc: 0.9965 - f1_score: 0.9811 - val_loss: 1.1158 - val_acc: 0.8902 - val_f1_score: 0.1070\n",
      "\n",
      "Epoch 00044: val_f1_score did not improve from 0.23784\n",
      "Epoch 45/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0173 - acc: 0.9974 - f1_score: 0.9859 - val_loss: 1.1027 - val_acc: 0.9155 - val_f1_score: 0.0874\n",
      "\n",
      "Epoch 00045: val_f1_score did not improve from 0.23784\n",
      "Epoch 46/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0153 - acc: 0.9976 - f1_score: 0.9868 - val_loss: 1.0870 - val_acc: 0.8847 - val_f1_score: 0.1618\n",
      "\n",
      "Epoch 00046: val_f1_score did not improve from 0.23784\n",
      "Epoch 47/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0180 - acc: 0.9970 - f1_score: 0.9838 - val_loss: 1.0572 - val_acc: 0.9090 - val_f1_score: 0.2035\n",
      "\n",
      "Epoch 00047: val_f1_score did not improve from 0.23784\n",
      "Epoch 48/300\n",
      "147/147 [==============================] - 7s 44ms/step - loss: 0.0135 - acc: 0.9980 - f1_score: 0.9892 - val_loss: 1.0650 - val_acc: 0.9140 - val_f1_score: 0.1584\n",
      "\n",
      "Epoch 00048: val_f1_score did not improve from 0.23784\n",
      "Epoch 49/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0158 - acc: 0.9976 - f1_score: 0.9871 - val_loss: 1.0453 - val_acc: 0.9130 - val_f1_score: 0.1963\n",
      "\n",
      "Epoch 00049: val_f1_score did not improve from 0.23784\n",
      "Epoch 50/300\n",
      "147/147 [==============================] - 7s 46ms/step - loss: 0.0111 - acc: 0.9984 - f1_score: 0.9913 - val_loss: 1.0605 - val_acc: 0.9064 - val_f1_score: 0.1850\n",
      "\n",
      "Epoch 00050: val_f1_score did not improve from 0.23784\n",
      "Epoch 51/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0106 - acc: 0.9983 - f1_score: 0.9906 - val_loss: 1.0759 - val_acc: 0.9004 - val_f1_score: 0.1397\n",
      "\n",
      "Epoch 00051: val_f1_score did not improve from 0.23784\n",
      "Epoch 52/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0137 - acc: 0.9979 - f1_score: 0.9881 - val_loss: 1.0507 - val_acc: 0.9100 - val_f1_score: 0.2054\n",
      "\n",
      "Epoch 00052: val_f1_score did not improve from 0.23784\n",
      "Epoch 53/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0137 - acc: 0.9980 - f1_score: 0.9889 - val_loss: 1.0643 - val_acc: 0.9074 - val_f1_score: 0.1794\n",
      "\n",
      "Epoch 00053: val_f1_score did not improve from 0.23784\n",
      "Epoch 54/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0155 - acc: 0.9976 - f1_score: 0.9872 - val_loss: 1.0358 - val_acc: 0.9221 - val_f1_score: 0.2300\n",
      "\n",
      "Epoch 00054: val_f1_score did not improve from 0.23784\n",
      "Epoch 55/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0135 - acc: 0.9981 - f1_score: 0.9896 - val_loss: 1.0374 - val_acc: 0.9191 - val_f1_score: 0.2233\n",
      "\n",
      "Epoch 00055: val_f1_score did not improve from 0.23784\n",
      "Epoch 56/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0142 - acc: 0.9976 - f1_score: 0.9871 - val_loss: 1.0774 - val_acc: 0.8912 - val_f1_score: 0.1634\n",
      "\n",
      "Epoch 00056: val_f1_score did not improve from 0.23784\n",
      "Epoch 57/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0110 - acc: 0.9982 - f1_score: 0.9903 - val_loss: 1.0585 - val_acc: 0.9044 - val_f1_score: 0.1888\n",
      "\n",
      "Epoch 00057: val_f1_score did not improve from 0.23784\n",
      "Epoch 58/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0108 - acc: 0.9983 - f1_score: 0.9907 - val_loss: 1.0859 - val_acc: 0.8862 - val_f1_score: 0.1509\n",
      "\n",
      "Epoch 00058: val_f1_score did not improve from 0.23784\n",
      "Epoch 59/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0084 - acc: 0.9986 - f1_score: 0.9924 - val_loss: 1.0676 - val_acc: 0.8978 - val_f1_score: 0.1984\n",
      "\n",
      "Epoch 00059: val_f1_score did not improve from 0.23784\n",
      "Epoch 60/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0102 - acc: 0.9984 - f1_score: 0.9911 - val_loss: 1.0636 - val_acc: 0.9084 - val_f1_score: 0.2026\n",
      "\n",
      "Epoch 00060: val_f1_score did not improve from 0.23784\n",
      "Epoch 61/300\n",
      "147/147 [==============================] - 6s 43ms/step - loss: 0.0118 - acc: 0.9982 - f1_score: 0.9900 - val_loss: 1.1013 - val_acc: 0.9019 - val_f1_score: 0.1339\n",
      "\n",
      "Epoch 00061: val_f1_score did not improve from 0.23784\n",
      "Epoch 62/300\n",
      "147/147 [==============================] - 6s 38ms/step - loss: 0.0115 - acc: 0.9982 - f1_score: 0.9900 - val_loss: 1.0496 - val_acc: 0.9231 - val_f1_score: 0.1915\n",
      "\n",
      "Epoch 00062: val_f1_score did not improve from 0.23784\n",
      "Epoch 63/300\n",
      "147/147 [==============================] - 6s 42ms/step - loss: 0.0106 - acc: 0.9983 - f1_score: 0.9903 - val_loss: 1.0790 - val_acc: 0.8993 - val_f1_score: 0.1811\n",
      "\n",
      "Epoch 00063: val_f1_score did not improve from 0.23784\n",
      "Epoch 64/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0122 - acc: 0.9980 - f1_score: 0.9893 - val_loss: 1.0845 - val_acc: 0.8928 - val_f1_score: 0.1452\n",
      "\n",
      "Epoch 00064: val_f1_score did not improve from 0.23784\n",
      "Epoch 65/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0190 - acc: 0.9972 - f1_score: 0.9841 - val_loss: 1.0278 - val_acc: 0.9196 - val_f1_score: 0.2167\n",
      "\n",
      "Epoch 00065: val_f1_score did not improve from 0.23784\n",
      "Epoch 66/300\n",
      "147/147 [==============================] - 7s 45ms/step - loss: 0.0140 - acc: 0.9978 - f1_score: 0.9883 - val_loss: 1.1021 - val_acc: 0.8796 - val_f1_score: 0.1620\n",
      "\n",
      "Epoch 00066: val_f1_score did not improve from 0.23784\n",
      "Epoch 67/300\n",
      "147/147 [==============================] - 6s 44ms/step - loss: 0.0113 - acc: 0.9983 - f1_score: 0.9907 - val_loss: 1.0745 - val_acc: 0.8852 - val_f1_score: 0.1745\n",
      "\n",
      "Epoch 00067: val_f1_score did not improve from 0.23784\n",
      "Epoch 68/300\n",
      "147/147 [==============================] - 6s 43ms/step - loss: 0.0114 - acc: 0.9983 - f1_score: 0.9903 - val_loss: 1.0306 - val_acc: 0.9130 - val_f1_score: 0.2252\n",
      "\n",
      "Epoch 00068: val_f1_score did not improve from 0.23784\n",
      "Epoch 69/300\n",
      "147/147 [==============================] - 6s 38ms/step - loss: 0.0102 - acc: 0.9985 - f1_score: 0.9916 - val_loss: 1.0641 - val_acc: 0.8938 - val_f1_score: 0.1667\n",
      "\n",
      "Epoch 00069: val_f1_score did not improve from 0.23784\n",
      "Epoch 70/300\n",
      "147/147 [==============================] - 6s 42ms/step - loss: 0.0083 - acc: 0.9985 - f1_score: 0.9920 - val_loss: 1.0269 - val_acc: 0.9201 - val_f1_score: 0.2330\n",
      "\n",
      "Epoch 00070: val_f1_score did not improve from 0.23784\n",
      "Epoch 71/300\n",
      "147/147 [==============================] - 7s 44ms/step - loss: 0.0063 - acc: 0.9991 - f1_score: 0.9951 - val_loss: 1.0679 - val_acc: 0.9049 - val_f1_score: 0.2034\n",
      "\n",
      "Epoch 00071: val_f1_score did not improve from 0.23784\n",
      "Epoch 72/300\n",
      "147/147 [==============================] - 6s 44ms/step - loss: 0.0123 - acc: 0.9980 - f1_score: 0.9893 - val_loss: 1.0800 - val_acc: 0.8958 - val_f1_score: 0.1890\n",
      "\n",
      "Epoch 00072: val_f1_score did not improve from 0.23784\n",
      "Epoch 73/300\n",
      "147/147 [==============================] - 6s 43ms/step - loss: 0.0111 - acc: 0.9983 - f1_score: 0.9907 - val_loss: 1.0741 - val_acc: 0.9024 - val_f1_score: 0.1925\n",
      "\n",
      "Epoch 00073: val_f1_score did not improve from 0.23784\n",
      "Epoch 74/300\n",
      "147/147 [==============================] - 6s 38ms/step - loss: 0.0112 - acc: 0.9983 - f1_score: 0.9904 - val_loss: 1.0710 - val_acc: 0.8968 - val_f1_score: 0.1707\n",
      "\n",
      "Epoch 00074: val_f1_score did not improve from 0.23784\n",
      "Epoch 75/300\n",
      "147/147 [==============================] - 6s 38ms/step - loss: 0.0079 - acc: 0.9988 - f1_score: 0.9932 - val_loss: 1.0814 - val_acc: 0.8933 - val_f1_score: 0.1725\n",
      "\n",
      "Epoch 00075: val_f1_score did not improve from 0.23784\n",
      "Epoch 76/300\n",
      "147/147 [==============================] - 5s 35ms/step - loss: 0.0089 - acc: 0.9986 - f1_score: 0.9923 - val_loss: 1.1120 - val_acc: 0.8781 - val_f1_score: 0.1718\n",
      "\n",
      "Epoch 00076: val_f1_score did not improve from 0.23784\n",
      "Epoch 77/300\n",
      "147/147 [==============================] - 5s 32ms/step - loss: 0.0068 - acc: 0.9990 - f1_score: 0.9947 - val_loss: 1.1058 - val_acc: 0.8938 - val_f1_score: 0.1860\n",
      "\n",
      "Epoch 00077: val_f1_score did not improve from 0.23784\n",
      "Epoch 00077: early stopping\n",
      "(0.12471395881006866, 8.988766e-08)\n",
      "(0.16897746967071056, 2.2105432e-08)\n",
      "0.45182256303751633\n",
      "1346 (18326,)\n",
      "29612\n",
      "(None, 1, 1) coeff (None, 5, 30) amplitude\n",
      "Epoch 1/300\n",
      "149/149 [==============================] - 17s 44ms/step - loss: 0.7881 - acc: 0.8094 - f1_score: 0.2995 - val_loss: 1.0497 - val_acc: 0.8883 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to ./models/lag_15_iter_0_split_1_episode_model_v9_new_validation_set_no_smoking_sparse_cross_entropy_10-100_f1_no_fpr.hdf5\n",
      "Epoch 2/300\n",
      "149/149 [==============================] - 6s 38ms/step - loss: 0.4409 - acc: 0.9432 - f1_score: 0.6361 - val_loss: 1.0777 - val_acc: 0.8995 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "Epoch 3/300\n",
      "149/149 [==============================] - 6s 38ms/step - loss: 0.3026 - acc: 0.9607 - f1_score: 0.7517 - val_loss: 1.0910 - val_acc: 0.8900 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "Epoch 4/300\n",
      "149/149 [==============================] - 6s 42ms/step - loss: 0.2242 - acc: 0.9700 - f1_score: 0.8139 - val_loss: 1.0929 - val_acc: 0.8530 - val_f1_score: 0.0535\n",
      "\n",
      "Epoch 00004: val_f1_score improved from 0.00000 to 0.05348, saving model to ./models/lag_15_iter_0_split_1_episode_model_v9_new_validation_set_no_smoking_sparse_cross_entropy_10-100_f1_no_fpr.hdf5\n",
      "Epoch 5/300\n",
      "149/149 [==============================] - 7s 44ms/step - loss: 0.1736 - acc: 0.9749 - f1_score: 0.8570 - val_loss: 0.9950 - val_acc: 0.9194 - val_f1_score: 0.1709\n",
      "\n",
      "Epoch 00005: val_f1_score improved from 0.05348 to 0.17094, saving model to ./models/lag_15_iter_0_split_1_episode_model_v9_new_validation_set_no_smoking_sparse_cross_entropy_10-100_f1_no_fpr.hdf5\n",
      "Epoch 6/300\n",
      "149/149 [==============================] - 7s 45ms/step - loss: 0.1357 - acc: 0.9808 - f1_score: 0.8897 - val_loss: 1.0739 - val_acc: 0.9082 - val_f1_score: 0.0433\n",
      "\n",
      "Epoch 00006: val_f1_score did not improve from 0.17094\n",
      "Epoch 7/300\n",
      "149/149 [==============================] - 6s 43ms/step - loss: 0.1202 - acc: 0.9826 - f1_score: 0.9011 - val_loss: 1.0913 - val_acc: 0.9120 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_f1_score did not improve from 0.17094\n",
      "Epoch 8/300\n",
      "149/149 [==============================] - 6s 38ms/step - loss: 0.1093 - acc: 0.9843 - f1_score: 0.9102 - val_loss: 0.9891 - val_acc: 0.9277 - val_f1_score: 0.1869\n",
      "\n",
      "Epoch 00008: val_f1_score improved from 0.17094 to 0.18692, saving model to ./models/lag_15_iter_0_split_1_episode_model_v9_new_validation_set_no_smoking_sparse_cross_entropy_10-100_f1_no_fpr.hdf5\n",
      "Epoch 9/300\n",
      "149/149 [==============================] - 7s 44ms/step - loss: 0.0992 - acc: 0.9861 - f1_score: 0.9210 - val_loss: 1.0464 - val_acc: 0.9037 - val_f1_score: 0.0866\n",
      "\n",
      "Epoch 00009: val_f1_score did not improve from 0.18692\n",
      "Epoch 10/300\n",
      "149/149 [==============================] - 6s 40ms/step - loss: 0.0746 - acc: 0.9888 - f1_score: 0.9367 - val_loss: 1.0581 - val_acc: 0.9066 - val_f1_score: 0.0891\n",
      "\n",
      "Epoch 00010: val_f1_score did not improve from 0.18692\n",
      "Epoch 11/300\n",
      "149/149 [==============================] - 6s 38ms/step - loss: 0.0739 - acc: 0.9890 - f1_score: 0.9373 - val_loss: 1.0416 - val_acc: 0.9286 - val_f1_score: 0.1313\n",
      "\n",
      "Epoch 00011: val_f1_score did not improve from 0.18692\n",
      "Epoch 12/300\n",
      "149/149 [==============================] - 6s 38ms/step - loss: 0.0699 - acc: 0.9904 - f1_score: 0.9449 - val_loss: 1.0192 - val_acc: 0.9315 - val_f1_score: 0.1451\n",
      "\n",
      "Epoch 00012: val_f1_score did not improve from 0.18692\n",
      "Epoch 13/300\n",
      "149/149 [==============================] - 7s 45ms/step - loss: 0.0608 - acc: 0.9909 - f1_score: 0.9479 - val_loss: 1.0451 - val_acc: 0.9273 - val_f1_score: 0.0838\n",
      "\n",
      "Epoch 00013: val_f1_score did not improve from 0.18692\n",
      "Epoch 14/300\n",
      "149/149 [==============================] - 7s 45ms/step - loss: 0.0557 - acc: 0.9912 - f1_score: 0.9511 - val_loss: 1.0704 - val_acc: 0.9024 - val_f1_score: 0.0784\n",
      "\n",
      "Epoch 00014: val_f1_score did not improve from 0.18692\n",
      "Epoch 15/300\n",
      "149/149 [==============================] - 7s 45ms/step - loss: 0.0589 - acc: 0.9914 - f1_score: 0.9515 - val_loss: 1.0297 - val_acc: 0.9323 - val_f1_score: 0.1093\n",
      "\n",
      "Epoch 00015: val_f1_score did not improve from 0.18692\n",
      "Epoch 16/300\n",
      "149/149 [==============================] - 7s 44ms/step - loss: 0.0591 - acc: 0.9906 - f1_score: 0.9481 - val_loss: 1.0231 - val_acc: 0.9240 - val_f1_score: 0.0985\n",
      "\n",
      "Epoch 00016: val_f1_score did not improve from 0.18692\n",
      "Epoch 17/300\n",
      "149/149 [==============================] - 7s 44ms/step - loss: 0.0511 - acc: 0.9922 - f1_score: 0.9554 - val_loss: 1.0346 - val_acc: 0.9223 - val_f1_score: 0.1461\n",
      "\n",
      "Epoch 00017: val_f1_score did not improve from 0.18692\n",
      "Epoch 18/300\n",
      "149/149 [==============================] - 6s 40ms/step - loss: 0.0486 - acc: 0.9924 - f1_score: 0.9576 - val_loss: 1.0368 - val_acc: 0.9302 - val_f1_score: 0.0968\n",
      "\n",
      "Epoch 00018: val_f1_score did not improve from 0.18692\n",
      "Epoch 19/300\n",
      "149/149 [==============================] - 6s 38ms/step - loss: 0.0490 - acc: 0.9918 - f1_score: 0.9549 - val_loss: 1.0352 - val_acc: 0.9228 - val_f1_score: 0.1058\n",
      "\n",
      "Epoch 00019: val_f1_score did not improve from 0.18692\n",
      "Epoch 20/300\n",
      "149/149 [==============================] - 6s 42ms/step - loss: 0.0424 - acc: 0.9934 - f1_score: 0.9637 - val_loss: 1.0486 - val_acc: 0.9161 - val_f1_score: 0.1217\n",
      "\n",
      "Epoch 00020: val_f1_score did not improve from 0.18692\n",
      "Epoch 21/300\n",
      "149/149 [==============================] - 7s 45ms/step - loss: 0.0315 - acc: 0.9949 - f1_score: 0.9729 - val_loss: 1.0568 - val_acc: 0.9169 - val_f1_score: 0.0826\n",
      "\n",
      "Epoch 00021: val_f1_score did not improve from 0.18692\n",
      "Epoch 22/300\n",
      "149/149 [==============================] - 6s 41ms/step - loss: 0.0333 - acc: 0.9950 - f1_score: 0.9717 - val_loss: 1.0844 - val_acc: 0.9070 - val_f1_score: 0.0588\n",
      "\n",
      "Epoch 00022: val_f1_score did not improve from 0.18692\n",
      "Epoch 23/300\n",
      "149/149 [==============================] - 6s 38ms/step - loss: 0.0345 - acc: 0.9948 - f1_score: 0.9716 - val_loss: 1.0247 - val_acc: 0.9240 - val_f1_score: 0.1644\n",
      "\n",
      "Epoch 00023: val_f1_score did not improve from 0.18692\n",
      "Epoch 24/300\n",
      "149/149 [==============================] - 6s 42ms/step - loss: 0.0288 - acc: 0.9956 - f1_score: 0.9756 - val_loss: 0.9413 - val_acc: 0.9336 - val_f1_score: 0.3277\n",
      "\n",
      "Epoch 00024: val_f1_score improved from 0.18692 to 0.32773, saving model to ./models/lag_15_iter_0_split_1_episode_model_v9_new_validation_set_no_smoking_sparse_cross_entropy_10-100_f1_no_fpr.hdf5\n",
      "Epoch 25/300\n",
      "149/149 [==============================] - 7s 44ms/step - loss: 0.0246 - acc: 0.9964 - f1_score: 0.9798 - val_loss: 1.0738 - val_acc: 0.9136 - val_f1_score: 0.0796\n",
      "\n",
      "Epoch 00025: val_f1_score did not improve from 0.32773\n",
      "Epoch 26/300\n",
      "149/149 [==============================] - 7s 45ms/step - loss: 0.0364 - acc: 0.9944 - f1_score: 0.9679 - val_loss: 1.0242 - val_acc: 0.9207 - val_f1_score: 0.1659\n",
      "\n",
      "Epoch 00026: val_f1_score did not improve from 0.32773\n",
      "Epoch 27/300\n",
      "149/149 [==============================] - 7s 45ms/step - loss: 0.0197 - acc: 0.9969 - f1_score: 0.9826 - val_loss: 1.0292 - val_acc: 0.9020 - val_f1_score: 0.2027\n",
      "\n",
      "Epoch 00027: val_f1_score did not improve from 0.32773\n",
      "Epoch 28/300\n",
      "149/149 [==============================] - 7s 44ms/step - loss: 0.0257 - acc: 0.9962 - f1_score: 0.9798 - val_loss: 1.0262 - val_acc: 0.9365 - val_f1_score: 0.1453\n",
      "\n",
      "Epoch 00028: val_f1_score did not improve from 0.32773\n",
      "Epoch 29/300\n",
      "149/149 [==============================] - 6s 39ms/step - loss: 0.0329 - acc: 0.9955 - f1_score: 0.9734 - val_loss: 1.0582 - val_acc: 0.9169 - val_f1_score: 0.0909\n",
      "\n",
      "Epoch 00029: val_f1_score did not improve from 0.32773\n",
      "Epoch 30/300\n",
      "149/149 [==============================] - 6s 38ms/step - loss: 0.0188 - acc: 0.9970 - f1_score: 0.9834 - val_loss: 1.0343 - val_acc: 0.9431 - val_f1_score: 0.1161\n",
      "\n",
      "Epoch 00030: val_f1_score did not improve from 0.32773\n",
      "Epoch 31/300\n",
      "149/149 [==============================] - 7s 44ms/step - loss: 0.0207 - acc: 0.9967 - f1_score: 0.9821 - val_loss: 1.0681 - val_acc: 0.9115 - val_f1_score: 0.0779\n",
      "\n",
      "Epoch 00031: val_f1_score did not improve from 0.32773\n",
      "Epoch 32/300\n",
      "149/149 [==============================] - 7s 45ms/step - loss: 0.0231 - acc: 0.9965 - f1_score: 0.9814 - val_loss: 1.0557 - val_acc: 0.9107 - val_f1_score: 0.0851\n",
      "\n",
      "Epoch 00032: val_f1_score did not improve from 0.32773\n",
      "Epoch 33/300\n",
      "149/149 [==============================] - 7s 44ms/step - loss: 0.0192 - acc: 0.9972 - f1_score: 0.9848 - val_loss: 1.0929 - val_acc: 0.9128 - val_f1_score: 0.0367\n",
      "\n",
      "Epoch 00033: val_f1_score did not improve from 0.32773\n",
      "Epoch 34/300\n",
      "149/149 [==============================] - 6s 44ms/step - loss: 0.0204 - acc: 0.9967 - f1_score: 0.9817 - val_loss: 1.1142 - val_acc: 0.9145 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00034: val_f1_score did not improve from 0.32773\n",
      "Epoch 35/300\n",
      "149/149 [==============================] - 6s 43ms/step - loss: 0.0173 - acc: 0.9972 - f1_score: 0.9845 - val_loss: 1.0431 - val_acc: 0.9215 - val_f1_score: 0.1525\n",
      "\n",
      "Epoch 00035: val_f1_score did not improve from 0.32773\n",
      "Epoch 36/300\n",
      "149/149 [==============================] - 6s 38ms/step - loss: 0.0149 - acc: 0.9976 - f1_score: 0.9871 - val_loss: 1.0946 - val_acc: 0.9053 - val_f1_score: 0.0500\n",
      "\n",
      "Epoch 00036: val_f1_score did not improve from 0.32773\n",
      "Epoch 37/300\n",
      "149/149 [==============================] - 6s 40ms/step - loss: 0.0176 - acc: 0.9973 - f1_score: 0.9849 - val_loss: 1.0790 - val_acc: 0.9228 - val_f1_score: 0.0700\n",
      "\n",
      "Epoch 00037: val_f1_score did not improve from 0.32773\n",
      "Epoch 38/300\n",
      "149/149 [==============================] - 6s 41ms/step - loss: 0.0179 - acc: 0.9973 - f1_score: 0.9850 - val_loss: 1.0692 - val_acc: 0.9086 - val_f1_score: 0.0678\n",
      "\n",
      "Epoch 00038: val_f1_score did not improve from 0.32773\n",
      "Epoch 39/300\n",
      "149/149 [==============================] - 6s 37ms/step - loss: 0.0198 - acc: 0.9970 - f1_score: 0.9834 - val_loss: 1.0451 - val_acc: 0.9439 - val_f1_score: 0.1401\n",
      "\n",
      "Epoch 00039: val_f1_score did not improve from 0.32773\n",
      "Epoch 40/300\n",
      "149/149 [==============================] - 6s 41ms/step - loss: 0.0222 - acc: 0.9967 - f1_score: 0.9820 - val_loss: 1.0878 - val_acc: 0.9203 - val_f1_score: 0.0588\n",
      "\n",
      "Epoch 00040: val_f1_score did not improve from 0.32773\n",
      "Epoch 41/300\n",
      "149/149 [==============================] - 6s 42ms/step - loss: 0.0158 - acc: 0.9977 - f1_score: 0.9865 - val_loss: 1.0997 - val_acc: 0.9145 - val_f1_score: 0.0374\n",
      "\n",
      "Epoch 00041: val_f1_score did not improve from 0.32773\n",
      "Epoch 42/300\n",
      "149/149 [==============================] - 7s 45ms/step - loss: 0.0173 - acc: 0.9971 - f1_score: 0.9843 - val_loss: 1.0388 - val_acc: 0.9140 - val_f1_score: 0.1481\n",
      "\n",
      "Epoch 00042: val_f1_score did not improve from 0.32773\n",
      "Epoch 43/300\n",
      "149/149 [==============================] - 6s 44ms/step - loss: 0.0142 - acc: 0.9978 - f1_score: 0.9879 - val_loss: 1.1052 - val_acc: 0.9091 - val_f1_score: 0.0437\n",
      "\n",
      "Epoch 00043: val_f1_score did not improve from 0.32773\n",
      "Epoch 44/300\n",
      "149/149 [==============================] - 6s 38ms/step - loss: 0.0158 - acc: 0.9975 - f1_score: 0.9858 - val_loss: 1.0649 - val_acc: 0.9132 - val_f1_score: 0.0793\n",
      "\n",
      "Epoch 00044: val_f1_score did not improve from 0.32773\n",
      "Epoch 45/300\n",
      "149/149 [==============================] - 6s 39ms/step - loss: 0.0175 - acc: 0.9972 - f1_score: 0.9849 - val_loss: 1.0795 - val_acc: 0.9344 - val_f1_score: 0.0482\n",
      "\n",
      "Epoch 00045: val_f1_score did not improve from 0.32773\n",
      "Epoch 46/300\n",
      "149/149 [==============================] - 7s 45ms/step - loss: 0.0136 - acc: 0.9980 - f1_score: 0.9890 - val_loss: 1.0900 - val_acc: 0.9257 - val_f1_score: 0.0219\n",
      "\n",
      "Epoch 00046: val_f1_score did not improve from 0.32773\n",
      "Epoch 47/300\n",
      "149/149 [==============================] - 6s 43ms/step - loss: 0.0105 - acc: 0.9984 - f1_score: 0.9913 - val_loss: 1.1330 - val_acc: 0.9041 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00047: val_f1_score did not improve from 0.32773\n",
      "Epoch 48/300\n",
      "149/149 [==============================] - 6s 39ms/step - loss: 0.0135 - acc: 0.9979 - f1_score: 0.9889 - val_loss: 1.1254 - val_acc: 0.9165 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00048: val_f1_score did not improve from 0.32773\n",
      "Epoch 49/300\n",
      "149/149 [==============================] - 6s 38ms/step - loss: 0.0160 - acc: 0.9974 - f1_score: 0.9857 - val_loss: 1.1196 - val_acc: 0.9165 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00049: val_f1_score did not improve from 0.32773\n",
      "Epoch 50/300\n",
      "149/149 [==============================] - 6s 43ms/step - loss: 0.0165 - acc: 0.9975 - f1_score: 0.9858 - val_loss: 1.1362 - val_acc: 0.9086 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00050: val_f1_score did not improve from 0.32773\n",
      "Epoch 51/300\n",
      "149/149 [==============================] - 7s 45ms/step - loss: 0.0176 - acc: 0.9973 - f1_score: 0.9852 - val_loss: 1.0723 - val_acc: 0.9223 - val_f1_score: 0.0788\n",
      "\n",
      "Epoch 00051: val_f1_score did not improve from 0.32773\n",
      "Epoch 52/300\n",
      "149/149 [==============================] - 6s 42ms/step - loss: 0.0154 - acc: 0.9980 - f1_score: 0.9889 - val_loss: 1.0441 - val_acc: 0.9244 - val_f1_score: 0.1333\n",
      "\n",
      "Epoch 00052: val_f1_score did not improve from 0.32773\n",
      "Epoch 53/300\n",
      "149/149 [==============================] - 6s 38ms/step - loss: 0.0139 - acc: 0.9978 - f1_score: 0.9880 - val_loss: 1.0477 - val_acc: 0.9277 - val_f1_score: 0.1212\n",
      "\n",
      "Epoch 00053: val_f1_score did not improve from 0.32773\n",
      "Epoch 54/300\n",
      "149/149 [==============================] - 6s 41ms/step - loss: 0.0142 - acc: 0.9976 - f1_score: 0.9872 - val_loss: 1.1030 - val_acc: 0.8783 - val_f1_score: 0.0579\n",
      "\n",
      "Epoch 00054: val_f1_score did not improve from 0.32773\n",
      "Epoch 55/300\n",
      "149/149 [==============================] - 7s 44ms/step - loss: 0.0150 - acc: 0.9978 - f1_score: 0.9881 - val_loss: 1.0920 - val_acc: 0.9215 - val_f1_score: 0.0308\n",
      "\n",
      "Epoch 00055: val_f1_score did not improve from 0.32773\n",
      "Epoch 56/300\n",
      "149/149 [==============================] - 7s 45ms/step - loss: 0.0175 - acc: 0.9974 - f1_score: 0.9855 - val_loss: 1.1166 - val_acc: 0.9124 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00056: val_f1_score did not improve from 0.32773\n",
      "Epoch 57/300\n",
      "149/149 [==============================] - 7s 45ms/step - loss: 0.0134 - acc: 0.9979 - f1_score: 0.9884 - val_loss: 1.0917 - val_acc: 0.9236 - val_f1_score: 0.0515\n",
      "\n",
      "Epoch 00057: val_f1_score did not improve from 0.32773\n",
      "Epoch 58/300\n",
      "149/149 [==============================] - 7s 45ms/step - loss: 0.0166 - acc: 0.9975 - f1_score: 0.9862 - val_loss: 1.0537 - val_acc: 0.9369 - val_f1_score: 0.0952\n",
      "\n",
      "Epoch 00058: val_f1_score did not improve from 0.32773\n",
      "Epoch 59/300\n",
      "149/149 [==============================] - 7s 44ms/step - loss: 0.0097 - acc: 0.9984 - f1_score: 0.9912 - val_loss: 1.0780 - val_acc: 0.9340 - val_f1_score: 0.0702\n",
      "\n",
      "Epoch 00059: val_f1_score did not improve from 0.32773\n",
      "Epoch 60/300\n",
      "149/149 [==============================] - 6s 39ms/step - loss: 0.0119 - acc: 0.9983 - f1_score: 0.9905 - val_loss: 1.1068 - val_acc: 0.9194 - val_f1_score: 0.0396\n",
      "\n",
      "Epoch 00060: val_f1_score did not improve from 0.32773\n",
      "Epoch 61/300\n",
      "149/149 [==============================] - 6s 38ms/step - loss: 0.0078 - acc: 0.9988 - f1_score: 0.9933 - val_loss: 1.1026 - val_acc: 0.9120 - val_f1_score: 0.0364\n",
      "\n",
      "Epoch 00061: val_f1_score did not improve from 0.32773\n",
      "Epoch 62/300\n",
      "149/149 [==============================] - 6s 43ms/step - loss: 0.0150 - acc: 0.9978 - f1_score: 0.9873 - val_loss: 1.0794 - val_acc: 0.9282 - val_f1_score: 0.0442\n",
      "\n",
      "Epoch 00062: val_f1_score did not improve from 0.32773\n",
      "Epoch 63/300\n",
      "149/149 [==============================] - 7s 45ms/step - loss: 0.0138 - acc: 0.9980 - f1_score: 0.9890 - val_loss: 1.0867 - val_acc: 0.9244 - val_f1_score: 0.0619\n",
      "\n",
      "Epoch 00063: val_f1_score did not improve from 0.32773\n",
      "Epoch 64/300\n",
      "149/149 [==============================] - 7s 45ms/step - loss: 0.0160 - acc: 0.9975 - f1_score: 0.9857 - val_loss: 1.0537 - val_acc: 0.9277 - val_f1_score: 0.1212\n",
      "\n",
      "Epoch 00064: val_f1_score did not improve from 0.32773\n",
      "Epoch 00064: early stopping\n",
      "(0.15563636363636363, 1.0156098e-06)\n",
      "(0.3407079646017699, 6.360617e-07)\n",
      "0.5998380873639244\n",
      "1317 (18070,)\n",
      "28974\n",
      "(None, 1, 1) coeff (None, 5, 30) amplitude\n",
      "Epoch 1/300\n",
      "145/145 [==============================] - 16s 55ms/step - loss: 0.8044 - acc: 0.8545 - f1_score: 0.2823 - val_loss: 1.0178 - val_acc: 0.8304 - val_f1_score: 0.1622\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.16216, saving model to ./models/lag_15_iter_0_split_2_episode_model_v9_new_validation_set_no_smoking_sparse_cross_entropy_10-100_f1_no_fpr.hdf5\n",
      "Epoch 2/300\n",
      "145/145 [==============================] - 6s 39ms/step - loss: 0.4850 - acc: 0.9369 - f1_score: 0.5862 - val_loss: 1.0505 - val_acc: 0.8359 - val_f1_score: 0.1401\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.16216\n",
      "Epoch 3/300\n",
      "145/145 [==============================] - 5s 38ms/step - loss: 0.3295 - acc: 0.9552 - f1_score: 0.7197 - val_loss: 1.0714 - val_acc: 0.8529 - val_f1_score: 0.1037\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.16216\n",
      "Epoch 4/300\n",
      "145/145 [==============================] - 6s 41ms/step - loss: 0.2920 - acc: 0.9615 - f1_score: 0.7513 - val_loss: 1.1053 - val_acc: 0.8486 - val_f1_score: 0.0158\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.16216\n",
      "Epoch 5/300\n",
      "145/145 [==============================] - 6s 45ms/step - loss: 0.2748 - acc: 0.9625 - f1_score: 0.7609 - val_loss: 1.0713 - val_acc: 0.8602 - val_f1_score: 0.0873\n",
      "\n",
      "Epoch 00005: val_f1_score did not improve from 0.16216\n",
      "Epoch 6/300\n",
      "145/145 [==============================] - 6s 44ms/step - loss: 0.2383 - acc: 0.9689 - f1_score: 0.7963 - val_loss: 1.1188 - val_acc: 0.8492 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_f1_score did not improve from 0.16216\n",
      "Epoch 7/300\n",
      "145/145 [==============================] - 6s 44ms/step - loss: 0.2047 - acc: 0.9718 - f1_score: 0.8247 - val_loss: 1.1138 - val_acc: 0.8347 - val_f1_score: 0.0933\n",
      "\n",
      "Epoch 00007: val_f1_score did not improve from 0.16216\n",
      "Epoch 8/300\n",
      "145/145 [==============================] - 6s 45ms/step - loss: 0.1894 - acc: 0.9730 - f1_score: 0.8379 - val_loss: 1.1123 - val_acc: 0.8632 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00008: val_f1_score did not improve from 0.16216\n",
      "Epoch 9/300\n",
      "145/145 [==============================] - 6s 44ms/step - loss: 0.1575 - acc: 0.9778 - f1_score: 0.8646 - val_loss: 1.1171 - val_acc: 0.8711 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00009: val_f1_score did not improve from 0.16216\n",
      "Epoch 10/300\n",
      "145/145 [==============================] - 6s 44ms/step - loss: 0.1286 - acc: 0.9820 - f1_score: 0.8890 - val_loss: 1.1963 - val_acc: 0.8243 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00010: val_f1_score did not improve from 0.16216\n",
      "Epoch 11/300\n",
      "145/145 [==============================] - 6s 44ms/step - loss: 0.1083 - acc: 0.9841 - f1_score: 0.9057 - val_loss: 1.1421 - val_acc: 0.8523 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00011: val_f1_score did not improve from 0.16216\n",
      "Epoch 12/300\n",
      "145/145 [==============================] - 6s 43ms/step - loss: 0.0853 - acc: 0.9874 - f1_score: 0.9286 - val_loss: 1.1421 - val_acc: 0.8626 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00012: val_f1_score did not improve from 0.16216\n",
      "Epoch 13/300\n",
      "145/145 [==============================] - 6s 44ms/step - loss: 0.0828 - acc: 0.9878 - f1_score: 0.9300 - val_loss: 1.1807 - val_acc: 0.8480 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00013: val_f1_score did not improve from 0.16216\n",
      "Epoch 14/300\n",
      "145/145 [==============================] - 6s 43ms/step - loss: 0.0733 - acc: 0.9890 - f1_score: 0.9386 - val_loss: 1.1177 - val_acc: 0.8626 - val_f1_score: 0.0504\n",
      "\n",
      "Epoch 00014: val_f1_score did not improve from 0.16216\n",
      "Epoch 15/300\n",
      "145/145 [==============================] - 6s 44ms/step - loss: 0.0585 - acc: 0.9908 - f1_score: 0.9496 - val_loss: 1.1391 - val_acc: 0.8736 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00015: val_f1_score did not improve from 0.16216\n",
      "Epoch 16/300\n",
      "145/145 [==============================] - 6s 39ms/step - loss: 0.0520 - acc: 0.9925 - f1_score: 0.9575 - val_loss: 1.1154 - val_acc: 0.8626 - val_f1_score: 0.0424\n",
      "\n",
      "Epoch 00016: val_f1_score did not improve from 0.16216\n",
      "Epoch 17/300\n",
      "145/145 [==============================] - 5s 38ms/step - loss: 0.0438 - acc: 0.9934 - f1_score: 0.9643 - val_loss: 1.1285 - val_acc: 0.8462 - val_f1_score: 0.1538\n",
      "\n",
      "Epoch 00017: val_f1_score did not improve from 0.16216\n",
      "Epoch 18/300\n",
      "145/145 [==============================] - 6s 41ms/step - loss: 0.0532 - acc: 0.9919 - f1_score: 0.9550 - val_loss: 1.1285 - val_acc: 0.8875 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00018: val_f1_score did not improve from 0.16216\n",
      "Epoch 19/300\n",
      "145/145 [==============================] - 6s 44ms/step - loss: 0.0444 - acc: 0.9932 - f1_score: 0.9630 - val_loss: 1.1405 - val_acc: 0.8419 - val_f1_score: 0.0511\n",
      "\n",
      "Epoch 00019: val_f1_score did not improve from 0.16216\n",
      "Epoch 20/300\n",
      "145/145 [==============================] - 5s 38ms/step - loss: 0.0461 - acc: 0.9927 - f1_score: 0.9609 - val_loss: 1.1196 - val_acc: 0.8948 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00020: val_f1_score did not improve from 0.16216\n",
      "Epoch 21/300\n",
      "145/145 [==============================] - 5s 36ms/step - loss: 0.0356 - acc: 0.9946 - f1_score: 0.9694 - val_loss: 1.1495 - val_acc: 0.8426 - val_f1_score: 0.0582\n",
      "\n",
      "Epoch 00021: val_f1_score did not improve from 0.16216\n",
      "Epoch 22/300\n",
      "145/145 [==============================] - 6s 44ms/step - loss: 0.0316 - acc: 0.9952 - f1_score: 0.9740 - val_loss: 1.1458 - val_acc: 0.8687 - val_f1_score: 0.0092\n",
      "\n",
      "Epoch 00022: val_f1_score did not improve from 0.16216\n",
      "Epoch 23/300\n",
      "145/145 [==============================] - 6s 41ms/step - loss: 0.0319 - acc: 0.9951 - f1_score: 0.9730 - val_loss: 1.0804 - val_acc: 0.8942 - val_f1_score: 0.0645\n",
      "\n",
      "Epoch 00023: val_f1_score did not improve from 0.16216\n",
      "Epoch 24/300\n",
      "145/145 [==============================] - 6s 44ms/step - loss: 0.0352 - acc: 0.9947 - f1_score: 0.9713 - val_loss: 1.1094 - val_acc: 0.8802 - val_f1_score: 0.0664\n",
      "\n",
      "Epoch 00024: val_f1_score did not improve from 0.16216\n",
      "Epoch 25/300\n",
      "145/145 [==============================] - 6s 44ms/step - loss: 0.0226 - acc: 0.9968 - f1_score: 0.9825 - val_loss: 1.1204 - val_acc: 0.9009 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00025: val_f1_score did not improve from 0.16216\n",
      "Epoch 26/300\n",
      "145/145 [==============================] - 6s 42ms/step - loss: 0.0276 - acc: 0.9955 - f1_score: 0.9754 - val_loss: 1.1491 - val_acc: 0.8827 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00026: val_f1_score did not improve from 0.16216\n",
      "Epoch 27/300\n",
      "145/145 [==============================] - 6s 38ms/step - loss: 0.0265 - acc: 0.9959 - f1_score: 0.9776 - val_loss: 1.1045 - val_acc: 0.8784 - val_f1_score: 0.0654\n",
      "\n",
      "Epoch 00027: val_f1_score did not improve from 0.16216\n",
      "Epoch 28/300\n",
      "145/145 [==============================] - 5s 38ms/step - loss: 0.0270 - acc: 0.9960 - f1_score: 0.9778 - val_loss: 1.1422 - val_acc: 0.8650 - val_f1_score: 0.0177\n",
      "\n",
      "Epoch 00028: val_f1_score did not improve from 0.16216\n",
      "Epoch 29/300\n",
      "145/145 [==============================] - 6s 42ms/step - loss: 0.0334 - acc: 0.9951 - f1_score: 0.9726 - val_loss: 1.1405 - val_acc: 0.8815 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00029: val_f1_score did not improve from 0.16216\n",
      "Epoch 30/300\n",
      "145/145 [==============================] - 6s 44ms/step - loss: 0.0207 - acc: 0.9967 - f1_score: 0.9817 - val_loss: 1.1291 - val_acc: 0.8857 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00030: val_f1_score did not improve from 0.16216\n",
      "Epoch 31/300\n",
      "145/145 [==============================] - 6s 43ms/step - loss: 0.0213 - acc: 0.9969 - f1_score: 0.9828 - val_loss: 1.1644 - val_acc: 0.8717 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00031: val_f1_score did not improve from 0.16216\n",
      "Epoch 32/300\n",
      "145/145 [==============================] - 6s 43ms/step - loss: 0.0226 - acc: 0.9960 - f1_score: 0.9779 - val_loss: 1.1156 - val_acc: 0.8729 - val_f1_score: 0.0711\n",
      "\n",
      "Epoch 00032: val_f1_score did not improve from 0.16216\n",
      "Epoch 33/300\n",
      "145/145 [==============================] - 6s 43ms/step - loss: 0.0213 - acc: 0.9965 - f1_score: 0.9802 - val_loss: 1.1355 - val_acc: 0.8590 - val_f1_score: 0.0492\n",
      "\n",
      "Epoch 00033: val_f1_score did not improve from 0.16216\n",
      "Epoch 34/300\n",
      "145/145 [==============================] - 5s 38ms/step - loss: 0.0216 - acc: 0.9968 - f1_score: 0.9829 - val_loss: 1.1177 - val_acc: 0.8717 - val_f1_score: 0.0705\n",
      "\n",
      "Epoch 00034: val_f1_score did not improve from 0.16216\n",
      "Epoch 35/300\n",
      "145/145 [==============================] - 5s 38ms/step - loss: 0.0185 - acc: 0.9970 - f1_score: 0.9834 - val_loss: 1.1423 - val_acc: 0.8839 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00035: val_f1_score did not improve from 0.16216\n",
      "Epoch 36/300\n",
      "145/145 [==============================] - 6s 42ms/step - loss: 0.0192 - acc: 0.9970 - f1_score: 0.9832 - val_loss: 1.0967 - val_acc: 0.8778 - val_f1_score: 0.0651\n",
      "\n",
      "Epoch 00036: val_f1_score did not improve from 0.16216\n",
      "Epoch 37/300\n",
      "145/145 [==============================] - 6s 44ms/step - loss: 0.0149 - acc: 0.9978 - f1_score: 0.9880 - val_loss: 1.1238 - val_acc: 0.8711 - val_f1_score: 0.0450\n",
      "\n",
      "Epoch 00037: val_f1_score did not improve from 0.16216\n",
      "Epoch 38/300\n",
      "145/145 [==============================] - 6s 42ms/step - loss: 0.0141 - acc: 0.9978 - f1_score: 0.9876 - val_loss: 1.1418 - val_acc: 0.8608 - val_f1_score: 0.0729\n",
      "\n",
      "Epoch 00038: val_f1_score did not improve from 0.16216\n",
      "Epoch 39/300\n",
      "145/145 [==============================] - 5s 38ms/step - loss: 0.0204 - acc: 0.9967 - f1_score: 0.9820 - val_loss: 1.1445 - val_acc: 0.8638 - val_f1_score: 0.0175\n",
      "\n",
      "Epoch 00039: val_f1_score did not improve from 0.16216\n",
      "Epoch 40/300\n",
      "145/145 [==============================] - 5s 38ms/step - loss: 0.0153 - acc: 0.9976 - f1_score: 0.9865 - val_loss: 1.1750 - val_acc: 0.8553 - val_f1_score: 0.0246\n",
      "\n",
      "Epoch 00040: val_f1_score did not improve from 0.16216\n",
      "Epoch 41/300\n",
      "145/145 [==============================] - 6s 43ms/step - loss: 0.0213 - acc: 0.9967 - f1_score: 0.9818 - val_loss: 1.1522 - val_acc: 0.8833 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00041: val_f1_score did not improve from 0.16216\n",
      "Epoch 00041: early stopping\n",
      "(0.19228336495888676, 0.0009758562)\n",
      "(0.16550241805480925, 0.0011948596)\n",
      "0.5965859680093282\n",
      "1329 (16957,)\n",
      "29238\n",
      "(None, 1, 1) coeff (None, 5, 30) amplitude\n",
      "Epoch 1/300\n",
      "147/147 [==============================] - 16s 56ms/step - loss: 0.7992 - acc: 0.7894 - f1_score: 0.2851 - val_loss: 1.0496 - val_acc: 0.8617 - val_f1_score: 0.0360\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.03597, saving model to ./models/lag_15_iter_0_split_3_episode_model_v9_new_validation_set_no_smoking_sparse_cross_entropy_10-100_f1_no_fpr.hdf5\n",
      "Epoch 2/300\n",
      "147/147 [==============================] - 6s 43ms/step - loss: 0.3927 - acc: 0.9475 - f1_score: 0.6764 - val_loss: 1.0328 - val_acc: 0.8467 - val_f1_score: 0.0748\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.03597 to 0.07477, saving model to ./models/lag_15_iter_0_split_3_episode_model_v9_new_validation_set_no_smoking_sparse_cross_entropy_10-100_f1_no_fpr.hdf5\n",
      "Epoch 3/300\n",
      "147/147 [==============================] - 6s 44ms/step - loss: 0.2653 - acc: 0.9623 - f1_score: 0.7760 - val_loss: 0.9973 - val_acc: 0.8627 - val_f1_score: 0.1582\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.07477 to 0.15823, saving model to ./models/lag_15_iter_0_split_3_episode_model_v9_new_validation_set_no_smoking_sparse_cross_entropy_10-100_f1_no_fpr.hdf5\n",
      "Epoch 4/300\n",
      "147/147 [==============================] - 6s 43ms/step - loss: 0.2050 - acc: 0.9715 - f1_score: 0.8278 - val_loss: 1.0294 - val_acc: 0.8885 - val_f1_score: 0.1148\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.15823\n",
      "Epoch 5/300\n",
      "147/147 [==============================] - 6s 44ms/step - loss: 0.1577 - acc: 0.9769 - f1_score: 0.8644 - val_loss: 0.9634 - val_acc: 0.8772 - val_f1_score: 0.2119\n",
      "\n",
      "Epoch 00005: val_f1_score improved from 0.15823 to 0.21192, saving model to ./models/lag_15_iter_0_split_3_episode_model_v9_new_validation_set_no_smoking_sparse_cross_entropy_10-100_f1_no_fpr.hdf5\n",
      "Epoch 6/300\n",
      "147/147 [==============================] - 6s 43ms/step - loss: 0.1311 - acc: 0.9813 - f1_score: 0.8883 - val_loss: 1.0412 - val_acc: 0.8638 - val_f1_score: 0.1141\n",
      "\n",
      "Epoch 00006: val_f1_score did not improve from 0.21192\n",
      "Epoch 7/300\n",
      "147/147 [==============================] - 6s 44ms/step - loss: 0.1199 - acc: 0.9822 - f1_score: 0.8991 - val_loss: 0.9509 - val_acc: 0.8720 - val_f1_score: 0.2298\n",
      "\n",
      "Epoch 00007: val_f1_score improved from 0.21192 to 0.22981, saving model to ./models/lag_15_iter_0_split_3_episode_model_v9_new_validation_set_no_smoking_sparse_cross_entropy_10-100_f1_no_fpr.hdf5\n",
      "Epoch 8/300\n",
      "147/147 [==============================] - 6s 43ms/step - loss: 0.1081 - acc: 0.9848 - f1_score: 0.9120 - val_loss: 1.0219 - val_acc: 0.8731 - val_f1_score: 0.1575\n",
      "\n",
      "Epoch 00008: val_f1_score did not improve from 0.22981\n",
      "Epoch 9/300\n",
      "147/147 [==============================] - 6s 44ms/step - loss: 0.0937 - acc: 0.9863 - f1_score: 0.9221 - val_loss: 1.0655 - val_acc: 0.8839 - val_f1_score: 0.0426\n",
      "\n",
      "Epoch 00009: val_f1_score did not improve from 0.22981\n",
      "Epoch 10/300\n",
      "147/147 [==============================] - 6s 44ms/step - loss: 0.1074 - acc: 0.9846 - f1_score: 0.9096 - val_loss: 1.0924 - val_acc: 0.8529 - val_f1_score: 0.0836\n",
      "\n",
      "Epoch 00010: val_f1_score did not improve from 0.22981\n",
      "Epoch 11/300\n",
      "147/147 [==============================] - 6s 44ms/step - loss: 0.0807 - acc: 0.9879 - f1_score: 0.9336 - val_loss: 1.0723 - val_acc: 0.8715 - val_f1_score: 0.1263\n",
      "\n",
      "Epoch 00011: val_f1_score did not improve from 0.22981\n",
      "Epoch 12/300\n",
      "147/147 [==============================] - 6s 44ms/step - loss: 0.0696 - acc: 0.9893 - f1_score: 0.9412 - val_loss: 1.0771 - val_acc: 0.8664 - val_f1_score: 0.0848\n",
      "\n",
      "Epoch 00012: val_f1_score did not improve from 0.22981\n",
      "Epoch 13/300\n",
      "147/147 [==============================] - 6s 44ms/step - loss: 0.0701 - acc: 0.9894 - f1_score: 0.9409 - val_loss: 1.0378 - val_acc: 0.8767 - val_f1_score: 0.1787\n",
      "\n",
      "Epoch 00013: val_f1_score did not improve from 0.22981\n",
      "Epoch 14/300\n",
      "147/147 [==============================] - 6s 44ms/step - loss: 0.0639 - acc: 0.9900 - f1_score: 0.9453 - val_loss: 1.0184 - val_acc: 0.8937 - val_f1_score: 0.1890\n",
      "\n",
      "Epoch 00014: val_f1_score did not improve from 0.22981\n",
      "Epoch 15/300\n",
      "147/147 [==============================] - 6s 44ms/step - loss: 0.0542 - acc: 0.9916 - f1_score: 0.9534 - val_loss: 1.0505 - val_acc: 0.8689 - val_f1_score: 0.1589\n",
      "\n",
      "Epoch 00015: val_f1_score did not improve from 0.22981\n",
      "Epoch 16/300\n",
      " 71/147 [=============>................] - ETA: 3s - loss: 0.0685 - acc: 0.9896 - f1_score: 0.9407"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "use_standardization = True\n",
    "n_lag = 15\n",
    "n_groups = 10\n",
    "X_feature,X_static,X_stress_episode,X_quit_episode,X_activity_episode,X_smoking_episode,y_time,y,groups = get_X_y_groups(n_lag)\n",
    "y[y<0] = 0\n",
    "y = np.float32(y)\n",
    "indexes = get_train_test_indexes(groups,n_groups_split = n_groups,n_val_groups=5)\n",
    "final_y_time = []\n",
    "final_probs = []\n",
    "final_y = []\n",
    "final_groups = []\n",
    "bias_dict = {}\n",
    "val_results = {}\n",
    "for kk,yyyy in enumerate(indexes):\n",
    "    train_index,test_index,val_index = yyyy\n",
    "    \n",
    "    X_feature_train,X_feature_test = X_feature[train_index],X_feature[test_index]\n",
    "    X_static_train,X_static_test = X_static[train_index],X_static[test_index]\n",
    "    X_stress_episode_train,X_stress_episode_test = X_stress_episode[train_index], X_stress_episode[test_index]\n",
    "    X_quit_episode_train,X_quit_episode_test = X_quit_episode[train_index], X_quit_episode[test_index]\n",
    "    X_activity_episode_train,X_activity_episode_test = X_activity_episode[train_index], X_activity_episode[test_index]\n",
    "    X_smoking_episode_train,X_smoking_episode_test = X_smoking_episode[train_index], X_smoking_episode[test_index]\n",
    "    y_train,y_test,groups_train,groups_test,time_train,time_test = y[train_index],y[test_index],groups[train_index],groups[test_index],y_time[train_index],y_time[test_index]\n",
    "    \n",
    "    X_feature_val,X_static_val,X_stress_episode_val,X_quit_episode_val,\\\n",
    "    X_activity_episode_val,X_smoking_episode_val,y_val,groups_val,time_val = X_feature[val_index],X_static[val_index],X_stress_episode[val_index],X_quit_episode[val_index],\\\n",
    "                                                                            X_activity_episode[val_index],X_smoking_episode[val_index],y[val_index],groups[val_index],y_time[val_index]\n",
    "    \n",
    "    positive_train_index = np.where(y_train==1)[0]\n",
    "    negative_train_index = np.where(y_train==0)[0]\n",
    "    \n",
    "    len_positive = len(positive_train_index)\n",
    "    print(len_positive,y_train.shape)\n",
    "    n_iters = 0\n",
    "    test_preds = []\n",
    "    bias_pred = []\n",
    "    # for i,n_iter in enumerate(range(n_iters)):\n",
    "    #     np.random.seed(np.random.randint(109))\n",
    "    indexes_sampled = np.array(list(positive_train_index)*2+list(np.random.choice(negative_train_index,len_positive*20)))\n",
    "    print(len(indexes_sampled))\n",
    "    np.random.shuffle(indexes_sampled)\n",
    "    train_feature = X_feature_train[indexes_sampled]\n",
    "    train_static = X_static_train[indexes_sampled]\n",
    "    train_stress = X_stress_episode_train[indexes_sampled]\n",
    "    train_quit = X_quit_episode_train[indexes_sampled]\n",
    "    train_activity = X_activity_episode_train[indexes_sampled]\n",
    "    train_smoking = X_smoking_episode_train[indexes_sampled]\n",
    "    train_y = y_train[indexes_sampled]\n",
    "    model = get_model()\n",
    "#         model.summary()\n",
    "    filepath = './models/lag_'+str(n_lag)+'_iter_'+str(n_iters)+'_split_'+str(kk)+'_episode_model_v9_new_validation_set_no_smoking_sparse_cross_entropy_10-100_f1_no_fpr.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_f1_score', verbose=1, save_best_only=True, mode='max',save_weights_only=False)\n",
    "    es = EarlyStopping(monitor='val_f1_score', mode='max', verbose=1,patience=40)\n",
    "    callbacks_list = [es,checkpoint]\n",
    "    # train_feature,val_feature,train_static,val_static,train_stress,val_stress, \\\n",
    "    # train_smoking,val_smoking,train_quit,val_quit,train_activity,val_activity, \\\n",
    "    # train_y,val_y = train_test_split(train_feature,\n",
    "    #                                     train_static,\n",
    "    #                                     train_stress,\n",
    "    #                                     train_smoking,\n",
    "    #                                     train_quit,\n",
    "    #                                     train_activity,\n",
    "    #                                     train_y,\n",
    "    #                                     test_size=.2,stratify=train_y)\n",
    "    model.fit([train_feature,train_static,train_stress,train_activity,train_smoking,train_quit],train_y,\n",
    "        validation_data=([X_feature_val,X_static_val,X_stress_episode_val,X_activity_episode_val,\n",
    "                                        X_smoking_episode_val,X_quit_episode_val],y_val), epochs=300, batch_size=200,\n",
    "                verbose=1,callbacks=callbacks_list,shuffle=True)\n",
    "    model.load_weights(filepath)\n",
    "    test_preds.append(model.predict([X_feature_test,X_static_test,X_stress_episode_test,\n",
    "                                    X_activity_episode_test,X_smoking_episode_test,X_quit_episode_test]))\n",
    "    bias_pred.append(model.predict([X_feature_val,X_static_val,X_stress_episode_val,X_activity_episode_val,\n",
    "                                    X_smoking_episode_val,X_quit_episode_val]))\n",
    "        \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    y_test_pred = np.concatenate([MinMaxScaler().fit_transform(a.reshape(-1,1)) for a in test_preds],axis=1).mean(axis=1)\n",
    "    bias_pred = np.concatenate([MinMaxScaler().fit_transform(a.reshape(-1,1)) for a in bias_pred],axis=1).mean(axis=1)\n",
    "    print(f1Bias_scorer_CV(bias_pred,y_val))\n",
    "    print(f1Bias_scorer_CV(y_test_pred,y_test))\n",
    "    print(roc_auc_score(y_test,y_test_pred))\n",
    "    final_y_time.extend(list(time_test))\n",
    "    final_probs.extend(list(y_test_pred))\n",
    "    final_y.extend(list(y_test))\n",
    "    final_groups.extend(list(groups_test))\n",
    "    for group_b in np.unique(groups_test):\n",
    "        bias_dict[group_b] = kk\n",
    "    val_results[kk] = [time_val,bias_pred,y_val,groups_val]\n",
    "#     print(len(np.unique(final_groups)))\n",
    "#     print(bias_dict)\n",
    "final_y_time,final_probs,final_y,final_groups = np.array(final_y_time),np.array(final_probs),np.array(final_y),np.array(final_groups)\n",
    "pickle.dump([final_y_time,final_probs,final_y,final_groups,bias_dict,val_results],open('./data/output/episode_encoded_lag_'+str(n_lag)+'_episode_model_v9_new_validation_set_no_smoking_sparse_cross_entropy_10-100_f1_no_fpr.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model.load_weights(filepath)\n",
    "# # model.summary()\n",
    "# # plt.hist(np.array(final_y)-np.array(final_probs))\n",
    "# metric = tfa.metrics.F1Score(num_classes=3, average='micro',threshold=0.5)\n",
    "# y_true = np.array([0,\n",
    "#                     1,\n",
    "#                     0], np.int32).reshape(-1,1)\n",
    "# y_pred = np.array([.3,\n",
    "#                     .51.0+,\n",
    "#                     .3], np.float32).reshape(-1,1)\n",
    "# metric.update_state(y_true, y_pred)\n",
    "# result = metric.result()\n",
    "# result.numpy()\n",
    "test_preds = []\n",
    "test_preds.append(model.predict([X_feature_test,X_static_test,X_stress_episode_test,\n",
    "                                    X_activity_episode_test,X_smoking_episode_test,X_quit_episode_test]))\n",
    "# bias_pred.append(model.predict([X_feature_val,X_static_val,X_stress_episode_val,X_activity_episode_val,\n",
    "#                                 X_smoking_episode_val,X_quit_episode_val]))\n",
    "    \n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# y_test_pred = np.concatenate([MinMaxScaler().fit_transform(a.reshape(-1,1)) for a in test_preds],axis=1).mean(axis=1)\n",
    "# bias_pred = np.concatenate([MinMaxScaler().fit_transform(a.reshape(-1,1)) for a in bias_pred],axis=1).mean(axis=1)\n",
    "# print(roc_auc_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.037e+03, 3.000e+00, 1.000e+00, 0.000e+00, 2.000e+00, 1.000e+00,\n",
       "        0.000e+00, 2.000e+00, 3.000e+00, 4.100e+01]),\n",
       " array([1.8653691e-07, 9.9999368e-02, 1.9999856e-01, 2.9999775e-01,\n",
       "        3.9999691e-01, 4.9999610e-01, 5.9999526e-01, 6.9999444e-01,\n",
       "        7.9999363e-01, 8.9999282e-01, 9.9999201e-01], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(test_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model(model.input,model.get_layer('dense_552').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.predict([X_feature_test,X_static_test,X_stress_episode_test,\n",
    "                                       X_activity_episode_test,X_smoking_episode_test,X_quit_episode_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_static_test[0],X_static_test[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = './models/episode_model.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='min',save_weights_only=False)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=0,patience=40)\n",
    "callbacks_list = [es,checkpoint]\n",
    "train_feature,val_feature,train_static,val_static,train_stress,val_stress, \\\n",
    "train_smoking,val_smoking,train_quit,val_quit,train_activity,val_activity, \\\n",
    "train_y,val_y = train_test_split(train_feature,\n",
    "                                 train_static,\n",
    "                                 train_stress,\n",
    "                                 train_smoking,\n",
    "                                 train_quit,\n",
    "                                 train_activity,\n",
    "                                 train_y,\n",
    "                                 test_size=.1,stratify=train_y)\n",
    "# train_y = tf.cast(train_y, tf.float32)\n",
    "# train_x = tf.cast(train_x, tf.float32)\n",
    "# val_y = tf.cast(val_y, tf.float32)\n",
    "# val_x = tf.cast(val_x, tf.float32)\n",
    "# history = model.fit(train_x,train_y,validation_data=(val_x,val_y), epochs=200, batch_size=30,\n",
    "#                     verbose=0,callbacks=callbacks_list,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([train_feature,train_static,train_stress,train_activity,train_smoking,train_quit],train_y,\n",
    "          validation_data=([val_feature,val_static,val_stress,val_activity,val_smoking,val_quit],val_y), epochs=200, batch_size=30,\n",
    "                    verbose=1,callbacks=callbacks_list,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,\n",
    "               np.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(final_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "filepath = './models/'+'-'.join([str(n_lag),str(n_groups)])+'-'+str(uuid.uuid4())+'.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min',save_weights_only=False)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=0,patience=40)\n",
    "callbacks_list = [es,checkpoint]\n",
    "train_x,val_x,train_y,val_y = train_test_split(X_train,y_train,test_size=.2,stratify=y_train)\n",
    "history = model.fit(train_x,train_y,validation_data=(val_x,val_y), epochs=100, batch_size=100,verbose=1,callbacks=callbacks_list,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc,roc_auc_score,f1_score\n",
    "roc_auc_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.Variable(np.arange(10)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51b244ab9aca612e739a0539ae1af887c58db9e180d786deb0ab1761def69c1f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('test1': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
