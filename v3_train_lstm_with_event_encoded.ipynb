{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Physical GPUs, 1 Logical GPU\n",
      "15 30 4\n",
      "[0 1]\n",
      "15 68\n",
      "(None, 1, 1) coeff (None, 10, 30) amplitude\n",
      "Epoch 1/200\n",
      "76/76 [==============================] - 4s 14ms/step - loss: 8.7965 - softmax_loss: 0.7097 - normalize_loss: 0.8497 - normalize3_loss: 0.8497 - softmax_acc: 0.5218\n",
      "Epoch 2/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 8.1083 - softmax_loss: 0.6838 - normalize_loss: 0.6351 - normalize3_loss: 0.6351 - softmax_acc: 0.5592\n",
      "Epoch 3/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.8628 - softmax_loss: 0.6886 - normalize_loss: 0.4886 - normalize3_loss: 0.4886 - softmax_acc: 0.5576\n",
      "Epoch 4/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.7972 - softmax_loss: 0.6889 - normalize_loss: 0.4539 - normalize3_loss: 0.4539 - softmax_acc: 0.5610\n",
      "Epoch 5/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.7853 - softmax_loss: 0.6906 - normalize_loss: 0.4396 - normalize3_loss: 0.4396 - softmax_acc: 0.5573\n",
      "Epoch 6/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.7672 - softmax_loss: 0.6886 - normalize_loss: 0.4408 - normalize3_loss: 0.4408 - softmax_acc: 0.5727\n",
      "Epoch 7/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.7535 - softmax_loss: 0.6886 - normalize_loss: 0.4337 - normalize3_loss: 0.4337 - softmax_acc: 0.5587\n",
      "Epoch 8/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.7629 - softmax_loss: 0.6910 - normalize_loss: 0.4265 - normalize3_loss: 0.4265 - softmax_acc: 0.5351\n",
      "Epoch 9/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.7370 - softmax_loss: 0.6880 - normalize_loss: 0.4284 - normalize3_loss: 0.4284 - softmax_acc: 0.5505\n",
      "Epoch 10/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.7364 - softmax_loss: 0.6885 - normalize_loss: 0.4259 - normalize3_loss: 0.4259 - softmax_acc: 0.5583\n",
      "Epoch 11/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.7281 - softmax_loss: 0.6877 - normalize_loss: 0.4258 - normalize3_loss: 0.4258 - softmax_acc: 0.5627\n",
      "Epoch 12/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.7403 - softmax_loss: 0.6883 - normalize_loss: 0.4288 - normalize3_loss: 0.4288 - softmax_acc: 0.5607\n",
      "Epoch 13/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.7142 - softmax_loss: 0.6857 - normalize_loss: 0.4286 - normalize3_loss: 0.4286 - softmax_acc: 0.5782\n",
      "Epoch 14/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.7165 - softmax_loss: 0.6876 - normalize_loss: 0.4201 - normalize3_loss: 0.4201 - softmax_acc: 0.5531\n",
      "Epoch 15/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.7003 - softmax_loss: 0.6859 - normalize_loss: 0.4207 - normalize3_loss: 0.4207 - softmax_acc: 0.5376\n",
      "Epoch 16/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.6944 - softmax_loss: 0.6855 - normalize_loss: 0.4196 - normalize3_loss: 0.4196 - softmax_acc: 0.5458\n",
      "Epoch 17/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.6720 - softmax_loss: 0.6831 - normalize_loss: 0.4205 - normalize3_loss: 0.4205 - softmax_acc: 0.5581\n",
      "Epoch 18/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.6922 - softmax_loss: 0.6833 - normalize_loss: 0.4298 - normalize3_loss: 0.4298 - softmax_acc: 0.5743\n",
      "Epoch 19/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.6503 - softmax_loss: 0.6790 - normalize_loss: 0.4302 - normalize3_loss: 0.4302 - softmax_acc: 0.5844\n",
      "Epoch 20/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.6529 - softmax_loss: 0.6792 - normalize_loss: 0.4306 - normalize3_loss: 0.4306 - softmax_acc: 0.5707\n",
      "Epoch 21/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.6390 - softmax_loss: 0.6778 - normalize_loss: 0.4307 - normalize3_loss: 0.4307 - softmax_acc: 0.5972\n",
      "Epoch 22/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.6183 - softmax_loss: 0.6786 - normalize_loss: 0.4163 - normalize3_loss: 0.4163 - softmax_acc: 0.5604\n",
      "Epoch 23/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.6369 - softmax_loss: 0.6790 - normalize_loss: 0.4234 - normalize3_loss: 0.4234 - softmax_acc: 0.5728\n",
      "Epoch 24/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.6283 - softmax_loss: 0.6782 - normalize_loss: 0.4230 - normalize3_loss: 0.4230 - softmax_acc: 0.5811\n",
      "Epoch 25/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.5792 - softmax_loss: 0.6738 - normalize_loss: 0.4204 - normalize3_loss: 0.4204 - softmax_acc: 0.5763\n",
      "Epoch 26/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.6443 - softmax_loss: 0.6800 - normalize_loss: 0.4221 - normalize3_loss: 0.4221 - softmax_acc: 0.5862\n",
      "Epoch 27/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.5570 - softmax_loss: 0.6713 - normalize_loss: 0.4222 - normalize3_loss: 0.4222 - softmax_acc: 0.5687\n",
      "Epoch 28/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.5991 - softmax_loss: 0.6751 - normalize_loss: 0.4241 - normalize3_loss: 0.4241 - softmax_acc: 0.5755\n",
      "Epoch 29/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.5708 - softmax_loss: 0.6721 - normalize_loss: 0.4249 - normalize3_loss: 0.4249 - softmax_acc: 0.5856\n",
      "Epoch 30/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.5939 - softmax_loss: 0.6751 - normalize_loss: 0.4212 - normalize3_loss: 0.4212 - softmax_acc: 0.5700\n",
      "Epoch 31/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.5826 - softmax_loss: 0.6720 - normalize_loss: 0.4315 - normalize3_loss: 0.4315 - softmax_acc: 0.5802\n",
      "Epoch 32/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.5741 - softmax_loss: 0.6725 - normalize_loss: 0.4245 - normalize3_loss: 0.4245 - softmax_acc: 0.5727\n",
      "Epoch 33/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.5117 - softmax_loss: 0.6654 - normalize_loss: 0.4290 - normalize3_loss: 0.4290 - softmax_acc: 0.5999\n",
      "Epoch 34/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.5490 - softmax_loss: 0.6683 - normalize_loss: 0.4331 - normalize3_loss: 0.4331 - softmax_acc: 0.5956\n",
      "Epoch 35/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.5431 - softmax_loss: 0.6680 - normalize_loss: 0.4317 - normalize3_loss: 0.4317 - softmax_acc: 0.5948\n",
      "Epoch 36/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.5684 - softmax_loss: 0.6689 - normalize_loss: 0.4396 - normalize3_loss: 0.4396 - softmax_acc: 0.5974\n",
      "Epoch 37/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.5458 - softmax_loss: 0.6678 - normalize_loss: 0.4338 - normalize3_loss: 0.4338 - softmax_acc: 0.5859\n",
      "Epoch 38/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.5159 - softmax_loss: 0.6646 - normalize_loss: 0.4350 - normalize3_loss: 0.4350 - softmax_acc: 0.5967\n",
      "Epoch 39/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.4284 - softmax_loss: 0.6553 - normalize_loss: 0.4376 - normalize3_loss: 0.4376 - softmax_acc: 0.6092\n",
      "Epoch 40/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.3832 - softmax_loss: 0.6516 - normalize_loss: 0.4334 - normalize3_loss: 0.4334 - softmax_acc: 0.6122\n",
      "Epoch 41/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.4284 - softmax_loss: 0.6562 - normalize_loss: 0.4331 - normalize3_loss: 0.4331 - softmax_acc: 0.6077\n",
      "Epoch 42/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.5083 - softmax_loss: 0.6599 - normalize_loss: 0.4545 - normalize3_loss: 0.4545 - softmax_acc: 0.5978\n",
      "Epoch 43/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.4505 - softmax_loss: 0.6584 - normalize_loss: 0.4334 - normalize3_loss: 0.4334 - softmax_acc: 0.6059\n",
      "Epoch 44/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.4259 - softmax_loss: 0.6530 - normalize_loss: 0.4478 - normalize3_loss: 0.4478 - softmax_acc: 0.6025\n",
      "Epoch 45/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.3804 - softmax_loss: 0.6500 - normalize_loss: 0.4401 - normalize3_loss: 0.4401 - softmax_acc: 0.6144\n",
      "Epoch 46/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.3884 - softmax_loss: 0.6508 - normalize_loss: 0.4400 - normalize3_loss: 0.4400 - softmax_acc: 0.6082\n",
      "Epoch 47/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.4372 - softmax_loss: 0.6564 - normalize_loss: 0.4365 - normalize3_loss: 0.4365 - softmax_acc: 0.5941\n",
      "Epoch 48/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.3713 - softmax_loss: 0.6477 - normalize_loss: 0.4470 - normalize3_loss: 0.4470 - softmax_acc: 0.6166\n",
      "Epoch 49/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.3622 - softmax_loss: 0.6492 - normalize_loss: 0.4352 - normalize3_loss: 0.4352 - softmax_acc: 0.6119\n",
      "Epoch 50/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.3390 - softmax_loss: 0.6431 - normalize_loss: 0.4541 - normalize3_loss: 0.4541 - softmax_acc: 0.6412\n",
      "Epoch 51/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.3533 - softmax_loss: 0.6468 - normalize_loss: 0.4427 - normalize3_loss: 0.4427 - softmax_acc: 0.6212\n",
      "Epoch 52/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.4243 - softmax_loss: 0.6504 - normalize_loss: 0.4602 - normalize3_loss: 0.4602 - softmax_acc: 0.6149\n",
      "Epoch 53/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.3565 - softmax_loss: 0.6464 - normalize_loss: 0.4465 - normalize3_loss: 0.4465 - softmax_acc: 0.6186\n",
      "Epoch 54/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.3070 - softmax_loss: 0.6401 - normalize_loss: 0.4530 - normalize3_loss: 0.4530 - softmax_acc: 0.6254\n",
      "Epoch 55/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.2934 - softmax_loss: 0.6390 - normalize_loss: 0.4515 - normalize3_loss: 0.4515 - softmax_acc: 0.6263\n",
      "Epoch 56/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.3019 - softmax_loss: 0.6410 - normalize_loss: 0.4462 - normalize3_loss: 0.4462 - softmax_acc: 0.6230\n",
      "Epoch 57/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.3106 - softmax_loss: 0.6408 - normalize_loss: 0.4513 - normalize3_loss: 0.4513 - softmax_acc: 0.6259\n",
      "Epoch 58/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.2930 - softmax_loss: 0.6396 - normalize_loss: 0.4483 - normalize3_loss: 0.4483 - softmax_acc: 0.6180\n",
      "Epoch 59/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.3363 - softmax_loss: 0.6431 - normalize_loss: 0.4524 - normalize3_loss: 0.4524 - softmax_acc: 0.6238\n",
      "Epoch 60/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.2631 - softmax_loss: 0.6365 - normalize_loss: 0.4489 - normalize3_loss: 0.4489 - softmax_acc: 0.6332\n",
      "Epoch 61/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.2857 - softmax_loss: 0.6391 - normalize_loss: 0.4475 - normalize3_loss: 0.4475 - softmax_acc: 0.6206\n",
      "Epoch 62/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.2664 - softmax_loss: 0.6358 - normalize_loss: 0.4544 - normalize3_loss: 0.4544 - softmax_acc: 0.6247\n",
      "Epoch 63/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.2553 - softmax_loss: 0.6347 - normalize_loss: 0.4542 - normalize3_loss: 0.4542 - softmax_acc: 0.6264\n",
      "Epoch 64/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.2430 - softmax_loss: 0.6335 - normalize_loss: 0.4538 - normalize3_loss: 0.4538 - softmax_acc: 0.6280\n",
      "Epoch 65/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.2260 - softmax_loss: 0.6306 - normalize_loss: 0.4601 - normalize3_loss: 0.4601 - softmax_acc: 0.6238\n",
      "Epoch 66/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.2167 - softmax_loss: 0.6311 - normalize_loss: 0.4530 - normalize3_loss: 0.4530 - softmax_acc: 0.6247\n",
      "Epoch 67/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.2168 - softmax_loss: 0.6309 - normalize_loss: 0.4538 - normalize3_loss: 0.4538 - softmax_acc: 0.6357\n",
      "Epoch 68/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.1989 - softmax_loss: 0.6293 - normalize_loss: 0.4528 - normalize3_loss: 0.4528 - softmax_acc: 0.6370\n",
      "Epoch 69/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.1733 - softmax_loss: 0.6272 - normalize_loss: 0.4505 - normalize3_loss: 0.4505 - softmax_acc: 0.6350\n",
      "Epoch 70/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.0830 - softmax_loss: 0.6194 - normalize_loss: 0.4447 - normalize3_loss: 0.4447 - softmax_acc: 0.6338\n",
      "Epoch 71/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.1230 - softmax_loss: 0.6224 - normalize_loss: 0.4497 - normalize3_loss: 0.4497 - softmax_acc: 0.6376\n",
      "Epoch 72/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.1970 - softmax_loss: 0.6314 - normalize_loss: 0.4415 - normalize3_loss: 0.4415 - softmax_acc: 0.6201\n",
      "Epoch 73/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.1733 - softmax_loss: 0.6283 - normalize_loss: 0.4454 - normalize3_loss: 0.4454 - softmax_acc: 0.6145\n",
      "Epoch 74/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.1313 - softmax_loss: 0.6226 - normalize_loss: 0.4525 - normalize3_loss: 0.4525 - softmax_acc: 0.6270\n",
      "Epoch 75/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.1564 - softmax_loss: 0.6273 - normalize_loss: 0.4417 - normalize3_loss: 0.4417 - softmax_acc: 0.6256\n",
      "Epoch 76/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.0666 - softmax_loss: 0.6182 - normalize_loss: 0.4421 - normalize3_loss: 0.4421 - softmax_acc: 0.6413\n",
      "Epoch 77/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.0941 - softmax_loss: 0.6208 - normalize_loss: 0.4432 - normalize3_loss: 0.4432 - softmax_acc: 0.6340\n",
      "Epoch 78/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.1323 - softmax_loss: 0.6254 - normalize_loss: 0.4391 - normalize3_loss: 0.4391 - softmax_acc: 0.6371\n",
      "Epoch 79/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.0825 - softmax_loss: 0.6186 - normalize_loss: 0.4481 - normalize3_loss: 0.4481 - softmax_acc: 0.6332\n",
      "Epoch 80/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.0416 - softmax_loss: 0.6158 - normalize_loss: 0.4419 - normalize3_loss: 0.4419 - softmax_acc: 0.6393\n",
      "Epoch 81/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.1933 - softmax_loss: 0.6305 - normalize_loss: 0.4441 - normalize3_loss: 0.4441 - softmax_acc: 0.6270\n",
      "Epoch 82/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.0371 - softmax_loss: 0.6157 - normalize_loss: 0.4399 - normalize3_loss: 0.4399 - softmax_acc: 0.6366\n",
      "Epoch 83/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.0976 - softmax_loss: 0.6216 - normalize_loss: 0.4408 - normalize3_loss: 0.4408 - softmax_acc: 0.6322\n",
      "Epoch 84/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.0613 - softmax_loss: 0.6177 - normalize_loss: 0.4423 - normalize3_loss: 0.4423 - softmax_acc: 0.6418\n",
      "Epoch 85/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.0842 - softmax_loss: 0.6203 - normalize_loss: 0.4405 - normalize3_loss: 0.4405 - softmax_acc: 0.6329\n",
      "Epoch 86/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 6.9971 - softmax_loss: 0.6100 - normalize_loss: 0.4485 - normalize3_loss: 0.4485 - softmax_acc: 0.6497\n",
      "Epoch 87/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.1270 - softmax_loss: 0.6243 - normalize_loss: 0.4420 - normalize3_loss: 0.4420 - softmax_acc: 0.6263\n",
      "Epoch 88/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.0163 - softmax_loss: 0.6137 - normalize_loss: 0.4396 - normalize3_loss: 0.4396 - softmax_acc: 0.6369\n",
      "Epoch 89/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.0034 - softmax_loss: 0.6114 - normalize_loss: 0.4448 - normalize3_loss: 0.4448 - softmax_acc: 0.6487\n",
      "Epoch 90/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.0460 - softmax_loss: 0.6179 - normalize_loss: 0.4337 - normalize3_loss: 0.4337 - softmax_acc: 0.6374\n",
      "Epoch 91/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.0645 - softmax_loss: 0.6178 - normalize_loss: 0.4431 - normalize3_loss: 0.4431 - softmax_acc: 0.6389\n",
      "Epoch 92/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.0596 - softmax_loss: 0.6188 - normalize_loss: 0.4359 - normalize3_loss: 0.4359 - softmax_acc: 0.6400\n",
      "Epoch 93/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 6.9815 - softmax_loss: 0.6086 - normalize_loss: 0.4476 - normalize3_loss: 0.4476 - softmax_acc: 0.6497\n",
      "Epoch 94/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.1027 - softmax_loss: 0.6224 - normalize_loss: 0.4396 - normalize3_loss: 0.4396 - softmax_acc: 0.6318\n",
      "Epoch 95/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.0130 - softmax_loss: 0.6118 - normalize_loss: 0.4474 - normalize3_loss: 0.4474 - softmax_acc: 0.6394\n",
      "Epoch 96/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 6.9191 - softmax_loss: 0.6043 - normalize_loss: 0.4382 - normalize3_loss: 0.4382 - softmax_acc: 0.6557\n",
      "Epoch 97/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 6.9861 - softmax_loss: 0.6104 - normalize_loss: 0.4409 - normalize3_loss: 0.4409 - softmax_acc: 0.6557\n",
      "Epoch 98/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 6.9960 - softmax_loss: 0.6123 - normalize_loss: 0.4363 - normalize3_loss: 0.4363 - softmax_acc: 0.6532\n",
      "Epoch 99/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 6.9165 - softmax_loss: 0.6032 - normalize_loss: 0.4421 - normalize3_loss: 0.4421 - softmax_acc: 0.6589\n",
      "Epoch 100/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.1460 - softmax_loss: 0.6265 - normalize_loss: 0.4407 - normalize3_loss: 0.4407 - softmax_acc: 0.6313\n",
      "Epoch 101/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.0031 - softmax_loss: 0.6127 - normalize_loss: 0.4380 - normalize3_loss: 0.4380 - softmax_acc: 0.6436\n",
      "Epoch 102/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 6.9852 - softmax_loss: 0.6099 - normalize_loss: 0.4429 - normalize3_loss: 0.4429 - softmax_acc: 0.6525\n",
      "Epoch 103/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 6.8954 - softmax_loss: 0.6012 - normalize_loss: 0.4415 - normalize3_loss: 0.4415 - softmax_acc: 0.6699\n",
      "Epoch 104/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 6.8949 - softmax_loss: 0.6015 - normalize_loss: 0.4398 - normalize3_loss: 0.4398 - softmax_acc: 0.6605\n",
      "Epoch 105/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 6.9236 - softmax_loss: 0.6035 - normalize_loss: 0.4445 - normalize3_loss: 0.4445 - softmax_acc: 0.6512\n",
      "Epoch 106/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 6.9271 - softmax_loss: 0.6042 - normalize_loss: 0.4424 - normalize3_loss: 0.4424 - softmax_acc: 0.6540\n",
      "Epoch 107/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 7.0145 - softmax_loss: 0.6125 - normalize_loss: 0.4448 - normalize3_loss: 0.4448 - softmax_acc: 0.6401\n",
      "Epoch 108/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 6.8560 - softmax_loss: 0.5968 - normalize_loss: 0.4440 - normalize3_loss: 0.4440 - softmax_acc: 0.6601\n",
      "Epoch 109/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 6.9128 - softmax_loss: 0.6028 - normalize_loss: 0.4423 - normalize3_loss: 0.4423 - softmax_acc: 0.6564\n",
      "Epoch 110/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 6.9291 - softmax_loss: 0.6046 - normalize_loss: 0.4415 - normalize3_loss: 0.4415 - softmax_acc: 0.6545\n",
      "Epoch 111/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 6.9119 - softmax_loss: 0.6028 - normalize_loss: 0.4419 - normalize3_loss: 0.4419 - softmax_acc: 0.6543\n",
      "Epoch 112/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 6.8702 - softmax_loss: 0.5992 - normalize_loss: 0.4392 - normalize3_loss: 0.4392 - softmax_acc: 0.6517\n",
      "Epoch 113/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 6.9276 - softmax_loss: 0.6045 - normalize_loss: 0.4412 - normalize3_loss: 0.4412 - softmax_acc: 0.6428\n",
      "Epoch 114/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 6.8140 - softmax_loss: 0.5928 - normalize_loss: 0.4430 - normalize3_loss: 0.4430 - softmax_acc: 0.6644\n",
      "Epoch 115/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 6.8410 - softmax_loss: 0.5955 - normalize_loss: 0.4429 - normalize3_loss: 0.4429 - softmax_acc: 0.6625\n",
      "Epoch 116/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 6.9625 - softmax_loss: 0.6089 - normalize_loss: 0.4365 - normalize3_loss: 0.4365 - softmax_acc: 0.6530\n",
      "Epoch 117/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 6.9677 - softmax_loss: 0.6087 - normalize_loss: 0.4401 - normalize3_loss: 0.4401 - softmax_acc: 0.6474\n",
      "Epoch 118/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 6.8504 - softmax_loss: 0.5971 - normalize_loss: 0.4399 - normalize3_loss: 0.4399 - softmax_acc: 0.6609\n",
      "Epoch 119/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 6.8393 - softmax_loss: 0.5970 - normalize_loss: 0.4347 - normalize3_loss: 0.4347 - softmax_acc: 0.6597\n",
      "Epoch 120/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 6.8389 - softmax_loss: 0.5953 - normalize_loss: 0.4428 - normalize3_loss: 0.4428 - softmax_acc: 0.6671\n",
      "Epoch 121/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 6.8578 - softmax_loss: 0.5974 - normalize_loss: 0.4421 - normalize3_loss: 0.4421 - softmax_acc: 0.6705\n",
      "Epoch 122/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 6.8377 - softmax_loss: 0.5961 - normalize_loss: 0.4381 - normalize3_loss: 0.4381 - softmax_acc: 0.6550\n",
      "Epoch 123/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 6.8668 - softmax_loss: 0.5976 - normalize_loss: 0.4456 - normalize3_loss: 0.4456 - softmax_acc: 0.6588\n",
      "Epoch 124/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 6.8189 - softmax_loss: 0.5942 - normalize_loss: 0.4384 - normalize3_loss: 0.4384 - softmax_acc: 0.6633\n",
      "Epoch 125/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 6.8009 - softmax_loss: 0.5926 - normalize_loss: 0.4373 - normalize3_loss: 0.4373 - softmax_acc: 0.6662\n",
      "Epoch 126/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 6.8850 - softmax_loss: 0.6002 - normalize_loss: 0.4413 - normalize3_loss: 0.4413 - softmax_acc: 0.6526\n",
      "Epoch 127/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 6.8043 - softmax_loss: 0.5935 - normalize_loss: 0.4348 - normalize3_loss: 0.4348 - softmax_acc: 0.6703\n",
      "Epoch 128/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 7.0370 - softmax_loss: 0.6163 - normalize_loss: 0.4369 - normalize3_loss: 0.4369 - softmax_acc: 0.6329\n",
      "Epoch 129/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 6.9795 - softmax_loss: 0.6112 - normalize_loss: 0.4339 - normalize3_loss: 0.4339 - softmax_acc: 0.6329\n",
      "Epoch 130/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 6.8597 - softmax_loss: 0.5993 - normalize_loss: 0.4334 - normalize3_loss: 0.4334 - softmax_acc: 0.6412\n",
      "Epoch 131/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 6.7985 - softmax_loss: 0.5923 - normalize_loss: 0.4377 - normalize3_loss: 0.4377 - softmax_acc: 0.6717\n",
      "Epoch 132/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 6.7500 - softmax_loss: 0.5883 - normalize_loss: 0.4337 - normalize3_loss: 0.4337 - softmax_acc: 0.6672\n",
      "Epoch 133/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 6.8137 - softmax_loss: 0.5935 - normalize_loss: 0.4393 - normalize3_loss: 0.4393 - softmax_acc: 0.6659\n",
      "Epoch 134/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 6.8314 - softmax_loss: 0.5959 - normalize_loss: 0.4360 - normalize3_loss: 0.4360 - softmax_acc: 0.6474\n",
      "Epoch 135/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 6.8395 - softmax_loss: 0.5967 - normalize_loss: 0.4361 - normalize3_loss: 0.4361 - softmax_acc: 0.6605\n",
      "Epoch 136/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 6.8770 - softmax_loss: 0.6003 - normalize_loss: 0.4370 - normalize3_loss: 0.4370 - softmax_acc: 0.6571\n",
      "Epoch 137/200\n",
      "76/76 [==============================] - 1s 15ms/step - loss: 6.8529 - softmax_loss: 0.5977 - normalize_loss: 0.4381 - normalize3_loss: 0.4381 - softmax_acc: 0.6484\n",
      "Epoch 138/200\n",
      "76/76 [==============================] - 1s 14ms/step - loss: 6.8389 - softmax_loss: 0.5969 - normalize_loss: 0.4349 - normalize3_loss: 0.4349 - softmax_acc: 0.6531\n",
      "Epoch 139/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.9151 - softmax_loss: 0.6026 - normalize_loss: 0.4443 - normalize3_loss: 0.4443 - softmax_acc: 0.6472\n",
      "Epoch 140/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.8763 - softmax_loss: 0.5984 - normalize_loss: 0.4459 - normalize3_loss: 0.4459 - softmax_acc: 0.6635\n",
      "Epoch 141/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7781 - softmax_loss: 0.5889 - normalize_loss: 0.4443 - normalize3_loss: 0.4443 - softmax_acc: 0.6615\n",
      "Epoch 142/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7640 - softmax_loss: 0.5887 - normalize_loss: 0.4383 - normalize3_loss: 0.4383 - softmax_acc: 0.6657\n",
      "Epoch 143/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7293 - softmax_loss: 0.5858 - normalize_loss: 0.4355 - normalize3_loss: 0.4355 - softmax_acc: 0.6682\n",
      "Epoch 144/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.8585 - softmax_loss: 0.5981 - normalize_loss: 0.4387 - normalize3_loss: 0.4387 - softmax_acc: 0.6580\n",
      "Epoch 145/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7564 - softmax_loss: 0.5870 - normalize_loss: 0.4431 - normalize3_loss: 0.4431 - softmax_acc: 0.6760\n",
      "Epoch 146/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.8023 - softmax_loss: 0.5927 - normalize_loss: 0.4375 - normalize3_loss: 0.4375 - softmax_acc: 0.6608\n",
      "Epoch 147/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7384 - softmax_loss: 0.5863 - normalize_loss: 0.4375 - normalize3_loss: 0.4375 - softmax_acc: 0.6729\n",
      "Epoch 148/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.8589 - softmax_loss: 0.5977 - normalize_loss: 0.4412 - normalize3_loss: 0.4412 - softmax_acc: 0.6648\n",
      "Epoch 149/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7222 - softmax_loss: 0.5847 - normalize_loss: 0.4378 - normalize3_loss: 0.4378 - softmax_acc: 0.6775\n",
      "Epoch 150/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7936 - softmax_loss: 0.5925 - normalize_loss: 0.4344 - normalize3_loss: 0.4344 - softmax_acc: 0.6500\n",
      "Epoch 151/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7404 - softmax_loss: 0.5873 - normalize_loss: 0.4336 - normalize3_loss: 0.4336 - softmax_acc: 0.6689\n",
      "Epoch 152/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7634 - softmax_loss: 0.5898 - normalize_loss: 0.4328 - normalize3_loss: 0.4328 - softmax_acc: 0.6592\n",
      "Epoch 153/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.9812 - softmax_loss: 0.6114 - normalize_loss: 0.4335 - normalize3_loss: 0.4335 - softmax_acc: 0.6376\n",
      "Epoch 154/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.9792 - softmax_loss: 0.6101 - normalize_loss: 0.4391 - normalize3_loss: 0.4391 - softmax_acc: 0.6414\n",
      "Epoch 155/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.6473 - softmax_loss: 0.5764 - normalize_loss: 0.4419 - normalize3_loss: 0.4419 - softmax_acc: 0.6759\n",
      "Epoch 156/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7269 - softmax_loss: 0.5856 - normalize_loss: 0.4353 - normalize3_loss: 0.4353 - softmax_acc: 0.6748\n",
      "Epoch 157/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7076 - softmax_loss: 0.5829 - normalize_loss: 0.4392 - normalize3_loss: 0.4392 - softmax_acc: 0.6689\n",
      "Epoch 158/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7021 - softmax_loss: 0.5823 - normalize_loss: 0.4394 - normalize3_loss: 0.4394 - softmax_acc: 0.6672\n",
      "Epoch 159/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7824 - softmax_loss: 0.5907 - normalize_loss: 0.4378 - normalize3_loss: 0.4378 - softmax_acc: 0.6581\n",
      "Epoch 160/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.6563 - softmax_loss: 0.5775 - normalize_loss: 0.4409 - normalize3_loss: 0.4409 - softmax_acc: 0.6776\n",
      "Epoch 161/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 7.0034 - softmax_loss: 0.6129 - normalize_loss: 0.4371 - normalize3_loss: 0.4371 - softmax_acc: 0.6385\n",
      "Epoch 162/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7516 - softmax_loss: 0.5876 - normalize_loss: 0.4380 - normalize3_loss: 0.4380 - softmax_acc: 0.6639\n",
      "Epoch 163/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7536 - softmax_loss: 0.5863 - normalize_loss: 0.4451 - normalize3_loss: 0.4451 - softmax_acc: 0.6741\n",
      "Epoch 164/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.8295 - softmax_loss: 0.5952 - normalize_loss: 0.4390 - normalize3_loss: 0.4390 - softmax_acc: 0.6648\n",
      "Epoch 165/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7792 - softmax_loss: 0.5899 - normalize_loss: 0.4402 - normalize3_loss: 0.4402 - softmax_acc: 0.6659\n",
      "Epoch 166/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.8149 - softmax_loss: 0.5929 - normalize_loss: 0.4431 - normalize3_loss: 0.4431 - softmax_acc: 0.6546\n",
      "Epoch 167/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7474 - softmax_loss: 0.5867 - normalize_loss: 0.4402 - normalize3_loss: 0.4402 - softmax_acc: 0.6687\n",
      "Epoch 168/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7004 - softmax_loss: 0.5816 - normalize_loss: 0.4422 - normalize3_loss: 0.4422 - softmax_acc: 0.6727\n",
      "Epoch 169/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7847 - softmax_loss: 0.5910 - normalize_loss: 0.4371 - normalize3_loss: 0.4371 - softmax_acc: 0.6616\n",
      "Epoch 170/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7253 - softmax_loss: 0.5851 - normalize_loss: 0.4372 - normalize3_loss: 0.4372 - softmax_acc: 0.6641\n",
      "Epoch 171/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7691 - softmax_loss: 0.5882 - normalize_loss: 0.4437 - normalize3_loss: 0.4437 - softmax_acc: 0.6593\n",
      "Epoch 172/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.8484 - softmax_loss: 0.5973 - normalize_loss: 0.4379 - normalize3_loss: 0.4379 - softmax_acc: 0.6437\n",
      "Epoch 173/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7826 - softmax_loss: 0.5902 - normalize_loss: 0.4402 - normalize3_loss: 0.4402 - softmax_acc: 0.6638\n",
      "Epoch 174/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7338 - softmax_loss: 0.5857 - normalize_loss: 0.4384 - normalize3_loss: 0.4384 - softmax_acc: 0.6675\n",
      "Epoch 175/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7042 - softmax_loss: 0.5820 - normalize_loss: 0.4422 - normalize3_loss: 0.4422 - softmax_acc: 0.6653\n",
      "Epoch 176/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7834 - softmax_loss: 0.5906 - normalize_loss: 0.4385 - normalize3_loss: 0.4385 - softmax_acc: 0.6618\n",
      "Epoch 177/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7040 - softmax_loss: 0.5823 - normalize_loss: 0.4406 - normalize3_loss: 0.4406 - softmax_acc: 0.6800\n",
      "Epoch 178/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7218 - softmax_loss: 0.5841 - normalize_loss: 0.4404 - normalize3_loss: 0.4404 - softmax_acc: 0.6634\n",
      "Epoch 179/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.8047 - softmax_loss: 0.5929 - normalize_loss: 0.4379 - normalize3_loss: 0.4379 - softmax_acc: 0.6632\n",
      "Epoch 180/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7484 - softmax_loss: 0.5868 - normalize_loss: 0.4402 - normalize3_loss: 0.4402 - softmax_acc: 0.6628\n",
      "Epoch 181/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7347 - softmax_loss: 0.5863 - normalize_loss: 0.4357 - normalize3_loss: 0.4357 - softmax_acc: 0.6675\n",
      "Epoch 182/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7192 - softmax_loss: 0.5847 - normalize_loss: 0.4362 - normalize3_loss: 0.4362 - softmax_acc: 0.6720\n",
      "Epoch 183/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.8167 - softmax_loss: 0.5933 - normalize_loss: 0.4416 - normalize3_loss: 0.4416 - softmax_acc: 0.6625\n",
      "Epoch 184/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7818 - softmax_loss: 0.5898 - normalize_loss: 0.4421 - normalize3_loss: 0.4421 - softmax_acc: 0.6620\n",
      "Epoch 185/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7258 - softmax_loss: 0.5857 - normalize_loss: 0.4346 - normalize3_loss: 0.4346 - softmax_acc: 0.6681\n",
      "Epoch 186/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.6584 - softmax_loss: 0.5784 - normalize_loss: 0.4372 - normalize3_loss: 0.4372 - softmax_acc: 0.6717\n",
      "Epoch 187/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.6490 - softmax_loss: 0.5770 - normalize_loss: 0.4395 - normalize3_loss: 0.4395 - softmax_acc: 0.6744\n",
      "Epoch 188/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7196 - softmax_loss: 0.5841 - normalize_loss: 0.4391 - normalize3_loss: 0.4391 - softmax_acc: 0.6677\n",
      "Epoch 189/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.6774 - softmax_loss: 0.5802 - normalize_loss: 0.4379 - normalize3_loss: 0.4379 - softmax_acc: 0.6741\n",
      "Epoch 190/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.6847 - softmax_loss: 0.5803 - normalize_loss: 0.4410 - normalize3_loss: 0.4410 - softmax_acc: 0.6738\n",
      "Epoch 191/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7408 - softmax_loss: 0.5847 - normalize_loss: 0.4467 - normalize3_loss: 0.4467 - softmax_acc: 0.6597\n",
      "Epoch 192/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7681 - softmax_loss: 0.5893 - normalize_loss: 0.4374 - normalize3_loss: 0.4374 - softmax_acc: 0.6654\n",
      "Epoch 193/200\n",
      "76/76 [==============================] - 1s 17ms/step - loss: 6.7325 - softmax_loss: 0.5846 - normalize_loss: 0.4431 - normalize3_loss: 0.4431 - softmax_acc: 0.6630\n",
      "Epoch 194/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.6523 - softmax_loss: 0.5775 - normalize_loss: 0.4388 - normalize3_loss: 0.4388 - softmax_acc: 0.6753\n",
      "Epoch 195/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7616 - softmax_loss: 0.5884 - normalize_loss: 0.4390 - normalize3_loss: 0.4390 - softmax_acc: 0.6666\n",
      "Epoch 196/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7013 - softmax_loss: 0.5816 - normalize_loss: 0.4425 - normalize3_loss: 0.4425 - softmax_acc: 0.6691\n",
      "Epoch 197/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.6272 - softmax_loss: 0.5749 - normalize_loss: 0.4393 - normalize3_loss: 0.4393 - softmax_acc: 0.6816\n",
      "Epoch 198/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.4773 - softmax_loss: 0.5604 - normalize_loss: 0.4366 - normalize3_loss: 0.4366 - softmax_acc: 0.7003\n",
      "Epoch 199/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.6644 - softmax_loss: 0.5775 - normalize_loss: 0.4450 - normalize3_loss: 0.4450 - softmax_acc: 0.6864\n",
      "Epoch 200/200\n",
      "76/76 [==============================] - 1s 16ms/step - loss: 6.7051 - softmax_loss: 0.5823 - normalize_loss: 0.4409 - normalize3_loss: 0.4409 - softmax_acc: 0.6762\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43844634628044765 Negative within positive\n",
      "----------\n",
      "3010 val results 0.1046200840015273 1.6323971e-05 index 0 test results 0.3128390596745027 0.5599455 test result with val bias 0.20595922634605332\n",
      "3013 val results 0.1046200840015273 1.6323971e-05 index 0 test results 0.3128390596745027 0.5599455 test result with val bias 0.20595922634605332\n",
      "3014 val results 0.1046200840015273 1.6323971e-05 index 0 test results 0.3128390596745027 0.5599455 test result with val bias 0.20595922634605332\n",
      "3015 val results 0.1046200840015273 1.6323971e-05 index 0 test results 0.3128390596745027 0.5599455 test result with val bias 0.20595922634605332\n",
      "3022 val results 0.1046200840015273 1.6323971e-05 index 0 test results 0.3128390596745027 0.5599455 test result with val bias 0.20595922634605332\n",
      "3024 val results 0.1046200840015273 1.6323971e-05 index 0 test results 0.3128390596745027 0.5599455 test result with val bias 0.20595922634605332\n",
      "6\n",
      "15 68\n",
      "(None, 1, 1) coeff (None, 10, 30) amplitude\n",
      "Epoch 1/200\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime,timedelta\n",
    "from scipy.stats import iqr,skew,kurtosis,mode\n",
    "from joblib import Parallel,delayed\n",
    "import zipfile\n",
    "import shutil\n",
    "from sklearn.feature_selection import SelectKBest,f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score,r2_score,classification_report\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,RandomForestRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV,StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "seed = 100\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from sklearn.model_selection import train_test_split,LeavePGroupsOut\n",
    "from keras.backend import expand_dims, repeat_elements\n",
    "from keras.layers import Lambda\n",
    "from tensorflow.keras.layers import Conv1D,BatchNormalization,Dropout,InputLayer,MaxPooling1D,Flatten,RepeatVector,Dense,Input,Activation,GRU,Bidirectional,LSTM\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow_addons as tfa\n",
    "import warnings\n",
    "import functools\n",
    "from tensorflow_addons.losses import metric_learning\n",
    "import tensorflow as tf\n",
    "\n",
    "# tf.config.run_functions_eagerly(False)\n",
    "warnings.filterwarnings('ignore')\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[3], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def get_X_y_groups(n_lag=10):\n",
    "    data = pickle.load(open(filepath_file.format(n_lag),'rb'))\n",
    "\n",
    "    X_feature = np.concatenate(data.feature_final.values)\n",
    "    X_static =  np.concatenate(data.static_features.values)\n",
    "\n",
    "    X_stress_episode = np.concatenate(data.stress_episode.values)\n",
    "    X_quit_episode = np.concatenate(data.quit_episode.values)\n",
    "    X_activity_episode = np.concatenate(data.activity_episode.values)\n",
    "    X_smoking_episode = np.concatenate(data.smoking_episode.values)\n",
    "\n",
    "    y_time = data['time'].values\n",
    "    y = data['label'].values\n",
    "    groups = data['user'].values\n",
    "    \n",
    "    return X_feature,X_static,X_stress_episode,X_quit_episode,X_activity_episode,X_smoking_episode,y_time,y,groups\n",
    "\n",
    "\n",
    "def get_train_test_indexes(groups,n_groups_split = 10,n_val_groups = 10):\n",
    "    groups_unique = np.unique(groups)\n",
    "    groups_split = np.array_split(groups_unique,n_groups_split)\n",
    "    indexes = []\n",
    "    for this_groups in groups_split:\n",
    "        train_groups = np.array([a for a in groups_unique if a not in this_groups])\n",
    "        val_groups = np.random.choice(train_groups,n_val_groups)\n",
    "        train_groups = np.array([a for a in groups_unique if a not in list(this_groups)+list(val_groups)])\n",
    "        test_groups = this_groups\n",
    "        train_index,test_index = np.array([i for i,a in enumerate(groups) \n",
    "                                           if a in train_groups]),np.array([i for i,a in enumerate(groups) \n",
    "                                                                               if a in test_groups])\n",
    "        val_index = np.array([i for i,a in enumerate(groups) \n",
    "                                           if a in val_groups])\n",
    "        indexes.append([train_index,test_index,val_index])\n",
    "    return indexes\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "def f1Bias_scorer_CV(probs, y, ret_bias=True):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "\n",
    "    f1,bias = 0.0,.5\n",
    "    min_recall = .7\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0) and recall[i]>min_recall:\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "\n",
    "class CenterLossLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_classes, n_features, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.n_features = n_features\n",
    "        self.centers = tf.Variable(\n",
    "            tf.zeros([n_classes, n_features]),\n",
    "            name=\"centers\",\n",
    "            trainable=False,\n",
    "            # in a distributed strategy, we want updates to this variable to be summed.\n",
    "            aggregation=tf.VariableAggregation.MEAN,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        # pass through layer\n",
    "        return tf.identity(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"n_classes\": self.n_classes, \"n_features\": self.n_features})\n",
    "        return config\n",
    "\n",
    "\n",
    "class CenterLoss(tf.keras.losses.Loss):\n",
    "    def __init__(\n",
    "        self,\n",
    "        centers_layer,\n",
    "        alpha=0.9,\n",
    "        update_centers=True,\n",
    "        p = 95,\n",
    "        name=\"center_loss\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.centers_layer = centers_layer\n",
    "        self.centers = self.centers_layer.centers\n",
    "        self.alpha = alpha\n",
    "        self.update_centers = update_centers\n",
    "        self.p = p\n",
    "        \n",
    "    def consistency_loss(self,labels,precise_embeddings):\n",
    "        \n",
    "        lshape = tf.shape(labels)\n",
    "        labels = tf.reshape(labels, [lshape[0], 1])\n",
    "        mask_for_equal = tf.math.equal(tf.cast(labels,tf.bool),tf.transpose(tf.cast(labels,tf.bool)))\n",
    "        only_positive = tf.repeat(tf.transpose(tf.reshape(tf.cast(labels,tf.bool),[lshape[0],1])),lshape[0],axis=0)\n",
    "        mask_only_positive = tf.math.multiply(tf.cast(mask_for_equal,tf.float32),tf.cast(only_positive,tf.float32))\n",
    "        pdist_matrix = metric_learning.pairwise_distance(\n",
    "                    precise_embeddings, squared=False\n",
    "                )\n",
    "        # return tf.reduce_sum(pdist_matrix)\n",
    "        positive_only_dist = tf.boolean_mask(pdist_matrix, tf.cast(mask_only_positive,tf.bool))\n",
    "\n",
    "        distance_95 = tf.reshape(positive_only_dist,(-1,))\n",
    "        # samples1 = tf.cast(tf.shape(labels)[0], tf.float32) #batch size\n",
    "        # p =  2*(100. - self.p)/100.  #100% will return 0 elements, 0% will return all elements\n",
    "        # samples = tf.cast(tf.math.floor(p * samples1), tf.int32)\n",
    "        # values, indices = tf.math.top_k(distance_95, samples)\n",
    "        # positive_dist_95th = tf.reduce_min(values)\n",
    "\n",
    "        return tf.reduce_mean(distance_95)\n",
    "        \n",
    "    def compute_rare_loss(self,labels,precise_embeddings,positive_dist_95th):\n",
    "        sparse_labels = tf.reshape(labels, (-1,))\n",
    "        centers_batch = tf.gather(self.centers, tf.ones([tf.shape(sparse_labels)[0],],dtype=tf.int32))\n",
    "        distance_to_positive_center = tf.norm(precise_embeddings-centers_batch,2,axis=1)\n",
    "        distance_to_positive_center_smaller_than_95 = tf.where(distance_to_positive_center<=positive_dist_95th,1,0)\n",
    "        negative_only = tf.math.logical_not(tf.cast(sparse_labels,tf.bool))\n",
    "        negative_within_circle = tf.reduce_sum(tf.boolean_mask(distance_to_positive_center_smaller_than_95,negative_only))\n",
    "        total_negative = tf.cast(tf.shape(sparse_labels)[0],tf.float32) - tf.reduce_sum(sparse_labels)\n",
    "        rare_loss = tf.cast(negative_within_circle,tf.float32)/(total_negative+K.epsilon())\n",
    "        return rare_loss\n",
    "    \n",
    "    def center_loss_positive_compute(self,labels,precise_embeddings,centers_batch):\n",
    "        loss_row = tf.reduce_mean(tf.math.square(precise_embeddings - centers_batch),axis=1)\n",
    "        loss_row_only_positive = tf.math.multiply(tf.cast(labels,tf.float32),loss_row)\n",
    "        center_loss_positive = tf.reduce_sum(loss_row_only_positive)/(tf.reduce_sum(labels)+K.epsilon())\n",
    "        return center_loss_positive\n",
    "    \n",
    "    def compute_positive_to_negative_loss(self,labels,precise_embeddings):\n",
    "        sparse_labels = tf.reshape(labels, (-1,))\n",
    "        centers_batch = tf.gather(self.centers, tf.ones([tf.shape(sparse_labels)[0],],dtype=tf.int32))\n",
    "        distance_to_positive_center = tf.norm(precise_embeddings-centers_batch,2,axis=1)\n",
    "        negative_only = tf.math.logical_not(tf.cast(sparse_labels,tf.bool))\n",
    "        return tf.reduce_mean(tf.boolean_mask(distance_to_positive_center,negative_only))\n",
    "    \n",
    "    \n",
    "    def call(self, sparse_labels, prelogits):\n",
    "        sparse_labels = tf.reshape(sparse_labels, (-1,))\n",
    "        # the reduction of batch dimension will be done by the parent class\n",
    "        # center_loss = tf.keras.losses.mean_squared_error(prelogits, centers_batch)\n",
    "        centers_batch = tf.gather(self.centers, tf.cast(sparse_labels,tf.int32))\n",
    "        if self.update_centers and tf.cast(tf.reduce_sum(sparse_labels),tf.int32)<tf.shape(sparse_labels)[0]:\n",
    "            diff = (1 - self.alpha) * (centers_batch - prelogits)\n",
    "            updates = tf.scatter_nd(tf.cast(sparse_labels[:, None],tf.int32), diff, self.centers.shape)\n",
    "            self.centers.assign_sub(updates)\n",
    "        distance_95th = self.consistency_loss(sparse_labels,prelogits)\n",
    "        # center_loss_positive = self.center_loss_positive_compute(sparse_labels,prelogits,centers_batch)        \n",
    "        rare_loss = self.compute_rare_loss(sparse_labels,prelogits,positive_dist_95th=distance_95th)\n",
    "        postive_to_negative_distance = self.compute_positive_to_negative_loss(sparse_labels,prelogits)\n",
    "        return tf.square(rare_loss-.2)*10 + distance_95th\n",
    "    \n",
    "def get_model():\n",
    "    n_t,n_f = train_feature.shape[1],train_feature.shape[2]\n",
    "    print(n_t,n_f)\n",
    "    x_input = Input(shape=(n_t,n_f))\n",
    "    x_feature = Conv1D(10,1,activation='tanh')(x_input)\n",
    "    # x_feature = Dropout(.2)(x_feature)\n",
    "    x_feature = LSTM(10,activation='tanh',return_sequences=False)(x_feature)\n",
    "    x_feature = Dropout(.2)(x_feature)\n",
    "    x_feature = Flatten()(x_feature)\n",
    "    x_feature = Dense(10,activation='relu')(x_feature)\n",
    "    \n",
    "    n_sf = train_static.shape[1]\n",
    "    x_input_static = Input(shape=(n_sf))\n",
    "    x_static = Dense(10,activation='relu')(x_input_static)\n",
    "    # x_static = Dense(10,activation='relu')(x_static)\n",
    "    n_timesteps = train_stress.shape[-2]\n",
    "    n_episodes_stress,n_episodes_quit,n_episodes_activity,n_episodes_smoking = train_stress.shape[1],train_quit.shape[1],train_activity.shape[1],train_smoking.shape[1]\n",
    "    \n",
    "    x_alpha_stress = Dense(1,activation='sigmoid',name='alpha_stress')(x_static)\n",
    "    x_alpha_stress = RepeatVector(n_timesteps)(x_alpha_stress)\n",
    "    x_alpha_stress = Lambda(lambda x: repeat_elements(expand_dims(x, axis=1), n_episodes_stress, 1))(x_alpha_stress)\n",
    "    \n",
    "    x_alpha_quit = Dense(1,activation='sigmoid',name='alpha_quit')(x_static)\n",
    "    x_alpha_quit = RepeatVector(n_timesteps)(x_alpha_quit)\n",
    "    x_alpha_quit = Lambda(lambda x: repeat_elements(expand_dims(x, axis=1), n_episodes_quit, 1))(x_alpha_quit)\n",
    "    \n",
    "    x_alpha_activity = Dense(1,activation='sigmoid',name='alpha_activity')(x_static)\n",
    "    x_alpha_activity = RepeatVector(n_timesteps)(x_alpha_activity)\n",
    "    x_alpha_activity = Lambda(lambda x: repeat_elements(expand_dims(x, axis=1), n_episodes_activity, 1))(x_alpha_activity)\n",
    "    \n",
    "    x_alpha_smoking = Dense(1,activation='sigmoid',name='alpha_smoking')(x_static)\n",
    "    x_alpha_smoking = RepeatVector(n_timesteps)(x_alpha_smoking)\n",
    "    x_alpha_smoking = Lambda(lambda x: repeat_elements(expand_dims(x, axis=1), n_episodes_smoking, 1))(x_alpha_smoking)\n",
    "\n",
    "    n_dim = 3\n",
    "    x_stress = Input(shape=(n_episodes_stress,n_timesteps,n_dim))\n",
    "    stress_alpha_time = tf.math.multiply(x_alpha_stress[:,:,:,0]*-1,x_stress[:,:,:,0])\n",
    "    stress_alpha_time_exp = tf.math.exp(stress_alpha_time)\n",
    "\n",
    "    x_stress_amplitude = x_stress[:,:,:,1]\n",
    "    stress_amplitude_coeff = Dense(1,activation='relu',name='amplitude_stress')(x_static)\n",
    "    stress_amplitude_coeff = Lambda(lambda x: expand_dims(x, axis=2))(stress_amplitude_coeff)\n",
    "    print(stress_amplitude_coeff.shape,'coeff',x_stress_amplitude.shape,'amplitude')\n",
    "    x_stress_amplitude = tf.math.multiply(x_stress_amplitude,stress_amplitude_coeff)\n",
    "    x_stress_duration = x_stress[:,:,:,2]\n",
    "    stress_duration_coeff = Dense(1,activation='sigmoid',name='duration_stress')(x_static)\n",
    "    stress_duration_coeff = Lambda(lambda x: expand_dims(x, axis=2))(stress_duration_coeff)\n",
    "    x_stress_duration = tf.math.multiply(x_stress_duration,stress_duration_coeff)\n",
    "    x_stress_all = tf.math.add(x_stress_amplitude,x_stress_duration)\n",
    "    stress_alpha_time_exp_amplitude = tf.math.multiply(stress_alpha_time_exp,x_stress_all)\n",
    "    stress_final = tf.math.reduce_sum(stress_alpha_time_exp_amplitude,axis=1)\n",
    "    stress_final = Lambda(lambda x: expand_dims(x, axis=2))(stress_final)\n",
    "    # stress_final = LSTM(10,activation='tanh',return_sequences=True)(stress_final)\n",
    "    \n",
    "    \n",
    "    \n",
    "    x_quit = Input(shape=(n_episodes_quit,n_timesteps,n_dim))\n",
    "\n",
    "    x_smoking = Input(shape=(n_episodes_smoking,n_timesteps,n_dim))\n",
    "    smoking_alpha_time = tf.math.multiply(x_alpha_smoking[:,:,:,0]*-1,x_smoking[:,:,:,0])\n",
    "    smoking_alpha_time_exp = tf.math.exp(smoking_alpha_time)\n",
    "    x_smoking_amplitude = x_smoking[:,:,:,1]\n",
    "    smoking_amplitude_coeff = Dense(1,activation='sigmoid',name='amplitude_smoking')(x_static)\n",
    "    smoking_amplitude_coeff = Lambda(lambda x: expand_dims(x, axis=2))(smoking_amplitude_coeff)\n",
    "    x_smoking_amplitude = tf.math.multiply(x_smoking_amplitude,smoking_amplitude_coeff)\n",
    "    x_smoking_duration = x_smoking[:,:,:,2]\n",
    "    smoking_duration_coeff = Dense(1,activation='sigmoid',name='duration_smoking')(x_static)\n",
    "    smoking_duration_coeff = Lambda(lambda x: expand_dims(x, axis=2))(smoking_duration_coeff)\n",
    "    x_smoking_duration = tf.math.multiply(x_smoking_duration,smoking_duration_coeff)\n",
    "    x_smoking_all = tf.math.add(x_smoking_amplitude,x_smoking_duration)\n",
    "    smoking_alpha_time_exp_amplitude = tf.math.multiply(smoking_alpha_time_exp,x_smoking_all)\n",
    "    smoking_final = tf.math.reduce_sum(smoking_alpha_time_exp_amplitude,axis=1)\n",
    "    smoking_final = Lambda(lambda x: expand_dims(x, axis=2))(smoking_final)\n",
    "    # smoking_final = LSTM(10,activation='tanh',return_sequences=True)(smoking_final)\n",
    "\n",
    "    \n",
    "    \n",
    "    x_activity = Input(shape=(n_episodes_activity,n_timesteps,n_dim))\n",
    "    activity_alpha_time = tf.math.multiply(x_alpha_activity[:,:,:,0]*-1,x_activity[:,:,:,0])\n",
    "    activity_alpha_time_exp = tf.math.exp(activity_alpha_time)\n",
    "    x_activity_amplitude = x_activity[:,:,:,1]\n",
    "    activity_amplitude_coeff = Dense(1,activation='sigmoid',name='amplitude_activity')(x_static)\n",
    "    activity_amplitude_coeff = Lambda(lambda x: expand_dims(x, axis=2))(activity_amplitude_coeff)\n",
    "    x_activity_amplitude = tf.math.multiply(x_activity_amplitude,activity_amplitude_coeff)\n",
    "    x_activity_duration = x_activity[:,:,:,2]\n",
    "    activity_duration_coeff = Dense(1,activation='sigmoid',name='duration_activity')(x_static)\n",
    "    activity_duration_coeff = Lambda(lambda x: expand_dims(x, axis=2))(activity_duration_coeff)\n",
    "    x_activity_duration = tf.math.multiply(x_activity_duration,activity_duration_coeff)\n",
    "    x_activity_all = tf.math.add(x_activity_amplitude,x_activity_duration)\n",
    "    activity_alpha_time_exp_amplitude = tf.math.multiply(activity_alpha_time_exp,x_activity_all)\n",
    "    activity_final = tf.math.reduce_sum(activity_alpha_time_exp_amplitude,axis=1)\n",
    "    activity_final = Lambda(lambda x: expand_dims(x, axis=2))(activity_final)\n",
    "    # activity_final = LSTM(10,activation='tanh',return_sequences=True)(activity_final)\n",
    "    \n",
    "    \n",
    "    x_episode = tf.concat([activity_final, stress_final, smoking_final],2)\n",
    "    x_episode = LSTM(10,activation='tanh',return_sequences=False)(x_episode)\n",
    "    x_episode = Dropout(.03)(x_episode)\n",
    "    x_episode = Flatten()(x_episode)\n",
    "    x_episode = Dense(10,activation='relu')(x_episode)\n",
    "\n",
    "    # merged_all = tf.concat([x_feature,x_episode],1)\n",
    "    # print(merged.shape,'concatenated shape')\n",
    "    merged_all = x_episode\n",
    "    \n",
    "    merged1 = Dense(10,activation='relu',name='normalize2')(merged_all)\n",
    "    merged1 = Lambda(lambda x: K.l2_normalize(x,axis=1))(merged1)\n",
    "    center_layer1 = CenterLossLayer(n_classes=2, n_features=10,name='normalize3')\n",
    "    merged1 = center_layer1(merged1)\n",
    "    \n",
    "    # merged = Dense(5,activation='relu',name='norma')(merged1)\n",
    "    # merged = Dense(2,activation='relu',name='normalize1')(merged)\n",
    "    # merged = Lambda(lambda x: K.l2_normalize(x,axis=1))(merged)\n",
    "    center_layer = CenterLossLayer(n_classes=2, n_features=10,name='normalize')\n",
    "    merged = center_layer(merged1)\n",
    "    # merged = Dense(10,activation='relu')(merged)\n",
    "    # \n",
    "    # output1 = Dense(2,activation='relu')(merged)\n",
    "    output = Dense(2,activation='softmax',name='softmax')(merged)\n",
    "    # output = Activation('softmax',name='softmax')(merged)\n",
    "    # output = Dense(2,activation='softmax')(merged)\n",
    "    # output = Reshape((-1,1))(output)\n",
    "#     output = Activation('softmax',name='softmax')(output)\n",
    "    model = Model(inputs=[x_input,x_input_static,x_stress,x_activity,x_smoking,x_quit], outputs=[output,merged,merged1])\n",
    "    # myloss = get_center_loss(num_classes=2,feature_dim=merged.shape[-1],alpha=.9)\n",
    "    myloss = CenterLoss(centers_layer=center_layer)\n",
    "    myloss1 = CenterLoss(centers_layer=center_layer1)\n",
    "    \n",
    "    model.compile(\n",
    "        loss={'softmax':tf.losses.SparseCategoricalCrossentropy(),'normalize':myloss,'normalize3':myloss1},\n",
    "        loss_weights = {'softmax':10,'normalize':0,'normalize3':2},\n",
    "        metrics={'softmax':['acc']},\n",
    "        optimizer='adam'\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "def get_X_y_groups(n_lag=10):\n",
    "    data = pickle.load(open(filepath_file.format(n_lag),'rb'))\n",
    "\n",
    "    X_feature = np.concatenate(data.feature_final.values)\n",
    "    X_static =  np.concatenate(data.static_features.values)\n",
    "\n",
    "    X_stress_episode = np.concatenate(data.stress_episode.values)\n",
    "    X_quit_episode = np.concatenate(data.quit_episode.values)\n",
    "    X_activity_episode = np.concatenate(data.activity_episode.values)\n",
    "    X_smoking_episode = np.concatenate(data.smoking_episode.values)\n",
    "\n",
    "    y_time = data['time'].values\n",
    "    print(data['label'].unique())\n",
    "    # y =  OneHotEncoder().fit_transform(data['label'].values.reshape(-1,1)).todense()\n",
    "    y = data['label'].values\n",
    "    groups = data['user'].values\n",
    "    \n",
    "    return X_feature,X_static,X_stress_episode,X_quit_episode,X_activity_episode,X_smoking_episode,y_time,y,groups\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# filepath_file = './data/episode_encoded_lagged_data/episode_encoded_'+'lagged_{}_windows_standardized_phenotype_with_episode_cluster_check_2'\n",
    "\n",
    "\n",
    "for n_lag in [15]:\n",
    "    for obs in [30]:\n",
    "        for n_cluster in [4]:\n",
    "            # try:\n",
    "            if not os.path.isfile('./data/output_final_all/result_only_episode_lag_{}_obs_{}_triplet_loss_cluster_{}.p'.format(n_lag,obs,n_cluster)):\n",
    "                print(n_lag,obs,n_cluster,'done')\n",
    "                continue\n",
    "            else:\n",
    "                print(n_lag,obs,n_cluster)\n",
    "                filepath_file = './data/episode_encoded_lagged_data_with_episode/episode_encoded_'+'lagged_'+str(n_lag)+'_obs_{}'.format(obs)+'_windows_with_episode_cluster_check_{}'.format(n_cluster)\n",
    "                use_standardization = True\n",
    "                all_data = []\n",
    "                columns = ['alpha_stress','alpha_smoking','alpha_activity']\n",
    "                amplitude_duration_data = []\n",
    "                amplitude_duration_columns = ['amplitude_stress','duration_stress'] \n",
    "                data_cluster = pickle.load(open(filepath_file.format(n_lag),'rb'))\n",
    "                temp = data_cluster.groupby(['user','cluster_label']).count().index.values\n",
    "                users = np.array([a[0] for a in temp])\n",
    "                labels = np.array([a[1] for a in temp])\n",
    "                cluster_dict = {}\n",
    "                for i,a in enumerate(users):\n",
    "                    cluster_dict[a] = labels[i]\n",
    "                n_groups = len(np.unique(users))\n",
    "                # n_groups = 20\n",
    "                X_feature,X_static,X_stress_episode,X_quit_episode,X_activity_episode,X_smoking_episode,y_time,y,groups = get_X_y_groups(n_lag)\n",
    "                # y[y<0] = 0\n",
    "                X_feature = np.nan_to_num(X_feature)\n",
    "                y = np.float32(y)\n",
    "                indexes = get_train_test_indexes(groups,n_groups_split = 10,n_val_groups=5)\n",
    "                final_y_time = []\n",
    "                final_probs = []\n",
    "                final_y = []\n",
    "                final_groups = []\n",
    "                bias_dict = {}\n",
    "                val_results = {}\n",
    "                for kk,yyyy in enumerate(indexes[1:]):\n",
    "                    train_index,test_index,val_index = yyyy\n",
    "                    \n",
    "                    X_feature_train,X_feature_test = X_feature[train_index],X_feature[test_index]\n",
    "                    X_static_train,X_static_test = X_static[train_index],X_static[test_index]\n",
    "                    X_stress_episode_train,X_stress_episode_test = X_stress_episode[train_index], X_stress_episode[test_index]\n",
    "                    X_quit_episode_train,X_quit_episode_test = X_quit_episode[train_index], X_quit_episode[test_index]\n",
    "                    X_activity_episode_train,X_activity_episode_test = X_activity_episode[train_index], X_activity_episode[test_index]\n",
    "                    X_smoking_episode_train,X_smoking_episode_test = X_smoking_episode[train_index], X_smoking_episode[test_index]\n",
    "                    y_train,y_test,groups_train,groups_test,time_train,time_test = y[train_index],y[test_index],groups[train_index],groups[test_index],y_time[train_index],y_time[test_index]\n",
    "                    \n",
    "                    X_feature_val,X_static_val,X_stress_episode_val,X_quit_episode_val,\\\n",
    "                    X_activity_episode_val,X_smoking_episode_val,y_val,groups_val,time_val = X_feature[val_index],X_static[val_index],X_stress_episode[val_index],X_quit_episode[val_index],\\\n",
    "                                                                                            X_activity_episode[val_index],X_smoking_episode[val_index],y[val_index],groups[val_index],y_time[val_index]\n",
    "                    \n",
    "                    # y_train = np.array(y_train).reshape(len(y_train),-1)\n",
    "                    positive_train_index = np.where(y_train==1)[0]\n",
    "                    negative_train_index = np.where(y_train==0)[0]\n",
    "                    len_positive = len(positive_train_index)\n",
    "                    n_iters = 1\n",
    "                    test_preds = []\n",
    "                    bias_pred = []\n",
    "                    for i,n_iter in enumerate(range(n_iters)):\n",
    "                        np.random.seed(np.random.randint(109))\n",
    "                        indexes_sampled = np.array(list(positive_train_index)+list(np.random.choice(negative_train_index,len_positive,replace=False)))\n",
    "                        train_feature = X_feature_train[indexes_sampled]\n",
    "                        train_static = X_static_train[indexes_sampled]\n",
    "                        train_stress = X_stress_episode_train[indexes_sampled]\n",
    "                        train_quit = X_quit_episode_train[indexes_sampled]\n",
    "                        train_activity = X_activity_episode_train[indexes_sampled]\n",
    "                        train_smoking = X_smoking_episode_train[indexes_sampled]\n",
    "                        train_y = y_train[indexes_sampled]\n",
    "                        from keras import backend as K\n",
    "                        K.clear_session()\n",
    "                        model = get_model()\n",
    "                        # model.summary()\n",
    "\n",
    "                        filepath = './models/lag_'+str(n_lag)+'_iter_'+str(n_iter)+'_temp3.hdf5'\n",
    "                        checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min',save_weights_only=False)\n",
    "                        es = EarlyStopping(monitor='loss', mode='min', verbose=1,patience=60)\n",
    "                        callbacks_list = [es,checkpoint]\n",
    "                        train_feature,val_feature,train_static,val_static,train_stress,val_stress, \\\n",
    "                        train_smoking,val_smoking,train_quit,val_quit,train_activity,val_activity, \\\n",
    "                        train_y,val_y = train_test_split(\n",
    "                                                        train_feature,\n",
    "                                                        train_static,\n",
    "                                                        train_stress,\n",
    "                                                        train_smoking,\n",
    "                                                        train_quit,\n",
    "                                                        train_activity,\n",
    "                                                        train_y,\n",
    "                                                        test_size=.15,\n",
    "                                                        stratify=train_y\n",
    "                                                        )\n",
    "                        val_feature = np.concatenate([val_feature,X_feature_val])\n",
    "                        val_static = np.concatenate([val_static,X_static_val])\n",
    "                        val_stress = np.concatenate([val_stress,X_stress_episode_val])\n",
    "                        val_activity = np.concatenate([val_activity,X_activity_episode_val])\n",
    "                        val_smoking = np.concatenate([val_smoking,X_smoking_episode_val])\n",
    "                        val_quit = np.concatenate([val_quit,X_quit_episode_val])\n",
    "                        val_y = np.array(list(val_y)+list(y_val))\n",
    "\n",
    "                        # K.set_value(model.optimizer.learning_rate, 0.0001)\n",
    "                        model.fit([train_feature,train_static,train_stress,train_activity,train_smoking,train_quit],train_y,\n",
    "                                                epochs=200, batch_size=50,\n",
    "                                                            verbose=1,callbacks=callbacks_list,shuffle=True)\n",
    "                        # model.get_layer('normalize').centers\n",
    "                        \n",
    "                        # model.fit([train_feature,train_static,train_stress,train_activity,train_smoking,train_quit],train_y,\n",
    "                        # validation_data=([val_feature,val_static,val_stress,val_activity,val_smoking,val_quit],val_y), epochs=30, batch_size=80,\n",
    "                        #             verbose=1,callbacks=callbacks_list,shuffle=True)\n",
    "                        if os.path.isfile(filepath):\n",
    "                            model.load_weights(filepath)\n",
    "                        # temp_model = Model(model.input[1],[model.get_layer(c).output for c in columns+amplitude_duration_columns])\n",
    "                        # X_static_test_new = np.hstack((X_static_test, np.zeros((X_static_test.shape[0], 3), dtype=X_static_test.dtype)))\n",
    "                        # X_static_test_new[:,-3] = np.mean(X_stress_episode_test[:,:,0,1],axis=1)\n",
    "                        # X_static_test_new[:,-2] = np.mean(X_stress_episode_test[:,:,0,2],axis=1)\n",
    "                        # X_static_test_new[:,-1] = [cluster_dict[a] for a in groups_test]\n",
    "                        # X_static_test_new = np.unique(X_static_test_new,axis=0)\n",
    "                        # pred_list = list(zip(*[list(a.reshape(-1)) for a in temp_model.predict(X_static_test_new[:,:-3])]))\n",
    "                        # for ii,a in enumerate(pred_list):\n",
    "                        #     for jj,c_name in enumerate(columns+amplitude_duration_columns):\n",
    "                        #         all_data.append([a[jj],\n",
    "                        #                         c_name,\n",
    "                        #                         X_static_test_new[ii][-1],\n",
    "                        #                         a[-2]*X_static_test_new[ii][-3],\n",
    "                        #                         a[-1]*X_static_test_new[ii][-2],\n",
    "                        #                         n_lag,\n",
    "                        #                         kk,\n",
    "                        #                         i])\n",
    "\n",
    "                        test_preds.append(model.predict([X_feature_test,X_static_test,X_stress_episode_test,\n",
    "                                                    X_activity_episode_test,X_smoking_episode_test,X_quit_episode_test])[0][:,1])\n",
    "                        bias_pred.append(model.predict([X_feature_val,X_static_val,X_stress_episode_val,X_activity_episode_val,\n",
    "                                                        X_smoking_episode_val,X_quit_episode_val])[0][:,1])\n",
    "                        \n",
    "                    from sklearn.preprocessing import MinMaxScaler\n",
    "                    y_test_pred = np.concatenate([a.reshape(-1,1) for a in test_preds],axis=1)\n",
    "                    y_test_check = np.int64(np.array(y_test).reshape(-1))\n",
    "                    import seaborn as sns\n",
    "                    for lll in np.arange(y_test_pred.shape[-1]):\n",
    "                        df_check = pd.DataFrame({'original':y_test_check,'predicted':y_test_pred[:,lll]})\n",
    "                        plt.figure()\n",
    "                        sns.kdeplot(data=df_check,x='predicted',hue='original')\n",
    "                        plt.show()\n",
    "                        # df_check['predicted']\n",
    "                        plt.figure()\n",
    "                        plt.hist(y_test_pred[:,lll])\n",
    "                        plt.show()\n",
    "                        distance_80th = np.percentile(y_test_pred[y_test_check==1,lll],20)\n",
    "                        negative_within_circle = y_test_pred[np.where((y_test_check==0)&(y_test_pred[:,lll]>distance_80th))[0],lll]\n",
    "                        print(negative_within_circle.shape[0]/np.where(y_test_check==0)[0].shape[0],'Negative within positive')\n",
    "                        print('-'*10)\n",
    "                    bias_pred = np.concatenate([a.reshape(-1,1) for a in bias_pred],axis=1)\n",
    "                    # print(roc_auc_score(y_test,y_test_pred))\n",
    "                    final_y_time.extend(list(time_test))\n",
    "                    final_probs.extend(list(y_test_pred))\n",
    "                    final_y.extend(list(y_test))\n",
    "                    final_groups.extend(list(groups_test))\n",
    "                    for group_b in np.unique(groups_test):\n",
    "                        bias_dict[group_b] = []\n",
    "                        for kkk in range(bias_pred.shape[1]):\n",
    "                            f1,bias = f1Bias_scorer_CV(bias_pred[:,kkk].reshape(-1),np.int64(y_val.reshape(-1)))\n",
    "                            \n",
    "                            f1_test, bias_test = f1Bias_scorer_CV(y_test_pred[:,kkk],np.int64(y_test.reshape(-1)))\n",
    "                            # plt.hist(y_test_pred[:,kkk],100)\n",
    "                            # plt.show()\n",
    "                            if kkk==0:\n",
    "                                print(group_b,\n",
    "                                'val results',\n",
    "                                f1,bias,\n",
    "                                'index',kkk,\n",
    "                                'test results',\n",
    "                                f1_test,bias_test,\n",
    "                                'test result with val bias',\n",
    "                                f1_score(y_test,np.array(y_test_pred[:,kkk]>=bias,dtype=np.int64)))\n",
    "                            \n",
    "                            bias_dict[group_b].append(bias)\n",
    "                        val_results[group_b] = [time_val,bias_pred,y_val,groups_val]\n",
    "                    print(len(np.unique(final_groups)))\n",
    "                    # print(bias_dict)\n",
    "                final_y_time,final_probs,final_y,final_groups = np.array(final_y_time),np.array(final_probs),np.array(final_y),np.array(final_groups)\n",
    "                pickle.dump([final_y_time,final_probs,final_y,final_groups,bias_dict,val_results],open('./data/output_final_all/result_only_episode_lag_{}_obs_{}_triplet_loss_cluster__{}.p'.format(n_lag,obs,n_cluster),'wb'))\n",
    "            # except:\n",
    "            #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "for lll in np.arange(y_test_pred.shape[-1]):\n",
    "    df_check = pd.DataFrame({'original':y_test_check,'predicted':y_test_pred[:,lll]})\n",
    "    plt.figure()\n",
    "    # plt.hist(y_test_pred[:,lll])\n",
    "    sns.kdeplot(data=df_check,x='predicted',hue='original')\n",
    "    plt.show()\n",
    "    distance_80th = np.percentile(y_test_pred[y_test_check==1,lll],20)\n",
    "    negative_within_circle = y_test_pred[np.where((y_test_check==0)&(y_test_pred[:,lll]>=distance_80th))[0],lll]\n",
    "    print(distance_80th,negative_within_circle.shape[0]/np.where(y_test_check==0)[0].shape[0],'Negative within positive')\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check['predicted']\n",
    "# .plot(kind='hist')\n",
    "# plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from tensorflow_addons.losses import metric_learning\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "num_classes = 2\n",
    "feature_dim = 20\n",
    "alpha = .2\n",
    "# centers = K.zeros([num_classes, feature_dim])\n",
    "centers = tf.Variable(\n",
    "            tf.zeros([num_classes, feature_dim]),\n",
    "            name=\"centers\",\n",
    "            trainable=False,\n",
    "            # in a distributed strategy, we want updates to this variable to be summed.\n",
    "            aggregation=tf.VariableAggregation.SUM,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "features = tf.cast(tf.convert_to_tensor(np.random.randn(10,feature_dim)),tf.float32)\n",
    "labels = tf.convert_to_tensor(np.array([0]*5+[1]*5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "precise_embeddings = tf.cast(tf.convert_to_tensor(np.random.randn(10,feature_dim)),tf.float32)\n",
    "labels = tf.convert_to_tensor(np.array([0]*3+[1]*7))\n",
    "\n",
    "lshape = tf.shape(labels)\n",
    "labels = tf.reshape(labels, [lshape[0], 1])\n",
    "mask_for_equal = tf.math.equal(tf.cast(labels,tf.bool),tf.transpose(tf.cast(labels,tf.bool)))\n",
    "only_positive = tf.repeat(tf.transpose(tf.reshape(tf.cast(labels,tf.bool),[10,1])),10,axis=0)\n",
    "mask_only_positive = tf.math.multiply(tf.cast(mask_for_equal,tf.float32),tf.cast(only_positive,tf.float32))\n",
    "\n",
    "# mask_negative_to_positive = tf.math.logical_and(only_positive,tf.math.logical_not(tf.cast(mask_only_positive,tf.bool)))\n",
    "\n",
    "pdist_matrix = metric_learning.pairwise_distance(\n",
    "            precise_embeddings, squared=False\n",
    "        )\n",
    "# positive_only_dist = tf.math.multiply(pdist_matrix,mask_only_positive)\n",
    "# consistency = tf.reduce_sum(positive_only_dist)/tf.reduce_sum(mask_only_positive)\n",
    "\n",
    "positive_only_dist = tf.boolean_mask(pdist_matrix, tf.cast(mask_only_positive,tf.bool))\n",
    "\n",
    "distance_95 = tf.reshape(positive_only_dist,(-1,))\n",
    "samples1 = tf.cast(tf.shape(labels)[0], tf.float32) #batch size\n",
    "p = 80\n",
    "p =  2*(100. - p)/100.  #100% will return 0 elements, 0% will return all elements\n",
    "samples = tf.cast(tf.math.floor(p * samples1), tf.int32)\n",
    "values, indices = tf.math.top_k(distance_95, samples)\n",
    "positive_dist_95th = tf.reduce_min(values)\n",
    "\n",
    "sparse_labels = tf.reshape(labels, (-1,))\n",
    "centers_batch = tf.gather(centers, tf.cast(sparse_labels,tf.int32))\n",
    "diff = (1 - .9) * (centers_batch - precise_embeddings)\n",
    "updates = tf.scatter_nd(tf.cast(sparse_labels[:, None],tf.int32), diff, centers.shape)\n",
    "centers.assign_add(updates)\n",
    "centers_batch = tf.gather(centers, tf.ones(sparse_labels.shape,dtype=tf.int32))\n",
    "distance_to_positive_center = tf.norm(precise_embeddings-centers_batch,2,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_to_positive_center_smaller_than_95 = tf.where(distance_to_positive_center<positive_dist_95th,1,0)\n",
    "negative_only = tf.math.logical_not(tf.cast(sparse_labels,tf.bool))\n",
    "negative_within_circle = tf.reduce_sum(tf.boolean_mask(distance_to_positive_center_smaller_than_95,negative_only))\n",
    "total_negative = tf.shape(sparse_labels)[0] - tf.reduce_sum(sparse_labels)\n",
    "rare_loss = negative_within_circle/(total_negative+K.epsilon())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.norm(distance_to_positive_center,2,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_labels = tf.reshape(labels, (-1,))\n",
    "centers_batch = tf.gather(centers, tf.cast(sparse_labels,tf.int32))\n",
    "diff = (1 - .9) * (centers_batch - precise_embeddings)\n",
    "updates = tf.scatter_nd(tf.cast(sparse_labels[:, None],tf.int32), diff, centers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers.assign_add(updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.math.multiply(pdist_matrix,mask_only_positive)\n",
    "# tf.reduce_sum(mask_only_positive)\n",
    "consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist_matrix = metric_learning.pairwise_distance(\n",
    "            precise_embeddings, squared=False\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency = tf.math.equal(labels, tf.transpose(labels))\n",
    "adjacency_not = tf.math.logical_not(adjacency)\n",
    "batch_size = tf.size(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.math.multiply(adjacency,tf.cast(labels,tf.bool))\n",
    "mask_for_equal = tf.math.equal(tf.cast(labels,tf.bool),tf.transpose(tf.cast(labels,tf.bool)))\n",
    "only_positive = tf.repeat(tf.transpose(tf.reshape(tf.cast(labels,tf.bool),[10,1])),10,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.math.multiply(tf.reshape(tf.cast(labels,tf.bool),[10,1]),mask_for_positive)\n",
    "tf.math.multiply(tf.cast(mask_for_equal,tf.int32),tf.cast(only_positive,tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist_matrix_tile = tf.tile(pdist_matrix, [batch_size, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist_matrix_tile = tf.tile(pdist_matrix, [batch_size, 1])\n",
    "mask = tf.math.logical_and(\n",
    "    tf.tile(adjacency_not, [batch_size, 1]),\n",
    "    tf.math.greater(\n",
    "        pdist_matrix_tile, tf.reshape(tf.transpose(pdist_matrix), [-1, 1])\n",
    "    ),\n",
    ")\n",
    "mask_final = tf.reshape(\n",
    "    tf.math.greater(\n",
    "        tf.math.reduce_sum(\n",
    "            tf.cast(mask, dtype=tf.dtypes.float32), 1, keepdims=True\n",
    "        ),\n",
    "        0.0,\n",
    "    ),\n",
    "    [batch_size, batch_size],\n",
    ")\n",
    "mask_final = tf.transpose(mask_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51b244ab9aca612e739a0539ae1af887c58db9e180d786deb0ab1761def69c1f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('test1': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
