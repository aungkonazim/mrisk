{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime,timedelta\n",
    "from scipy.stats import iqr,skew,kurtosis,mode\n",
    "from joblib import Parallel,delayed\n",
    "import zipfile\n",
    "import shutil\n",
    "from sklearn.feature_selection import SelectKBest,f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score,r2_score,classification_report\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,RandomForestRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV,StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "seed = 100\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from sklearn.model_selection import train_test_split,LeavePGroupsOut\n",
    "from keras.backend import expand_dims, repeat_elements\n",
    "from keras.layers import Lambda\n",
    "from tensorflow.keras.layers import Conv1D,BatchNormalization,Dropout,InputLayer,MaxPooling1D,Flatten,RepeatVector,Dense,Input,Activation,GRU,Bidirectional,LSTM\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow_addons as tfa\n",
    "import warnings\n",
    "import functools\n",
    "from tensorflow_addons.losses import metric_learning\n",
    "import tensorflow as tf\n",
    "\n",
    "# tf.config.run_functions_eagerly(False)\n",
    "warnings.filterwarnings('ignore')\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def get_X_y_groups(n_lag=10):\n",
    "    data = pickle.load(open(filepath_file.format(n_lag),'rb'))\n",
    "\n",
    "    X_feature = np.concatenate(data.feature_final.values)\n",
    "    X_static =  np.concatenate(data.static_features.values)\n",
    "\n",
    "    X_stress_episode = np.concatenate(data.stress_episode.values)\n",
    "    X_quit_episode = np.concatenate(data.quit_episode.values)\n",
    "    X_activity_episode = np.concatenate(data.activity_episode.values)\n",
    "    X_smoking_episode = np.concatenate(data.smoking_episode.values)\n",
    "\n",
    "    y_time = data['time'].values\n",
    "    y = data['label'].values\n",
    "    groups = data['user'].values\n",
    "    \n",
    "    return X_feature,X_static,X_stress_episode,X_quit_episode,X_activity_episode,X_smoking_episode,y_time,y,groups\n",
    "\n",
    "\n",
    "def get_train_test_indexes(groups,n_groups_split = 10,n_val_groups = 10):\n",
    "    groups_unique = np.unique(groups)\n",
    "    groups_split = np.array_split(groups_unique,n_groups_split)\n",
    "    indexes = []\n",
    "    for this_groups in groups_split:\n",
    "        train_groups = np.array([a for a in groups_unique if a not in this_groups])\n",
    "        val_groups = np.random.choice(train_groups,n_val_groups)\n",
    "        train_groups = np.array([a for a in groups_unique if a not in list(this_groups)+list(val_groups)])\n",
    "        test_groups = this_groups\n",
    "        train_index,test_index = np.array([i for i,a in enumerate(groups) \n",
    "                                           if a in train_groups]),np.array([i for i,a in enumerate(groups) \n",
    "                                                                               if a in test_groups])\n",
    "        val_index = np.array([i for i,a in enumerate(groups) \n",
    "                                           if a in val_groups])\n",
    "        indexes.append([train_index,test_index,val_index])\n",
    "    return indexes\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "def f1Bias_scorer_CV(probs, y, ret_bias=True):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "\n",
    "    f1,bias = 0.0,.5\n",
    "    min_recall = .8\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0) and thresholds[i]>0.2:\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "\n",
    "class CenterLossLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_classes, n_features, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.n_features = n_features\n",
    "        self.centers = tf.Variable(\n",
    "            tf.zeros([n_classes, n_features]),\n",
    "            name=\"centers\",\n",
    "            trainable=False,\n",
    "            # in a distributed strategy, we want updates to this variable to be summed.\n",
    "            aggregation=tf.VariableAggregation.MEAN,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        # pass through layer\n",
    "        return tf.identity(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"n_classes\": self.n_classes, \"n_features\": self.n_features})\n",
    "        return config\n",
    "\n",
    "\n",
    "class CenterLoss(tf.keras.losses.Loss):\n",
    "    def __init__(\n",
    "        self,\n",
    "        centers_layer,\n",
    "        alpha=0.9,\n",
    "        update_centers=True,\n",
    "        p = 80,\n",
    "        name=\"center_loss\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.centers_layer = centers_layer\n",
    "        self.centers = self.centers_layer.centers\n",
    "        self.alpha = alpha\n",
    "        self.update_centers = update_centers\n",
    "        self.p = p\n",
    "        \n",
    "    def consistency_loss(self,labels,precise_embeddings):\n",
    "        \n",
    "        lshape = tf.shape(labels)\n",
    "        labels = tf.reshape(labels, [lshape[0], 1])\n",
    "        mask_for_equal = tf.math.equal(tf.cast(labels,tf.bool),tf.transpose(tf.cast(labels,tf.bool)))\n",
    "        only_positive = tf.repeat(tf.transpose(tf.reshape(tf.cast(labels,tf.bool),[lshape[0],1])),lshape[0],axis=0)\n",
    "        mask_only_positive = tf.math.multiply(tf.cast(mask_for_equal,tf.float32),tf.cast(only_positive,tf.float32))\n",
    "        \n",
    "        pdist_matrix = metric_learning.pairwise_distance(\n",
    "                    precise_embeddings, squared=False\n",
    "                )\n",
    "        positive_only_dist = tf.boolean_mask(pdist_matrix, tf.cast(mask_only_positive,tf.bool))\n",
    "        no_of_positives = tf.cast(tf.reduce_sum(labels),tf.int32)\n",
    "        positive_only_dist = tf.reshape(positive_only_dist,[no_of_positives,no_of_positives])\n",
    "        positive_only_dist = tf.reduce_mean(positive_only_dist,axis=1)\n",
    "        \n",
    "        distance_95 = tf.reshape(positive_only_dist,(-1,))\n",
    "        samples1 = tf.cast(no_of_positives, tf.float32) #batch size\n",
    "        p = (100. - self.p)/100.  #100% will return 0 elements, 0% will return all elements\n",
    "        samples = tf.cast(tf.math.floor(p * samples1), tf.int32)\n",
    "        values, indices = tf.math.top_k(distance_95, samples)\n",
    "        positive_dist_95th = tf.reduce_min(values)\n",
    "        return positive_dist_95th, pdist_matrix\n",
    "    \n",
    "    \n",
    "    def compute_rare_loss_v1(self,labels,pdist_matrix,positive_dist_95th):\n",
    "        lshape = tf.shape(labels)\n",
    "        labels = tf.reshape(labels, [lshape[0], 1])\n",
    "        mask_for_equal = tf.math.equal(tf.cast(labels,tf.bool),tf.transpose(tf.cast(labels,tf.bool)))\n",
    "        only_negative = tf.cast(tf.repeat(tf.transpose(tf.reshape(tf.cast(1-labels,tf.bool),[lshape[0],1])),lshape[0],axis=0),tf.int32)\n",
    "        mask_negative_to_positive = tf.math.logical_not(tf.cast(tf.math.add(tf.cast(mask_for_equal,tf.int32),only_negative),tf.bool))\n",
    "        no_of_positives = tf.cast(tf.reduce_sum(labels),tf.int32)\n",
    "        no_of_negatives = lshape[0] - no_of_positives\n",
    "        negative_to_positive_distance = tf.reshape(tf.boolean_mask(pdist_matrix,mask_negative_to_positive),[no_of_negatives,no_of_positives])\n",
    "        average_neg_to_pos_distance = tf.reduce_mean(negative_to_positive_distance,axis=1)\n",
    "        less_distance_mask = tf.reduce_sum(tf.where(average_neg_to_pos_distance<=positive_dist_95th,1,0))\n",
    "        return tf.cast(less_distance_mask,tf.float32)/(tf.cast(no_of_negatives,tf.float32)+K.epsilon())\n",
    "        \n",
    "    def compute_rare_loss(self,labels,precise_embeddings,positive_dist_95th):\n",
    "        sparse_labels = tf.reshape(labels, (-1,))\n",
    "        centers_batch = tf.gather(self.centers, tf.ones([tf.shape(sparse_labels)[0],],dtype=tf.int32))\n",
    "        distance_to_positive_center = tf.norm(precise_embeddings-centers_batch,2,axis=1)\n",
    "        distance_to_positive_center_smaller_than_95 = tf.where(distance_to_positive_center<=positive_dist_95th,1,0)\n",
    "        negative_only = tf.math.logical_not(tf.cast(sparse_labels,tf.bool))\n",
    "        negative_within_circle = tf.reduce_sum(tf.boolean_mask(distance_to_positive_center_smaller_than_95,negative_only))\n",
    "        total_negative = tf.cast(tf.shape(sparse_labels)[0],tf.float32) - tf.reduce_sum(sparse_labels)\n",
    "        rare_loss = tf.cast(negative_within_circle,tf.float32)/(total_negative+K.epsilon())\n",
    "        return rare_loss\n",
    "    \n",
    "    def center_loss_positive_compute(self,labels,precise_embeddings,centers_batch):\n",
    "        loss_row = tf.reduce_mean(tf.math.square(precise_embeddings - centers_batch),axis=1)\n",
    "        loss_row_only_positive = tf.math.multiply(tf.cast(labels,tf.float32),loss_row)\n",
    "        center_loss_positive = tf.reduce_sum(loss_row_only_positive)/(tf.reduce_sum(labels)+K.epsilon())\n",
    "        return center_loss_positive\n",
    "    \n",
    "    def compute_positive_to_negative_loss(self,labels,precise_embeddings):\n",
    "        sparse_labels = tf.reshape(labels, (-1,))\n",
    "        centers_batch = tf.gather(self.centers, tf.ones([tf.shape(sparse_labels)[0],],dtype=tf.int32))\n",
    "        distance_to_positive_center = tf.norm(precise_embeddings-centers_batch,2,axis=1)\n",
    "        negative_only = tf.math.logical_not(tf.cast(sparse_labels,tf.bool))\n",
    "        return tf.reduce_min(tf.boolean_mask(distance_to_positive_center,negative_only))\n",
    "    \n",
    "    \n",
    "    def call(self, sparse_labels, prelogits):\n",
    "        sparse_labels = tf.reshape(sparse_labels, (-1,))\n",
    "        # the reduction of batch dimension will be done by the parent class\n",
    "        # center_loss = tf.keras.losses.mean_squared_error(prelogits, centers_batch)\n",
    "        # centers_batch = tf.gather(self.centers, tf.cast(sparse_labels,tf.int32))\n",
    "        # if self.update_centers and tf.cast(tf.reduce_sum(sparse_labels),tf.int32)<tf.shape(sparse_labels)[0]:\n",
    "        #     diff = (1 - self.alpha) * (centers_batch - prelogits)\n",
    "        #     updates = tf.scatter_nd(tf.cast(sparse_labels[:, None],tf.int32), diff, self.centers.shape)\n",
    "        #     self.centers.assign_sub(updates)\n",
    "        distance_95th, pdist_matrix = self.consistency_loss(sparse_labels,prelogits)\n",
    "        rare_loss = self.compute_rare_loss_v1(sparse_labels,pdist_matrix,distance_95th)\n",
    "        # center_loss_positive = self.center_loss_positive_compute(sparse_labels,prelogits,centers_batch)        \n",
    "        # rare_loss = self.compute_rare_loss(sparse_labels,prelogits,positive_dist_95th=distance_95th)\n",
    "        # postive_to_negative_distance = self.compute_positive_to_negative_loss(sparse_labels,prelogits)\n",
    "        return tf.square(rare_loss-.2) + distance_95th*.2\n",
    "        # return rare_loss\n",
    "    \n",
    "def get_model():\n",
    "    n_t,n_f = train_feature.shape[1],train_feature.shape[2]\n",
    "    print(n_t,n_f)\n",
    "    x_input = Input(shape=(n_t,n_f))\n",
    "    x_feature = Conv1D(100,1,activation='linear')(x_input)\n",
    "    x_feature = Conv1D(100,1,activation='tanh')(x_feature)\n",
    "    # x_feature = Dropout(.2)(x_feature)\n",
    "    x_feature = LSTM(10,activation='tanh',return_sequences=False)(x_feature)\n",
    "    x_feature = Dropout(.2)(x_feature)\n",
    "    x_feature = Flatten()(x_feature)\n",
    "    x_feature = Dense(10,activation='relu')(x_feature)\n",
    "    \n",
    "    n_sf = train_static.shape[1]\n",
    "    x_input_static = Input(shape=(n_sf))\n",
    "    x_static = Dense(10,activation='relu')(x_input_static)\n",
    "    # x_static = Dense(10,activation='relu')(x_static)\n",
    "    n_timesteps = train_stress.shape[-2]\n",
    "    n_episodes_stress,n_episodes_quit,n_episodes_activity,n_episodes_smoking = train_stress.shape[1],train_quit.shape[1],train_activity.shape[1],train_smoking.shape[1]\n",
    "    \n",
    "    x_alpha_stress = Dense(1,activation='sigmoid',name='alpha_stress')(x_static)\n",
    "    x_alpha_stress = RepeatVector(n_timesteps)(x_alpha_stress)\n",
    "    x_alpha_stress = Lambda(lambda x: repeat_elements(expand_dims(x, axis=1), n_episodes_stress, 1))(x_alpha_stress)\n",
    "    \n",
    "    x_alpha_quit = Dense(1,activation='sigmoid',name='alpha_quit')(x_static)\n",
    "    x_alpha_quit = RepeatVector(n_timesteps)(x_alpha_quit)\n",
    "    x_alpha_quit = Lambda(lambda x: repeat_elements(expand_dims(x, axis=1), n_episodes_quit, 1))(x_alpha_quit)\n",
    "    \n",
    "    x_alpha_activity = Dense(1,activation='sigmoid',name='alpha_activity')(x_static)\n",
    "    x_alpha_activity = RepeatVector(n_timesteps)(x_alpha_activity)\n",
    "    x_alpha_activity = Lambda(lambda x: repeat_elements(expand_dims(x, axis=1), n_episodes_activity, 1))(x_alpha_activity)\n",
    "    \n",
    "    x_alpha_smoking = Dense(1,activation='sigmoid',name='alpha_smoking')(x_static)\n",
    "    x_alpha_smoking = RepeatVector(n_timesteps)(x_alpha_smoking)\n",
    "    x_alpha_smoking = Lambda(lambda x: repeat_elements(expand_dims(x, axis=1), n_episodes_smoking, 1))(x_alpha_smoking)\n",
    "\n",
    "    n_dim = 3\n",
    "    x_stress = Input(shape=(n_episodes_stress,n_timesteps,n_dim))\n",
    "    stress_alpha_time = tf.math.multiply(x_alpha_stress[:,:,:,0]*-1,x_stress[:,:,:,0])\n",
    "    stress_alpha_time_exp = tf.math.exp(stress_alpha_time)\n",
    "\n",
    "    x_stress_amplitude = x_stress[:,:,:,1]\n",
    "    stress_amplitude_coeff = Dense(1,activation='relu',name='amplitude_stress')(x_static)\n",
    "    stress_amplitude_coeff = Lambda(lambda x: expand_dims(x, axis=2))(stress_amplitude_coeff)\n",
    "    print(stress_amplitude_coeff.shape,'coeff',x_stress_amplitude.shape,'amplitude')\n",
    "    x_stress_amplitude = tf.math.multiply(x_stress_amplitude,stress_amplitude_coeff)\n",
    "    x_stress_duration = x_stress[:,:,:,2]\n",
    "    stress_duration_coeff = Dense(1,activation='sigmoid',name='duration_stress')(x_static)\n",
    "    stress_duration_coeff = Lambda(lambda x: expand_dims(x, axis=2))(stress_duration_coeff)\n",
    "    x_stress_duration = tf.math.multiply(x_stress_duration,stress_duration_coeff)\n",
    "    x_stress_all = tf.math.add(x_stress_amplitude,x_stress_duration)\n",
    "    stress_alpha_time_exp_amplitude = tf.math.multiply(stress_alpha_time_exp,x_stress_all)\n",
    "    stress_final = tf.math.reduce_sum(stress_alpha_time_exp_amplitude,axis=1)\n",
    "    stress_final = Lambda(lambda x: expand_dims(x, axis=2))(stress_final)\n",
    "    # stress_final = LSTM(10,activation='tanh',return_sequences=True)(stress_final)\n",
    "    \n",
    "    \n",
    "    \n",
    "    x_quit = Input(shape=(n_episodes_quit,n_timesteps,n_dim))\n",
    "\n",
    "    x_smoking = Input(shape=(n_episodes_smoking,n_timesteps,n_dim))\n",
    "    smoking_alpha_time = tf.math.multiply(x_alpha_smoking[:,:,:,0]*-1,x_smoking[:,:,:,0])\n",
    "    smoking_alpha_time_exp = tf.math.exp(smoking_alpha_time)\n",
    "    x_smoking_amplitude = x_smoking[:,:,:,1]\n",
    "    smoking_amplitude_coeff = Dense(1,activation='sigmoid',name='amplitude_smoking')(x_static)\n",
    "    smoking_amplitude_coeff = Lambda(lambda x: expand_dims(x, axis=2))(smoking_amplitude_coeff)\n",
    "    x_smoking_amplitude = tf.math.multiply(x_smoking_amplitude,smoking_amplitude_coeff)\n",
    "    x_smoking_duration = x_smoking[:,:,:,2]\n",
    "    smoking_duration_coeff = Dense(1,activation='sigmoid',name='duration_smoking')(x_static)\n",
    "    smoking_duration_coeff = Lambda(lambda x: expand_dims(x, axis=2))(smoking_duration_coeff)\n",
    "    x_smoking_duration = tf.math.multiply(x_smoking_duration,smoking_duration_coeff)\n",
    "    x_smoking_all = tf.math.add(x_smoking_amplitude,x_smoking_duration)\n",
    "    smoking_alpha_time_exp_amplitude = tf.math.multiply(smoking_alpha_time_exp,x_smoking_all)\n",
    "    smoking_final = tf.math.reduce_sum(smoking_alpha_time_exp_amplitude,axis=1)\n",
    "    smoking_final = Lambda(lambda x: expand_dims(x, axis=2))(smoking_final)\n",
    "    # smoking_final = LSTM(10,activation='tanh',return_sequences=True)(smoking_final)\n",
    "\n",
    "    \n",
    "    \n",
    "    x_activity = Input(shape=(n_episodes_activity,n_timesteps,n_dim))\n",
    "    activity_alpha_time = tf.math.multiply(x_alpha_activity[:,:,:,0]*-1,x_activity[:,:,:,0])\n",
    "    activity_alpha_time_exp = tf.math.exp(activity_alpha_time)\n",
    "    x_activity_amplitude = x_activity[:,:,:,1]\n",
    "    activity_amplitude_coeff = Dense(1,activation='sigmoid',name='amplitude_activity')(x_static)\n",
    "    activity_amplitude_coeff = Lambda(lambda x: expand_dims(x, axis=2))(activity_amplitude_coeff)\n",
    "    x_activity_amplitude = tf.math.multiply(x_activity_amplitude,activity_amplitude_coeff)\n",
    "    x_activity_duration = x_activity[:,:,:,2]\n",
    "    activity_duration_coeff = Dense(1,activation='sigmoid',name='duration_activity')(x_static)\n",
    "    activity_duration_coeff = Lambda(lambda x: expand_dims(x, axis=2))(activity_duration_coeff)\n",
    "    x_activity_duration = tf.math.multiply(x_activity_duration,activity_duration_coeff)\n",
    "    x_activity_all = tf.math.add(x_activity_amplitude,x_activity_duration)\n",
    "    activity_alpha_time_exp_amplitude = tf.math.multiply(activity_alpha_time_exp,x_activity_all)\n",
    "    activity_final = tf.math.reduce_sum(activity_alpha_time_exp_amplitude,axis=1)\n",
    "    activity_final = Lambda(lambda x: expand_dims(x, axis=2))(activity_final)\n",
    "    # activity_final = LSTM(10,activation='tanh',return_sequences=True)(activity_final)\n",
    "    \n",
    "    \n",
    "    x_episode = tf.concat([activity_final, stress_final, smoking_final],2)\n",
    "    x_episode = LSTM(10,activation='tanh',return_sequences=False)(x_episode)\n",
    "    x_episode = Dropout(.1)(x_episode)\n",
    "    x_episode = Flatten()(x_episode)\n",
    "    x_episode = Dense(10,activation='relu')(x_episode)\n",
    "\n",
    "    merged_all = tf.concat([x_feature,x_episode],1)\n",
    "    # print(merged.shape,'concatenated shape')\n",
    "    # merged_all = x_feature\n",
    "    \n",
    "    merged1 = Dense(10,activation='relu',name='normalize2')(merged_all)\n",
    "    merged1 = Lambda(lambda x: K.l2_normalize(x,axis=1))(merged1)\n",
    "    center_layer1 = CenterLossLayer(n_classes=2, n_features=10,name='normalize3')\n",
    "    merged1 = center_layer1(merged1)\n",
    "    \n",
    "    merged = Dense(5,activation='relu',name='norma')(merged1)\n",
    "    # merged = Dense(2,activation='relu',name='normalize1')(merged)\n",
    "    # merged = Lambda(lambda x: K.l2_normalize(x,axis=1))(merged)\n",
    "    center_layer = CenterLossLayer(n_classes=2, n_features=5,name='normalize')\n",
    "    merged = center_layer(merged)\n",
    "    # merged = Dense(10,activation='relu')(merged)\n",
    "    # \n",
    "    # output1 = Dense(2,activation='relu')(merged)\n",
    "    output = Dense(2,activation='softmax',name='softmax')(merged)\n",
    "    # output = Activation('softmax',name='softmax')(merged)\n",
    "    # output = Dense(2,activation='softmax')(merged)\n",
    "    # output = Reshape((-1,1))(output)\n",
    "#     output = Activation('softmax',name='softmax')(output)\n",
    "    model = Model(inputs=[x_input,x_input_static,x_stress,x_activity,x_smoking,x_quit], outputs=[output,merged,merged1])\n",
    "    # myloss = get_center_loss(num_classes=2,feature_dim=merged.shape[-1],alpha=.9)\n",
    "    myloss = CenterLoss(centers_layer=center_layer)\n",
    "    myloss1 = CenterLoss(centers_layer=center_layer1)\n",
    "    \n",
    "    model.compile(\n",
    "        loss={'softmax':tf.losses.SparseCategoricalCrossentropy(),'normalize':myloss,'normalize3':myloss1},\n",
    "        loss_weights = {'softmax':10,'normalize':0,'normalize3':50},\n",
    "        metrics={'softmax':['acc']},\n",
    "        optimizer='adam'\n",
    "    )\n",
    "    return model\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "def get_X_y_groups(n_lag=10):\n",
    "    data = pickle.load(open(filepath_file.format(n_lag),'rb'))\n",
    "\n",
    "    X_feature = np.concatenate(data.feature_final.values)\n",
    "    X_static =  np.concatenate(data.static_features.values)\n",
    "\n",
    "    X_stress_episode = np.concatenate(data.stress_episode.values)\n",
    "    X_quit_episode = np.concatenate(data.quit_episode.values)\n",
    "    X_activity_episode = np.concatenate(data.activity_episode.values)\n",
    "    X_smoking_episode = np.concatenate(data.smoking_episode.values)\n",
    "\n",
    "    y_time = data['time'].values\n",
    "    print(data['label'].unique())\n",
    "    # y =  OneHotEncoder().fit_transform(data['label'].values.reshape(-1,1)).todense()\n",
    "    y = data['label'].values\n",
    "    groups = data['user'].values\n",
    "    \n",
    "    return X_feature,X_static,X_stress_episode,X_quit_episode,X_activity_episode,X_smoking_episode,y_time,y,groups\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# filepath_file = './data/episode_encoded_lagged_data/episode_encoded_'+'lagged_{}_windows_standardized_phenotype_with_episode_cluster_check_2'\n",
    "\n",
    "\n",
    "for n_lag in [15]:\n",
    "    for obs in [30]:\n",
    "        for n_cluster in [4]:\n",
    "            # try:\n",
    "            if not os.path.isfile('./data/output_final_all/result_only_episode_lag_{}_obs_{}_triplet_loss_cluster_{}.p'.format(n_lag,obs,n_cluster)):\n",
    "                print(n_lag,obs,n_cluster,'done')\n",
    "                continue\n",
    "            else:\n",
    "                print(n_lag,obs,n_cluster)\n",
    "                filepath_file = './data/episode_encoded_lagged_data_with_episode/episode_encoded_'+'lagged_'+str(n_lag)+'_obs_{}'.format(obs)+'_windows_with_episode_cluster_check_{}'.format(n_cluster)\n",
    "                use_standardization = True\n",
    "                all_data = []\n",
    "                columns = ['alpha_stress','alpha_smoking','alpha_activity']\n",
    "                amplitude_duration_data = []\n",
    "                amplitude_duration_columns = ['amplitude_stress','duration_stress'] \n",
    "                data_cluster = pickle.load(open(filepath_file.format(n_lag),'rb'))\n",
    "                temp = data_cluster.groupby(['user','cluster_label']).count().index.values\n",
    "                users = np.array([a[0] for a in temp])\n",
    "                labels = np.array([a[1] for a in temp])\n",
    "                cluster_dict = {}\n",
    "                for i,a in enumerate(users):\n",
    "                    cluster_dict[a] = labels[i]\n",
    "                n_groups = len(np.unique(users))\n",
    "                # n_groups = 20\n",
    "                X_feature,X_static,X_stress_episode,X_quit_episode,X_activity_episode,X_smoking_episode,y_time,y,groups = get_X_y_groups(n_lag)\n",
    "                # y[y<0] = 0\n",
    "                X_feature = np.nan_to_num(X_feature)\n",
    "                y = np.float32(y)\n",
    "                indexes = get_train_test_indexes(groups,n_groups_split = 3,n_val_groups=5)\n",
    "                final_y_time = []\n",
    "                final_probs = []\n",
    "                final_y = []\n",
    "                final_groups = []\n",
    "                final_dist = []\n",
    "                bias_dict = {}\n",
    "                val_results = {}\n",
    "                for kk,yyyy in enumerate(indexes):\n",
    "                    train_index,test_index,val_index = yyyy\n",
    "                    \n",
    "                    X_feature_train,X_feature_test = X_feature[train_index],X_feature[test_index]\n",
    "                    X_static_train,X_static_test = X_static[train_index],X_static[test_index]\n",
    "                    X_stress_episode_train,X_stress_episode_test = X_stress_episode[train_index], X_stress_episode[test_index]\n",
    "                    X_quit_episode_train,X_quit_episode_test = X_quit_episode[train_index], X_quit_episode[test_index]\n",
    "                    X_activity_episode_train,X_activity_episode_test = X_activity_episode[train_index], X_activity_episode[test_index]\n",
    "                    X_smoking_episode_train,X_smoking_episode_test = X_smoking_episode[train_index], X_smoking_episode[test_index]\n",
    "                    y_train,y_test,groups_train,groups_test,time_train,time_test = y[train_index],y[test_index],groups[train_index],groups[test_index],y_time[train_index],y_time[test_index]\n",
    "                    \n",
    "                    X_feature_val,X_static_val,X_stress_episode_val,X_quit_episode_val,\\\n",
    "                    X_activity_episode_val,X_smoking_episode_val,y_val,groups_val,time_val = X_feature[val_index],X_static[val_index],X_stress_episode[val_index],X_quit_episode[val_index],\\\n",
    "                                                                                            X_activity_episode[val_index],X_smoking_episode[val_index],y[val_index],groups[val_index],y_time[val_index]\n",
    "                    \n",
    "                    # y_train = np.array(y_train).reshape(len(y_train),-1)\n",
    "                    \n",
    "                    X_feature_train,val_feature,\\\n",
    "                    X_static_train,val_static,\\\n",
    "                    X_stress_episode_train,val_stress,\\\n",
    "                    X_smoking_episode_train,val_smoking,\\\n",
    "                    X_quit_episode_train,val_quit,\\\n",
    "                    X_activity_episode_train,val_activity, \\\n",
    "                    y_train,val_y = train_test_split(\n",
    "                                                    X_feature_train,\n",
    "                                                    X_static_train,\n",
    "                                                    X_stress_episode_train,\n",
    "                                                    X_smoking_episode_train,\n",
    "                                                    X_quit_episode_train,\n",
    "                                                    X_activity_episode_train,\n",
    "                                                    y_train,\n",
    "                                                    test_size=.05,\n",
    "                                                    stratify=y_train\n",
    "                                                    )\n",
    "                    val_feature = np.concatenate([val_feature,X_feature_val])\n",
    "                    val_static = np.concatenate([val_static,X_static_val])\n",
    "                    val_stress = np.concatenate([val_stress,X_stress_episode_val])\n",
    "                    val_activity = np.concatenate([val_activity,X_activity_episode_val])\n",
    "                    val_smoking = np.concatenate([val_smoking,X_smoking_episode_val])\n",
    "                    val_quit = np.concatenate([val_quit,X_quit_episode_val])\n",
    "                    val_y = np.array(list(val_y)+list(y_val))\n",
    "                    \n",
    "                    positive_train_index = np.where(y_train==1)[0]\n",
    "                    negative_train_index = np.where(y_train==0)[0]\n",
    "                    len_positive = len(positive_train_index)\n",
    "                    n_iters = 10\n",
    "                    test_preds = []\n",
    "                    bias_pred = []\n",
    "                    test_dist = []\n",
    "                    for i,n_iter in enumerate(range(n_iters)):\n",
    "                        # np.random.seed(np.random.randint(109))\n",
    "                        indexes_sampled = np.array(list(positive_train_index)+list(np.random.choice(negative_train_index,len_positive,replace=False)))\n",
    "                        train_feature = X_feature_train[indexes_sampled]\n",
    "                        train_static = X_static_train[indexes_sampled]\n",
    "                        train_stress = X_stress_episode_train[indexes_sampled]\n",
    "                        train_quit = X_quit_episode_train[indexes_sampled]\n",
    "                        train_activity = X_activity_episode_train[indexes_sampled]\n",
    "                        train_smoking = X_smoking_episode_train[indexes_sampled]\n",
    "                        train_y = y_train[indexes_sampled]\n",
    "                        from keras import backend as K\n",
    "                        K.clear_session()\n",
    "                        model = get_model()\n",
    "                        # model.summary()\n",
    "\n",
    "                        n_epochs = 800\n",
    "                        n_step = 20\n",
    "                        validation_results = [0]\n",
    "                        counter_ = 0\n",
    "                        best_filepath = None\n",
    "                        for iiii in np.arange(0,n_epochs,n_step):\n",
    "                            filepath = './models/lag_'+str(n_lag)+'_iter_'+str(n_iter)+'_temp_with_episode_revised_loss_80_3_{}.hdf5'.format(iiii)\n",
    "                            checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min',save_weights_only=False)\n",
    "                            es = EarlyStopping(monitor='loss', mode='min', verbose=1,patience=60)\n",
    "                            callbacks_list = [es,checkpoint]\n",
    "                            model.fit([train_feature,train_static,train_stress,train_activity,train_smoking,train_quit],train_y,\n",
    "                                                    epochs=n_step, batch_size=100,\n",
    "                                                                verbose=0,callbacks=callbacks_list,shuffle=True)\n",
    "                            if os.path.isfile(filepath):\n",
    "                                model.load_weights(filepath)\n",
    "                            val_y_pred = model.predict([val_feature,val_static,val_stress,val_activity,val_smoking,val_quit])[0][:,1]\n",
    "                            val_f1_now = f1Bias_scorer_CV(val_y_pred,val_y)[0]\n",
    "                            # print('<->'*20)\n",
    "                            from sklearn.metrics import roc_auc_score\n",
    "                            print('Validation Result now',val_f1_now,'AUC',roc_auc_score(val_y,val_y_pred),'step',iiii,'stagnant',counter_)\n",
    "                            # print('<->'*20)\n",
    "                            if val_f1_now>=max(validation_results):\n",
    "                                best_filepath = filepath\n",
    "                                counter_ = 0\n",
    "                            else:\n",
    "                                counter_ += n_step\n",
    "                                if counter_>200:\n",
    "                                    break\n",
    "                            validation_results.append(val_f1_now)\n",
    "                        if os.path.isfile(best_filepath):\n",
    "                            print()\n",
    "                            print(best_filepath[-15:])\n",
    "                            model.load_weights(best_filepath)\n",
    "                        model_distance = tf.keras.Model(model.input,model.get_layer('normalize3').output)\n",
    "                        dist = model_distance.predict([X_feature_test,X_static_test,X_stress_episode_test,\n",
    "                                                    X_activity_episode_test,X_smoking_episode_test,X_quit_episode_test])\n",
    "                        test_dist.append(dist.reshape(-1,1,10))\n",
    "                \n",
    "                        test_preds.append(model.predict([X_feature_test,X_static_test,X_stress_episode_test,\n",
    "                                                    X_activity_episode_test,X_smoking_episode_test,X_quit_episode_test])[0][:,1])\n",
    "                        bias_pred.append(model.predict([X_feature_val,X_static_val,X_stress_episode_val,X_activity_episode_val,\n",
    "                                                        X_smoking_episode_val,X_quit_episode_val])[0][:,1])\n",
    "                    test_dist = np.concatenate(test_dist,axis=1)\n",
    "                    from sklearn.preprocessing import MinMaxScaler\n",
    "                    y_test_pred = np.concatenate([a.reshape(-1,1) for a in test_preds],axis=1)\n",
    "                    y_test_check = np.int64(np.array(y_test).reshape(-1))\n",
    "                    import seaborn as sns\n",
    "                    for lll in np.arange(y_test_pred.shape[-1]):\n",
    "                        df_check = pd.DataFrame({'original':y_test_check,'predicted':y_test_pred[:,lll]})\n",
    "                        plt.figure()\n",
    "                        sns.kdeplot(data=df_check,x='predicted',hue='original')\n",
    "                        plt.show()\n",
    "                        # df_check['predicted']\n",
    "                        plt.figure()\n",
    "                        plt.hist(y_test_pred[:,lll])\n",
    "                        plt.show()\n",
    "                        distance_80th = np.percentile(y_test_pred[y_test_check==1,lll],20)\n",
    "                        negative_within_circle = y_test_pred[np.where((y_test_check==0)&(y_test_pred[:,lll]>distance_80th))[0],lll]\n",
    "                        print(negative_within_circle.shape[0]/np.where(y_test_check==0)[0].shape[0],'Negative within positive')\n",
    "                        print('-'*10)\n",
    "                    bias_pred = np.concatenate([a.reshape(-1,1) for a in bias_pred],axis=1)\n",
    "                    # print(roc_auc_score(y_test,y_test_pred))\n",
    "                    final_y_time.extend(list(time_test))\n",
    "                    final_probs.extend(list(y_test_pred))\n",
    "                    final_y.extend(list(y_test))\n",
    "                    final_groups.extend(list(groups_test))\n",
    "                    final_dist.append(test_dist)\n",
    "                    for group_b in np.unique(groups_test):\n",
    "                        bias_dict[group_b] = []\n",
    "                        for kkk in range(bias_pred.shape[1]):\n",
    "                            f1,bias = f1Bias_scorer_CV(bias_pred[:,kkk].reshape(-1),np.int64(y_val.reshape(-1)))\n",
    "                            \n",
    "                            f1_test, bias_test = f1Bias_scorer_CV(y_test_pred[:,kkk],np.int64(y_test.reshape(-1)))\n",
    "                            # plt.hist(y_test_pred[:,kkk],100)\n",
    "                            # plt.show()\n",
    "                            if kkk==0 and group_b==groups_test[0]:\n",
    "                                print(group_b,\n",
    "                                'val results',\n",
    "                                f1,bias,\n",
    "                                'index',kkk,\n",
    "                                'test results',\n",
    "                                f1_test,bias_test,\n",
    "                                'test result with val bias',\n",
    "                                f1_score(y_test,np.array(y_test_pred[:,kkk]>=bias,dtype=np.int64)))\n",
    "                            \n",
    "                            bias_dict[group_b].append(bias)\n",
    "                        val_results[group_b] = [time_val,bias_pred,y_val,groups_val]\n",
    "                    print(len(np.unique(final_groups)))\n",
    "                    # print(bias_dict)\n",
    "                final_y_time,final_probs,final_y,final_groups,final_dist = np.array(final_y_time),np.array(final_probs),np.array(final_y),np.array(final_groups),np.concatenate(final_dist)\n",
    "                pickle.dump([final_y_time,final_probs,final_y,final_groups,bias_dict,val_results,final_dist],open('./data/output_final_all/distance_only_episode_lag_{}_obs_{}_triplet_loss_cluster_{}_revised_loss_alpha_{}_softmax_{}_loss_{}_rare_{}_positive_{}_percentile_{}.p'.format(n_lag,obs,n_cluster,.3,10,50,1,.2,80),'wb'))\n",
    "            # except:\n",
    "            #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18053, 2, 10), (18053, 2), (18053,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dist.shape,final_probs.shape,final_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "for lll in np.arange(y_test_pred.shape[-1]):\n",
    "    df_check = pd.DataFrame({'original':y_test_check,'predicted':y_test_pred[:,lll]})\n",
    "    plt.figure()\n",
    "    # plt.hist(y_test_pred[:,lll])\n",
    "    sns.kdeplot(data=df_check,x='predicted',hue='original')\n",
    "    plt.show()\n",
    "    distance_80th = np.percentile(y_test_pred[y_test_check==1,lll],20)\n",
    "    negative_within_circle = y_test_pred[np.where((y_test_check==0)&(y_test_pred[:,lll]>=distance_80th))[0],lll]\n",
    "    print(distance_80th,negative_within_circle.shape[0]/np.where(y_test_check==0)[0].shape[0],'Negative within positive')\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check['predicted']\n",
    "# .plot(kind='hist')\n",
    "# plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from tensorflow_addons.losses import metric_learning\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "num_classes = 2\n",
    "feature_dim = 20\n",
    "alpha = .2\n",
    "# centers = K.zeros([num_classes, feature_dim])\n",
    "centers = tf.Variable(\n",
    "            tf.zeros([num_classes, feature_dim]),\n",
    "            name=\"centers\",\n",
    "            trainable=False,\n",
    "            # in a distributed strategy, we want updates to this variable to be summed.\n",
    "            aggregation=tf.VariableAggregation.SUM,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "features = tf.cast(tf.convert_to_tensor(np.random.randn(10,feature_dim)),tf.float32)\n",
    "labels = tf.convert_to_tensor(np.array([0]*5+[1]*5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_for_equal = tf.math.equal(tf.cast(labels,tf.bool),tf.transpose(tf.cast(labels,tf.bool)))\n",
    "        only_positive = tf.repeat(tf.transpose(tf.reshape(tf.cast(labels,tf.bool),[lshape[0],1])),lshape[0],axis=0)\n",
    "        mask_only_positive = tf.math.multiply(tf.cast(mask_for_equal,tf.float32),tf.cast(only_positive,tf.float32))\n",
    "        \n",
    "        only_negative = tf.cast(tf.repeat(tf.transpose(tf.reshape(tf.cast(1-labels,tf.bool),[10,1])),10,axis=0),tf.int32)\n",
    "\n",
    "        mask_negative_to_positive = tf.math.logical_not(tf.cast(tf.math.add(tf.cast(mask_for_equal,tf.int32),only_negative),tf.bool))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow_addons.losses import metric_learning\n",
    "import numpy as np\n",
    "feature_dim = 20\n",
    "\n",
    "precise_embeddings = tf.cast(tf.convert_to_tensor(np.random.randn(10,feature_dim)),tf.float32)\n",
    "labels = tf.convert_to_tensor(np.array([0]*3+[1]*6+[0]))\n",
    "\n",
    "lshape = tf.shape(labels)\n",
    "labels = tf.reshape(labels, [lshape[0], 1])\n",
    "mask_for_equal = tf.math.equal(tf.cast(labels,tf.bool),tf.transpose(tf.cast(labels,tf.bool)))\n",
    "only_positive = tf.repeat(tf.transpose(tf.reshape(tf.cast(labels,tf.bool),[lshape[0],1])),lshape[0],axis=0)\n",
    "mask_only_positive = tf.math.multiply(tf.cast(mask_for_equal,tf.float32),tf.cast(only_positive,tf.float32))\n",
    "\n",
    "only_negative = tf.cast(tf.repeat(tf.transpose(tf.reshape(tf.cast(1-labels,tf.bool),[10,1])),10,axis=0),tf.int32)\n",
    "\n",
    "mask_negative_to_positive = tf.math.logical_not(tf.cast(tf.math.add(tf.cast(mask_for_equal,tf.int32),only_negative),tf.bool))\n",
    "pdist_matrix = metric_learning.pairwise_distance(\n",
    "            precise_embeddings, squared=False\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mask_negative_to_positive = tf.math.logical_and(only_positive,tf.math.logical_not(tf.cast(mask_only_positive,tf.bool)))\n",
    "\n",
    "pdist_matrix = metric_learning.pairwise_distance(\n",
    "            precise_embeddings, squared=False\n",
    "        )\n",
    "# positive_only_dist = tf.math.multiply(pdist_matrix,mask_only_positive)\n",
    "# consistency = tf.reduce_sum(positive_only_dist)/tf.reduce_sum(mask_only_positive)\n",
    "\n",
    "positive_only_dist = tf.boolean_mask(pdist_matrix, tf.cast(mask_only_positive,tf.bool))\n",
    "\n",
    "# distance_95 = tf.reshape(positive_only_dist,(-1,))\n",
    "# samples1 = tf.cast(tf.shape(labels)[0], tf.float32) #batch size\n",
    "# p = 80\n",
    "# p =  2*(100. - p)/100.  #100% will return 0 elements, 0% will return all elements\n",
    "# samples = tf.cast(tf.math.floor(p * samples1), tf.int32)\n",
    "# values, indices = tf.math.top_k(distance_95, samples)\n",
    "# positive_dist_95th = tf.reduce_min(values)\n",
    "\n",
    "# sparse_labels = tf.reshape(labels, (-1,))\n",
    "# centers_batch = tf.gather(centers, tf.cast(sparse_labels,tf.int32))\n",
    "# diff = (1 - .9) * (centers_batch - precise_embeddings)\n",
    "# updates = tf.scatter_nd(tf.cast(sparse_labels[:, None],tf.int32), diff, centers.shape)\n",
    "# centers.assign_add(updates)\n",
    "# centers_batch = tf.gather(centers, tf.ones(sparse_labels.shape,dtype=tf.int32))\n",
    "# distance_to_positive_center = tf.norm(precise_embeddings-centers_batch,2,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_only_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.cast(mask_for_equal,tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_to_positive_center_smaller_than_95 = tf.where(distance_to_positive_center<positive_dist_95th,1,0)\n",
    "negative_only = tf.math.logical_not(tf.cast(sparse_labels,tf.bool))\n",
    "negative_within_circle = tf.reduce_sum(tf.boolean_mask(distance_to_positive_center_smaller_than_95,negative_only))\n",
    "total_negative = tf.shape(sparse_labels)[0] - tf.reduce_sum(sparse_labels)\n",
    "rare_loss = negative_within_circle/(total_negative+K.epsilon())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.norm(distance_to_positive_center,2,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_labels = tf.reshape(labels, (-1,))\n",
    "centers_batch = tf.gather(centers, tf.cast(sparse_labels,tf.int32))\n",
    "diff = (1 - .9) * (centers_batch - precise_embeddings)\n",
    "updates = tf.scatter_nd(tf.cast(sparse_labels[:, None],tf.int32), diff, centers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers.assign_add(updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.math.multiply(pdist_matrix,mask_only_positive)\n",
    "# tf.reduce_sum(mask_only_positive)\n",
    "consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist_matrix = metric_learning.pairwise_distance(\n",
    "            precise_embeddings, squared=False\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency = tf.math.equal(labels, tf.transpose(labels))\n",
    "adjacency_not = tf.math.logical_not(adjacency)\n",
    "batch_size = tf.size(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.math.multiply(adjacency,tf.cast(labels,tf.bool))\n",
    "mask_for_equal = tf.math.equal(tf.cast(labels,tf.bool),tf.transpose(tf.cast(labels,tf.bool)))\n",
    "only_positive = tf.repeat(tf.transpose(tf.reshape(tf.cast(labels,tf.bool),[10,1])),10,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.math.multiply(tf.reshape(tf.cast(labels,tf.bool),[10,1]),mask_for_positive)\n",
    "tf.math.multiply(tf.cast(mask_for_equal,tf.int32),tf.cast(only_positive,tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist_matrix_tile = tf.tile(pdist_matrix, [batch_size, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist_matrix_tile = tf.tile(pdist_matrix, [batch_size, 1])\n",
    "mask = tf.math.logical_and(\n",
    "    tf.tile(adjacency_not, [batch_size, 1]),\n",
    "    tf.math.greater(\n",
    "        pdist_matrix_tile, tf.reshape(tf.transpose(pdist_matrix), [-1, 1])\n",
    "    ),\n",
    ")\n",
    "mask_final = tf.reshape(\n",
    "    tf.math.greater(\n",
    "        tf.math.reduce_sum(\n",
    "            tf.cast(mask, dtype=tf.dtypes.float32), 1, keepdims=True\n",
    "        ),\n",
    "        0.0,\n",
    "    ),\n",
    "    [batch_size, batch_size],\n",
    ")\n",
    "mask_final = tf.transpose(mask_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51b244ab9aca612e739a0539ae1af887c58db9e180d786deb0ab1761def69c1f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('test1': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
