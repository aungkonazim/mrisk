{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azim/miniconda3/envs/test1/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:38: UserWarning: You are currently using a nightly version of TensorFlow (2.5.0-dev20210310). \n",
      "TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not. \n",
      "If you encounter a bug, do not file an issue on GitHub.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Physical GPUs, 1 Logical GPU\n",
      "15 30 4\n",
      "[0 1]\n",
      "15 68\n",
      "(None, 1, 1) coeff (None, 10, 30) amplitude\n",
      "Validation Result now 0.1746987951807229 AUC 0.5588035018445512 step 0 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.1918047079337402 AUC 0.5950945527093208 step 20 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.24042553191489363 AUC 0.6201875068768438 step 40 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2562499999999999 AUC 0.6485019379724262 step 60 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.27245508982035926 AUC 0.6087049943013023 step 80 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.29391891891891886 AUC 0.6674254550135046 step 100 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.28102189781021897 AUC 0.6773872768529072 step 120 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.28980322003577813 AUC 0.6794887365084111 step 140 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3073770491803278 AUC 0.6990848867215153 step 160 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.27814569536423844 AUC 0.6803689725113092 step 180 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.31411530815109345 AUC 0.6685215979605098 step 200 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3367556468172484 AUC 0.6726669074859505 step 220 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.28680688336520077 AUC 0.6602849556456551 step 240 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3224489795918367 AUC 0.6546215504194615 step 260 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.33047210300429186 AUC 0.6809839958977681 step 280 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.32905982905982906 AUC 0.6820002117548875 step 300 stagnant 60\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.30417227456258417 AUC 0.7500544957431039 step 320 stagnant 80\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2998027613412229 AUC 0.7136627575053613 step 340 stagnant 100\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "ew_loss220.hdf5\n",
      "15 68\n",
      "(None, 1, 1) coeff (None, 10, 30) amplitude\n",
      "Validation Result now 0.16661690056733355 AUC 0.5246154676565169 step 0 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.15824765310683953 AUC 0.48581709360188835 step 20 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.21372549019607842 AUC 0.6122866621409249 step 40 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2801664355062414 AUC 0.6641536343989587 step 60 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.29780564263322884 AUC 0.6408995223058862 step 80 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.319327731092437 AUC 0.6401121885698032 step 100 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.28153564899451555 AUC 0.6511986987454561 step 120 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.27526132404181186 AUC 0.6208850523885744 step 140 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2975517890772128 AUC 0.6173018275277201 step 160 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2664359861591695 AUC 0.637839975585907 step 180 stagnant 60\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2621184919210054 AUC 0.6137554521693457 step 200 stagnant 80\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.26229508196721313 AUC 0.6366218659757644 step 220 stagnant 100\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "ew_loss100.hdf5\n",
      "15 68\n",
      "(None, 1, 1) coeff (None, 10, 30) amplitude\n",
      "Validation Result now 0.16325416325416325 AUC 0.4995588439843966 step 0 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.20417287630402386 AUC 0.5936283577162859 step 20 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.24144869215291756 AUC 0.6308551783412119 step 40 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2957074721780604 AUC 0.6400504267276189 step 60 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3392226148409894 AUC 0.6557571379043324 step 80 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.35161290322580646 AUC 0.6414507078218519 step 100 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3471337579617835 AUC 0.6495461802117133 step 120 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.34295415959252973 AUC 0.6386797290367852 step 140 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3898305084745763 AUC 0.6558126716615906 step 160 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.36882129277566544 AUC 0.6516289556124387 step 180 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.37802907915993533 AUC 0.666270664266778 step 200 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3933333333333333 AUC 0.6598536607645182 step 220 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.36493738819320215 AUC 0.6740744546792641 step 240 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3446969696969697 AUC 0.6672775379965081 step 260 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.4091710758377425 AUC 0.6661326083842479 step 280 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.38513513513513514 AUC 0.6689487407850294 step 300 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3483483483483484 AUC 0.6608299130766947 step 320 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3610108303249098 AUC 0.6573961622540685 step 340 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.33840947546531297 AUC 0.6386382084706107 step 360 stagnant 60\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3765182186234817 AUC 0.6746598946623236 step 380 stagnant 80\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3767535070140281 AUC 0.6894017716825586 step 400 stagnant 100\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "ew_loss280.hdf5\n",
      "15 68\n",
      "(None, 1, 1) coeff (None, 10, 30) amplitude\n",
      "Validation Result now 0.1739130434782609 AUC 0.5504573490364115 step 0 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2093784078516903 AUC 0.6216293085372512 step 20 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2977867203219316 AUC 0.6520934669465153 step 40 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2876712328767123 AUC 0.6454589994789168 step 60 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2851782363977486 AUC 0.6652814367776719 step 80 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2784810126582279 AUC 0.6286654874825873 step 100 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2792452830188679 AUC 0.6001372254712065 step 120 stagnant 60\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.27586206896551724 AUC 0.6026185983072065 step 140 stagnant 80\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.25149700598802394 AUC 0.594894215977529 step 160 stagnant 100\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "new_loss40.hdf5\n",
      "15 68\n",
      "(None, 1, 1) coeff (None, 10, 30) amplitude\n",
      "Validation Result now 0.15674967234600262 AUC 0.4760172019705661 step 0 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.15658549358470805 AUC 0.48758535071384235 step 20 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.15654450261780103 AUC 0.49441340782122906 step 40 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.15654450261780103 AUC 0.4973618870266915 step 60 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.15654450261780103 AUC 0.499844816883923 step 80 stagnant 60\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.15654450261780103 AUC 0.49604309004357583 step 100 stagnant 80\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.15654450261780103 AUC 0.5041245492423535 step 120 stagnant 100\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "_new_loss0.hdf5\n",
      "15 68\n",
      "(None, 1, 1) coeff (None, 10, 30) amplitude\n",
      "Validation Result now 0.2533589251439539 AUC 0.6597083387829077 step 0 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2677685950413223 AUC 0.669307374675361 step 20 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2813688212927757 AUC 0.6217528322216201 step 40 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2998102466793169 AUC 0.6265557237138486 step 60 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.33333333333333337 AUC 0.6136454226689836 step 80 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.310204081632653 AUC 0.6279471816877695 step 100 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2982107355864811 AUC 0.6366145998766839 step 120 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.33904761904761904 AUC 0.6731142915864801 step 140 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2824427480916031 AUC 0.63799775373737 step 160 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2906976744186046 AUC 0.6436165243549261 step 180 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.27372262773722633 AUC 0.6853592255583997 step 200 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2935420743639922 AUC 0.6718686746012468 step 220 stagnant 60\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.29711751662971175 AUC 0.6772165235245148 step 240 stagnant 80\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.29940119760479045 AUC 0.6525839286344508 step 260 stagnant 100\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "ew_loss140.hdf5\n",
      "15 68\n",
      "(None, 1, 1) coeff (None, 10, 30) amplitude\n",
      "Validation Result now 0.1569141957491472 AUC 0.4739775041572467 step 0 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.15809579782544814 AUC 0.48202730392431636 step 20 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.15985894798707023 AUC 0.49855041323343485 step 40 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.16131881071533707 AUC 0.49708006618378253 step 60 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.21494102228047182 AUC 0.5777737295225758 step 80 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.24561403508771928 AUC 0.5788107056627825 step 100 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.28145695364238416 AUC 0.6436865903103454 step 120 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2820037105751391 AUC 0.6032154564459641 step 140 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3146997929606625 AUC 0.5999477878880357 step 160 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2808988764044944 AUC 0.6003126498632936 step 180 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3223140495867769 AUC 0.6085897747301682 step 200 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3208791208791209 AUC 0.6071583532113043 step 220 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.34893617021276596 AUC 0.6367536937733683 step 240 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3524416135881104 AUC 0.6352137997753737 step 260 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.32997987927565386 AUC 0.6158330374992993 step 280 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3225806451612903 AUC 0.6380346032398498 step 300 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3168724279835391 AUC 0.6437462761242212 step 320 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.32083333333333336 AUC 0.6264322000294797 step 340 stagnant 60\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.32499999999999996 AUC 0.6288035433651173 step 360 stagnant 80\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.32758620689655177 AUC 0.6523166399897029 step 380 stagnant 100\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "ew_loss260.hdf5\n",
      "15 68\n",
      "(None, 1, 1) coeff (None, 10, 30) amplitude\n",
      "Validation Result now 0.15812431842966196 AUC 0.49717867752844674 step 0 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.15790863480327436 AUC 0.4834779287050358 step 20 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.16353017521090205 AUC 0.5047930303577619 step 40 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.15778364116094987 AUC 0.4720820903113835 step 60 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.15765884524123386 AUC 0.45347101553076774 step 80 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.16008595218909483 AUC 0.48908424315273963 step 100 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.18141916605705924 AUC 0.5803666888801696 step 120 stagnant 60\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.19023136246786632 AUC 0.5850964003745156 step 140 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.21489361702127657 AUC 0.6465701936311603 step 160 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.26606875934230195 AUC 0.6482362063489098 step 180 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2623456790123457 AUC 0.6749256262858401 step 200 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2843894899536321 AUC 0.6412976007340836 step 220 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.29233511586452765 AUC 0.6363400451328554 step 240 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2782608695652174 AUC 0.6471426584372906 step 260 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.27402135231316727 AUC 0.6319284849768212 step 280 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2994652406417112 AUC 0.6459837156339464 step 300 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.26948051948051943 AUC 0.6255924465786016 step 320 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.27152317880794696 AUC 0.6267996570401234 step 340 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3024193548387097 AUC 0.646045996483208 step 360 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.29432624113475175 AUC 0.6502442447305211 step 380 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.30648330058939094 AUC 0.6366997170373415 step 400 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2851919561243144 AUC 0.6252462688581222 step 420 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.30831643002028397 AUC 0.6429906018198464 step 440 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.30451127819548873 AUC 0.656514888237016 step 460 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2807625649913345 AUC 0.6577682903284069 step 480 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3312883435582822 AUC 0.6604230115281853 step 500 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3175257731958763 AUC 0.6548218871512532 step 520 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.32098765432098764 AUC 0.6428120633852963 step 540 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2982791586998088 AUC 0.632609422262082 step 560 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.27906976744186046 AUC 0.6573613887798975 step 580 stagnant 60\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.27814569536423844 AUC 0.6590590609293548 step 600 stagnant 80\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.27065026362038663 AUC 0.6371247838335524 step 620 stagnant 100\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "ew_loss500.hdf5\n",
      "15 68\n",
      "(None, 1, 1) coeff (None, 10, 30) amplitude\n",
      "Validation Result now 0.1651554404145078 AUC 0.5217899931283463 step 0 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2378716744913928 AUC 0.6450318566543974 step 20 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3328380386329866 AUC 0.6587113261876438 step 40 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3656821378340366 AUC 0.6833600102971004 step 60 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3271461716937355 AUC 0.6945025732370885 step 80 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3445692883895131 AUC 0.6558479641428391 step 100 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.35571687840290384 AUC 0.6562024459765534 step 120 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.35991820040899797 AUC 0.6433450836535607 step 140 stagnant 60\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.4233009708737864 AUC 0.6720549981419547 step 160 stagnant 80\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.4107485604606526 AUC 0.6714487978758078 step 180 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.40152963671128106 AUC 0.6612196873916573 step 200 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.4054054054054054 AUC 0.6605927268424232 step 220 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3770197486535009 AUC 0.6670060972951427 step 240 stagnant 60\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.33791748526522597 AUC 0.6579141313170946 step 260 stagnant 80\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3320610687022901 AUC 0.6463169181774963 step 280 stagnant 100\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "ew_loss160.hdf5\n",
      "15 68\n",
      "(None, 1, 1) coeff (None, 10, 30) amplitude\n",
      "Validation Result now 0.1574098446959726 AUC 0.47242048292570515 step 0 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.16146895948703005 AUC 0.47413839635117255 step 20 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.24290780141843968 AUC 0.6414211244184526 step 40 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.21225710014947685 AUC 0.6128528988621289 step 60 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2543720190779014 AUC 0.5946450925804825 step 80 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2495894909688013 AUC 0.6306351193404873 step 100 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.26306306306306304 AUC 0.6691044429081834 step 120 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.24868651488616464 AUC 0.6370183873827304 step 140 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.24462809917355374 AUC 0.6530541490463764 step 160 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.28625235404896426 AUC 0.6367739350493784 step 180 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3132530120481928 AUC 0.6239015215211474 step 200 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2883895131086142 AUC 0.6247573641914181 step 220 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.30738522954091824 AUC 0.6198065556821932 step 240 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.304 AUC 0.5924361984600022 step 260 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3089430894308943 AUC 0.6031728978656352 step 280 stagnant 60\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.26186579378068736 AUC 0.6385453062037955 step 300 stagnant 80\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.27598566308243727 AUC 0.6455103811795577 step 320 stagnant 100\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "ew_loss200.hdf5\n",
      "15 68\n",
      "(None, 1, 1) coeff (None, 10, 30) amplitude\n",
      "Validation Result now 0.15790863480327436 AUC 0.5003425446709391 step 0 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2015503875968992 AUC 0.585123388742529 step 20 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2564102564102564 AUC 0.6585353827884798 step 40 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2620232172470978 AUC 0.6576032460778636 step 60 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.32195121951219513 AUC 0.6688371542634356 step 80 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.30793650793650795 AUC 0.621967701151573 step 100 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.30303030303030304 AUC 0.6794103664397567 step 120 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.303886925795053 AUC 0.6777978114509571 step 140 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.31737346101231195 AUC 0.6799298925240145 step 160 stagnant 60\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.29045643153526973 AUC 0.6773971379873736 step 180 stagnant 80\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.33590733590733585 AUC 0.6671732175739948 step 200 stagnant 100\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3327841845140033 AUC 0.6797731523867059 step 220 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.34353741496598644 AUC 0.6901257865552255 step 240 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3237774030354132 AUC 0.6813514529084119 step 260 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2961608775137112 AUC 0.6609010170462684 step 280 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3021806853582555 AUC 0.6944200511118169 step 300 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2857142857142857 AUC 0.7018615745844311 step 320 stagnant 60\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2786647314949202 AUC 0.6932538422093923 step 340 stagnant 80\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2823871906841339 AUC 0.6898200913867661 step 360 stagnant 100\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "ew_loss240.hdf5\n",
      "15 68\n",
      "(None, 1, 1) coeff (None, 10, 30) amplitude\n",
      "Validation Result now 0.16510067114093957 AUC 0.5198945792824831 step 0 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.16359129716331586 AUC 0.49063659332058657 step 20 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.19348268839103872 AUC 0.5530430422949247 step 40 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.30183727034120733 AUC 0.6546651470139446 step 60 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.26930693069306927 AUC 0.6684738493094091 step 80 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3153846153846154 AUC 0.6603425654312223 step 100 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.27800829875518673 AUC 0.6167641361957612 step 120 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2962962962962963 AUC 0.6517280859641802 step 140 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3197158081705151 AUC 0.6620936953096292 step 160 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.30679611650485433 AUC 0.671886320841871 step 180 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.31861804222648754 AUC 0.6700293135197192 step 200 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3380281690140845 AUC 0.6716865031171565 step 220 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.31800766283524906 AUC 0.6375815100614712 step 240 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.31388329979879276 AUC 0.6404905447290679 step 260 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.30566037735849055 AUC 0.655494001316202 step 280 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3170731707317073 AUC 0.6558313559163693 step 300 stagnant 60\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.30877192982456136 AUC 0.6541103284484387 step 320 stagnant 80\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.3060869565217392 AUC 0.6556263481208829 step 340 stagnant 100\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "ew_loss220.hdf5\n",
      "15 68\n",
      "(None, 1, 1) coeff (None, 10, 30) amplitude\n",
      "Validation Result now 0.16058002148227715 AUC 0.47725088179302416 step 0 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.15571205007824726 AUC 0.513050951962781 step 20 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.17331812998859752 AUC 0.5435431367542128 step 40 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2194787379972565 AUC 0.5632612536304545 step 60 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.24999999999999994 AUC 0.5906056604987866 step 80 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.25328947368421056 AUC 0.6061623786301951 step 100 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.25 AUC 0.6089354334435704 step 120 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2913907284768212 AUC 0.6152564206365518 step 140 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.30707070707070705 AUC 0.6008622783580277 step 160 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.30990990990990985 AUC 0.6459172827280673 step 180 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.31401869158878504 AUC 0.647937777279531 step 200 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.28884826325411334 AUC 0.6148920776683711 step 220 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2915129151291513 AUC 0.6239139776909998 step 240 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.31696428571428575 AUC 0.6567214530537339 step 260 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.33781190019193863 AUC 0.6400971373645652 step 280 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.30447761194029854 AUC 0.6413219940667111 step 300 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.31893687707641194 AUC 0.6709157776075434 step 320 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.25577812018489987 AUC 0.6376681842433602 step 340 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.23875968992248062 AUC 0.6140564762741105 step 360 stagnant 60\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.27624309392265195 AUC 0.5967455142218319 step 380 stagnant 80\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.30201342281879195 AUC 0.6297465792243543 step 400 stagnant 100\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "ew_loss280.hdf5\n",
      "15 68\n",
      "(None, 1, 1) coeff (None, 10, 30) amplitude\n",
      "Validation Result now 0.16044260027662516 AUC 0.4834960939527371 step 0 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.16411464468001571 AUC 0.5152344147364794 step 20 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.15707906488048332 AUC 0.43610607674246243 step 40 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.15712033631108774 AUC 0.4224375063578367 step 60 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.1753155680224404 AUC 0.5481799459817434 step 80 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.17244611059044046 AUC 0.5532117195950084 step 100 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.16280170373876005 AUC 0.5058782741561464 step 120 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.16184573002754823 AUC 0.47827955381999593 step 140 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.18265541059094398 AUC 0.5775510754864653 step 160 stagnant 60\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.1716247139588101 AUC 0.5606599901596259 step 180 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.15707906488048332 AUC 0.44297409739479204 step 200 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.18223760092272198 AUC 0.5667744125358893 step 220 stagnant 40\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.15753424657534246 AUC 0.451271982544754 step 240 stagnant 60\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.1615828524319868 AUC 0.4458717139066908 step 260 stagnant 80\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2150274893097129 AUC 0.6325414323349713 step 280 stagnant 100\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2425149700598802 AUC 0.6701149496874538 step 300 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.21108179419525067 AUC 0.6631804961292452 step 320 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2613065326633166 AUC 0.6169691439912475 step 340 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.2573913043478261 AUC 0.5780410181673237 step 360 stagnant 0\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n",
      "Validation Result now 0.26666666666666666 AUC 0.6388639765491843 step 380 stagnant 20\n",
      "<-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><-><->\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime,timedelta\n",
    "from scipy.stats import iqr,skew,kurtosis,mode\n",
    "from joblib import Parallel,delayed\n",
    "import zipfile\n",
    "import shutil\n",
    "from sklearn.feature_selection import SelectKBest,f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score,r2_score,classification_report\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,RandomForestRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV,StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "seed = 100\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from sklearn.model_selection import train_test_split,LeavePGroupsOut\n",
    "from keras.backend import expand_dims, repeat_elements\n",
    "from keras.layers import Lambda\n",
    "from tensorflow.keras.layers import Conv1D,BatchNormalization,Dropout,InputLayer,MaxPooling1D,Flatten,RepeatVector,Dense,Input,Activation,GRU,Bidirectional,LSTM\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow_addons as tfa\n",
    "import warnings\n",
    "import functools\n",
    "from tensorflow_addons.losses import metric_learning\n",
    "import tensorflow as tf\n",
    "\n",
    "# tf.config.run_functions_eagerly(False)\n",
    "warnings.filterwarnings('ignore')\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[3], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def get_X_y_groups(n_lag=10):\n",
    "    data = pickle.load(open(filepath_file.format(n_lag),'rb'))\n",
    "\n",
    "    X_feature = np.concatenate(data.feature_final.values)\n",
    "    X_static =  np.concatenate(data.static_features.values)\n",
    "\n",
    "    X_stress_episode = np.concatenate(data.stress_episode.values)\n",
    "    X_quit_episode = np.concatenate(data.quit_episode.values)\n",
    "    X_activity_episode = np.concatenate(data.activity_episode.values)\n",
    "    X_smoking_episode = np.concatenate(data.smoking_episode.values)\n",
    "\n",
    "    y_time = data['time'].values\n",
    "    y = data['label'].values\n",
    "    groups = data['user'].values\n",
    "    \n",
    "    return X_feature,X_static,X_stress_episode,X_quit_episode,X_activity_episode,X_smoking_episode,y_time,y,groups\n",
    "\n",
    "\n",
    "def get_train_test_indexes(groups,n_groups_split = 10,n_val_groups = 10):\n",
    "    groups_unique = np.unique(groups)\n",
    "    groups_split = np.array_split(groups_unique,n_groups_split)\n",
    "    indexes = []\n",
    "    for this_groups in groups_split:\n",
    "        train_groups = np.array([a for a in groups_unique if a not in this_groups])\n",
    "        val_groups = np.random.choice(train_groups,n_val_groups)\n",
    "        train_groups = np.array([a for a in groups_unique if a not in list(this_groups)+list(val_groups)])\n",
    "        test_groups = this_groups\n",
    "        train_index,test_index = np.array([i for i,a in enumerate(groups) \n",
    "                                           if a in train_groups]),np.array([i for i,a in enumerate(groups) \n",
    "                                                                               if a in test_groups])\n",
    "        val_index = np.array([i for i,a in enumerate(groups) \n",
    "                                           if a in val_groups])\n",
    "        indexes.append([train_index,test_index,val_index])\n",
    "    return indexes\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "def f1Bias_scorer_CV(probs, y, ret_bias=True):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "\n",
    "    f1,bias = 0.0,.5\n",
    "    min_recall = .8\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0) and thresholds[i]>0.2:\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "\n",
    "class CenterLossLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_classes, n_features, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.n_features = n_features\n",
    "        self.centers = tf.Variable(\n",
    "            tf.zeros([n_classes, n_features]),\n",
    "            name=\"centers\",\n",
    "            trainable=False,\n",
    "            # in a distributed strategy, we want updates to this variable to be summed.\n",
    "            aggregation=tf.VariableAggregation.MEAN,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        # pass through layer\n",
    "        return tf.identity(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"n_classes\": self.n_classes, \"n_features\": self.n_features})\n",
    "        return config\n",
    "\n",
    "\n",
    "class CenterLoss(tf.keras.losses.Loss):\n",
    "    def __init__(\n",
    "        self,\n",
    "        centers_layer,\n",
    "        alpha=0.9,\n",
    "        update_centers=True,\n",
    "        p = 50,\n",
    "        name=\"center_loss\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.centers_layer = centers_layer\n",
    "        self.centers = self.centers_layer.centers\n",
    "        self.alpha = alpha\n",
    "        self.update_centers = update_centers\n",
    "        self.p = p\n",
    "        \n",
    "    def consistency_loss(self,labels,precise_embeddings):\n",
    "        \n",
    "        lshape = tf.shape(labels)\n",
    "        labels = tf.reshape(labels, [lshape[0], 1])\n",
    "        mask_for_equal = tf.math.equal(tf.cast(labels,tf.bool),tf.transpose(tf.cast(labels,tf.bool)))\n",
    "        only_positive = tf.repeat(tf.transpose(tf.reshape(tf.cast(labels,tf.bool),[lshape[0],1])),lshape[0],axis=0)\n",
    "        mask_only_positive = tf.math.multiply(tf.cast(mask_for_equal,tf.float32),tf.cast(only_positive,tf.float32))\n",
    "        \n",
    "        pdist_matrix = metric_learning.pairwise_distance(\n",
    "                    precise_embeddings, squared=False\n",
    "                )\n",
    "        positive_only_dist = tf.boolean_mask(pdist_matrix, tf.cast(mask_only_positive,tf.bool))\n",
    "        no_of_positives = tf.cast(tf.reduce_sum(labels),tf.int32)\n",
    "        positive_only_dist = tf.reshape(positive_only_dist,[no_of_positives,no_of_positives])\n",
    "        positive_only_dist = tf.reduce_mean(positive_only_dist,axis=1)\n",
    "        \n",
    "        distance_95 = tf.reshape(positive_only_dist,(-1,))\n",
    "        samples1 = tf.cast(no_of_positives, tf.float32) #batch size\n",
    "        p = (100. - self.p)/100.  #100% will return 0 elements, 0% will return all elements\n",
    "        samples = tf.cast(tf.math.floor(p * samples1), tf.int32)\n",
    "        values, indices = tf.math.top_k(distance_95, samples)\n",
    "        positive_dist_95th = tf.reduce_min(values)\n",
    "        return positive_dist_95th, pdist_matrix\n",
    "    \n",
    "    \n",
    "    def compute_rare_loss_v1(self,labels,pdist_matrix,positive_dist_95th):\n",
    "        lshape = tf.shape(labels)\n",
    "        labels = tf.reshape(labels, [lshape[0], 1])\n",
    "        mask_for_equal = tf.math.equal(tf.cast(labels,tf.bool),tf.transpose(tf.cast(labels,tf.bool)))\n",
    "        only_negative = tf.cast(tf.repeat(tf.transpose(tf.reshape(tf.cast(1-labels,tf.bool),[lshape[0],1])),lshape[0],axis=0),tf.int32)\n",
    "        mask_negative_to_positive = tf.math.logical_not(tf.cast(tf.math.add(tf.cast(mask_for_equal,tf.int32),only_negative),tf.bool))\n",
    "        no_of_positives = tf.cast(tf.reduce_sum(labels),tf.int32)\n",
    "        no_of_negatives = lshape[0] - no_of_positives\n",
    "        negative_to_positive_distance = tf.reshape(tf.boolean_mask(pdist_matrix,mask_negative_to_positive),[no_of_negatives,no_of_positives])\n",
    "        average_neg_to_pos_distance = tf.reduce_mean(negative_to_positive_distance,axis=1)\n",
    "        less_distance_mask = tf.reduce_sum(tf.where(average_neg_to_pos_distance<=positive_dist_95th,1,0))\n",
    "        return tf.cast(less_distance_mask,tf.float32)/(tf.cast(no_of_negatives,tf.float32)+K.epsilon())\n",
    "        \n",
    "    def compute_rare_loss(self,labels,precise_embeddings,positive_dist_95th):\n",
    "        sparse_labels = tf.reshape(labels, (-1,))\n",
    "        centers_batch = tf.gather(self.centers, tf.ones([tf.shape(sparse_labels)[0],],dtype=tf.int32))\n",
    "        distance_to_positive_center = tf.norm(precise_embeddings-centers_batch,2,axis=1)\n",
    "        distance_to_positive_center_smaller_than_95 = tf.where(distance_to_positive_center<=positive_dist_95th,1,0)\n",
    "        negative_only = tf.math.logical_not(tf.cast(sparse_labels,tf.bool))\n",
    "        negative_within_circle = tf.reduce_sum(tf.boolean_mask(distance_to_positive_center_smaller_than_95,negative_only))\n",
    "        total_negative = tf.cast(tf.shape(sparse_labels)[0],tf.float32) - tf.reduce_sum(sparse_labels)\n",
    "        rare_loss = tf.cast(negative_within_circle,tf.float32)/(total_negative+K.epsilon())\n",
    "        return rare_loss\n",
    "    \n",
    "    def center_loss_positive_compute(self,labels,precise_embeddings,centers_batch):\n",
    "        loss_row = tf.reduce_mean(tf.math.square(precise_embeddings - centers_batch),axis=1)\n",
    "        loss_row_only_positive = tf.math.multiply(tf.cast(labels,tf.float32),loss_row)\n",
    "        center_loss_positive = tf.reduce_sum(loss_row_only_positive)/(tf.reduce_sum(labels)+K.epsilon())\n",
    "        return center_loss_positive\n",
    "    \n",
    "    def compute_positive_to_negative_loss(self,labels,precise_embeddings):\n",
    "        sparse_labels = tf.reshape(labels, (-1,))\n",
    "        centers_batch = tf.gather(self.centers, tf.ones([tf.shape(sparse_labels)[0],],dtype=tf.int32))\n",
    "        distance_to_positive_center = tf.norm(precise_embeddings-centers_batch,2,axis=1)\n",
    "        negative_only = tf.math.logical_not(tf.cast(sparse_labels,tf.bool))\n",
    "        return tf.reduce_min(tf.boolean_mask(distance_to_positive_center,negative_only))\n",
    "    \n",
    "    \n",
    "    def call(self, sparse_labels, prelogits):\n",
    "        sparse_labels = tf.reshape(sparse_labels, (-1,))\n",
    "        # the reduction of batch dimension will be done by the parent class\n",
    "        # center_loss = tf.keras.losses.mean_squared_error(prelogits, centers_batch)\n",
    "        # centers_batch = tf.gather(self.centers, tf.cast(sparse_labels,tf.int32))\n",
    "        # if self.update_centers and tf.cast(tf.reduce_sum(sparse_labels),tf.int32)<tf.shape(sparse_labels)[0]:\n",
    "        #     diff = (1 - self.alpha) * (centers_batch - prelogits)\n",
    "        #     updates = tf.scatter_nd(tf.cast(sparse_labels[:, None],tf.int32), diff, self.centers.shape)\n",
    "        #     self.centers.assign_sub(updates)\n",
    "        distance_95th, pdist_matrix = self.consistency_loss(sparse_labels,prelogits)\n",
    "        rare_loss = self.compute_rare_loss_v1(sparse_labels,pdist_matrix,distance_95th)\n",
    "        # center_loss_positive = self.center_loss_positive_compute(sparse_labels,prelogits,centers_batch)        \n",
    "        # rare_loss = self.compute_rare_loss(sparse_labels,prelogits,positive_dist_95th=distance_95th)\n",
    "        # postive_to_negative_distance = self.compute_positive_to_negative_loss(sparse_labels,prelogits)\n",
    "        return tf.square(rare_loss-.2) + distance_95th*.2\n",
    "        # return rare_loss\n",
    "    \n",
    "def get_model():\n",
    "    n_t,n_f = train_feature.shape[1],train_feature.shape[2]\n",
    "    print(n_t,n_f)\n",
    "    x_input = Input(shape=(n_t,n_f))\n",
    "    x_feature = Conv1D(100,1,activation='linear')(x_input)\n",
    "    x_feature = Conv1D(100,1,activation='tanh')(x_feature)\n",
    "    # x_feature = Dropout(.2)(x_feature)\n",
    "    x_feature = LSTM(10,activation='tanh',return_sequences=False)(x_feature)\n",
    "    x_feature = Dropout(.2)(x_feature)\n",
    "    x_feature = Flatten()(x_feature)\n",
    "    x_feature = Dense(10,activation='relu')(x_feature)\n",
    "    \n",
    "    n_sf = train_static.shape[1]\n",
    "    x_input_static = Input(shape=(n_sf))\n",
    "    x_static = Dense(10,activation='relu')(x_input_static)\n",
    "    # x_static = Dense(10,activation='relu')(x_static)\n",
    "    n_timesteps = train_stress.shape[-2]\n",
    "    n_episodes_stress,n_episodes_quit,n_episodes_activity,n_episodes_smoking = train_stress.shape[1],train_quit.shape[1],train_activity.shape[1],train_smoking.shape[1]\n",
    "    \n",
    "    x_alpha_stress = Dense(1,activation='sigmoid',name='alpha_stress')(x_static)\n",
    "    x_alpha_stress = RepeatVector(n_timesteps)(x_alpha_stress)\n",
    "    x_alpha_stress = Lambda(lambda x: repeat_elements(expand_dims(x, axis=1), n_episodes_stress, 1))(x_alpha_stress)\n",
    "    \n",
    "    x_alpha_quit = Dense(1,activation='sigmoid',name='alpha_quit')(x_static)\n",
    "    x_alpha_quit = RepeatVector(n_timesteps)(x_alpha_quit)\n",
    "    x_alpha_quit = Lambda(lambda x: repeat_elements(expand_dims(x, axis=1), n_episodes_quit, 1))(x_alpha_quit)\n",
    "    \n",
    "    x_alpha_activity = Dense(1,activation='sigmoid',name='alpha_activity')(x_static)\n",
    "    x_alpha_activity = RepeatVector(n_timesteps)(x_alpha_activity)\n",
    "    x_alpha_activity = Lambda(lambda x: repeat_elements(expand_dims(x, axis=1), n_episodes_activity, 1))(x_alpha_activity)\n",
    "    \n",
    "    x_alpha_smoking = Dense(1,activation='sigmoid',name='alpha_smoking')(x_static)\n",
    "    x_alpha_smoking = RepeatVector(n_timesteps)(x_alpha_smoking)\n",
    "    x_alpha_smoking = Lambda(lambda x: repeat_elements(expand_dims(x, axis=1), n_episodes_smoking, 1))(x_alpha_smoking)\n",
    "\n",
    "    n_dim = 3\n",
    "    x_stress = Input(shape=(n_episodes_stress,n_timesteps,n_dim))\n",
    "    stress_alpha_time = tf.math.multiply(x_alpha_stress[:,:,:,0]*-1,x_stress[:,:,:,0])\n",
    "    stress_alpha_time_exp = tf.math.exp(stress_alpha_time)\n",
    "\n",
    "    x_stress_amplitude = x_stress[:,:,:,1]\n",
    "    stress_amplitude_coeff = Dense(1,activation='relu',name='amplitude_stress')(x_static)\n",
    "    stress_amplitude_coeff = Lambda(lambda x: expand_dims(x, axis=2))(stress_amplitude_coeff)\n",
    "    print(stress_amplitude_coeff.shape,'coeff',x_stress_amplitude.shape,'amplitude')\n",
    "    x_stress_amplitude = tf.math.multiply(x_stress_amplitude,stress_amplitude_coeff)\n",
    "    x_stress_duration = x_stress[:,:,:,2]\n",
    "    stress_duration_coeff = Dense(1,activation='sigmoid',name='duration_stress')(x_static)\n",
    "    stress_duration_coeff = Lambda(lambda x: expand_dims(x, axis=2))(stress_duration_coeff)\n",
    "    x_stress_duration = tf.math.multiply(x_stress_duration,stress_duration_coeff)\n",
    "    x_stress_all = tf.math.add(x_stress_amplitude,x_stress_duration)\n",
    "    stress_alpha_time_exp_amplitude = tf.math.multiply(stress_alpha_time_exp,x_stress_all)\n",
    "    stress_final = tf.math.reduce_sum(stress_alpha_time_exp_amplitude,axis=1)\n",
    "    stress_final = Lambda(lambda x: expand_dims(x, axis=2))(stress_final)\n",
    "    # stress_final = LSTM(10,activation='tanh',return_sequences=True)(stress_final)\n",
    "    \n",
    "    \n",
    "    \n",
    "    x_quit = Input(shape=(n_episodes_quit,n_timesteps,n_dim))\n",
    "\n",
    "    x_smoking = Input(shape=(n_episodes_smoking,n_timesteps,n_dim))\n",
    "    smoking_alpha_time = tf.math.multiply(x_alpha_smoking[:,:,:,0]*-1,x_smoking[:,:,:,0])\n",
    "    smoking_alpha_time_exp = tf.math.exp(smoking_alpha_time)\n",
    "    x_smoking_amplitude = x_smoking[:,:,:,1]\n",
    "    smoking_amplitude_coeff = Dense(1,activation='sigmoid',name='amplitude_smoking')(x_static)\n",
    "    smoking_amplitude_coeff = Lambda(lambda x: expand_dims(x, axis=2))(smoking_amplitude_coeff)\n",
    "    x_smoking_amplitude = tf.math.multiply(x_smoking_amplitude,smoking_amplitude_coeff)\n",
    "    x_smoking_duration = x_smoking[:,:,:,2]\n",
    "    smoking_duration_coeff = Dense(1,activation='sigmoid',name='duration_smoking')(x_static)\n",
    "    smoking_duration_coeff = Lambda(lambda x: expand_dims(x, axis=2))(smoking_duration_coeff)\n",
    "    x_smoking_duration = tf.math.multiply(x_smoking_duration,smoking_duration_coeff)\n",
    "    x_smoking_all = tf.math.add(x_smoking_amplitude,x_smoking_duration)\n",
    "    smoking_alpha_time_exp_amplitude = tf.math.multiply(smoking_alpha_time_exp,x_smoking_all)\n",
    "    smoking_final = tf.math.reduce_sum(smoking_alpha_time_exp_amplitude,axis=1)\n",
    "    smoking_final = Lambda(lambda x: expand_dims(x, axis=2))(smoking_final)\n",
    "    # smoking_final = LSTM(10,activation='tanh',return_sequences=True)(smoking_final)\n",
    "\n",
    "    \n",
    "    \n",
    "    x_activity = Input(shape=(n_episodes_activity,n_timesteps,n_dim))\n",
    "    activity_alpha_time = tf.math.multiply(x_alpha_activity[:,:,:,0]*-1,x_activity[:,:,:,0])\n",
    "    activity_alpha_time_exp = tf.math.exp(activity_alpha_time)\n",
    "    x_activity_amplitude = x_activity[:,:,:,1]\n",
    "    activity_amplitude_coeff = Dense(1,activation='sigmoid',name='amplitude_activity')(x_static)\n",
    "    activity_amplitude_coeff = Lambda(lambda x: expand_dims(x, axis=2))(activity_amplitude_coeff)\n",
    "    x_activity_amplitude = tf.math.multiply(x_activity_amplitude,activity_amplitude_coeff)\n",
    "    x_activity_duration = x_activity[:,:,:,2]\n",
    "    activity_duration_coeff = Dense(1,activation='sigmoid',name='duration_activity')(x_static)\n",
    "    activity_duration_coeff = Lambda(lambda x: expand_dims(x, axis=2))(activity_duration_coeff)\n",
    "    x_activity_duration = tf.math.multiply(x_activity_duration,activity_duration_coeff)\n",
    "    x_activity_all = tf.math.add(x_activity_amplitude,x_activity_duration)\n",
    "    activity_alpha_time_exp_amplitude = tf.math.multiply(activity_alpha_time_exp,x_activity_all)\n",
    "    activity_final = tf.math.reduce_sum(activity_alpha_time_exp_amplitude,axis=1)\n",
    "    activity_final = Lambda(lambda x: expand_dims(x, axis=2))(activity_final)\n",
    "    # activity_final = LSTM(10,activation='tanh',return_sequences=True)(activity_final)\n",
    "    \n",
    "    \n",
    "    x_episode = tf.concat([activity_final, stress_final, smoking_final],2)\n",
    "    x_episode = LSTM(10,activation='tanh',return_sequences=False)(x_episode)\n",
    "    x_episode = Dropout(.1)(x_episode)\n",
    "    x_episode = Flatten()(x_episode)\n",
    "    x_episode = Dense(10,activation='relu')(x_episode)\n",
    "\n",
    "    merged_all = tf.concat([x_feature,x_episode],1)\n",
    "    # print(merged.shape,'concatenated shape')\n",
    "    # merged_all = x_feature\n",
    "    \n",
    "    merged1 = Dense(10,activation='relu',name='normalize2')(merged_all)\n",
    "    merged1 = Lambda(lambda x: K.l2_normalize(x,axis=1))(merged1)\n",
    "    center_layer1 = CenterLossLayer(n_classes=2, n_features=10,name='normalize3')\n",
    "    merged1 = center_layer1(merged1)\n",
    "    \n",
    "    merged = Dense(5,activation='relu',name='norma')(merged1)\n",
    "    # merged = Dense(2,activation='relu',name='normalize1')(merged)\n",
    "    # merged = Lambda(lambda x: K.l2_normalize(x,axis=1))(merged)\n",
    "    center_layer = CenterLossLayer(n_classes=2, n_features=5,name='normalize')\n",
    "    merged = center_layer(merged)\n",
    "    # merged = Dense(10,activation='relu')(merged)\n",
    "    # \n",
    "    # output1 = Dense(2,activation='relu')(merged)\n",
    "    output = Dense(2,activation='softmax',name='softmax')(merged)\n",
    "    # output = Activation('softmax',name='softmax')(merged)\n",
    "    # output = Dense(2,activation='softmax')(merged)\n",
    "    # output = Reshape((-1,1))(output)\n",
    "#     output = Activation('softmax',name='softmax')(output)\n",
    "    model = Model(inputs=[x_input,x_input_static,x_stress,x_activity,x_smoking,x_quit], outputs=[output,merged,merged1])\n",
    "    # myloss = get_center_loss(num_classes=2,feature_dim=merged.shape[-1],alpha=.9)\n",
    "    myloss = CenterLoss(centers_layer=center_layer)\n",
    "    myloss1 = CenterLoss(centers_layer=center_layer1)\n",
    "    \n",
    "    model.compile(\n",
    "        loss={'softmax':tf.losses.SparseCategoricalCrossentropy(),'normalize':myloss,'normalize3':myloss1},\n",
    "        loss_weights = {'softmax':10,'normalize':0,'normalize3':50},\n",
    "        metrics={'softmax':['acc']},\n",
    "        optimizer='adam'\n",
    "    )\n",
    "    return model\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "def get_X_y_groups(n_lag=10):\n",
    "    data = pickle.load(open(filepath_file.format(n_lag),'rb'))\n",
    "\n",
    "    X_feature = np.concatenate(data.feature_final.values)\n",
    "    X_static =  np.concatenate(data.static_features.values)\n",
    "\n",
    "    X_stress_episode = np.concatenate(data.stress_episode.values)\n",
    "    X_quit_episode = np.concatenate(data.quit_episode.values)\n",
    "    X_activity_episode = np.concatenate(data.activity_episode.values)\n",
    "    X_smoking_episode = np.concatenate(data.smoking_episode.values)\n",
    "\n",
    "    y_time = data['time'].values\n",
    "    print(data['label'].unique())\n",
    "    # y =  OneHotEncoder().fit_transform(data['label'].values.reshape(-1,1)).todense()\n",
    "    y = data['label'].values\n",
    "    groups = data['user'].values\n",
    "    \n",
    "    return X_feature,X_static,X_stress_episode,X_quit_episode,X_activity_episode,X_smoking_episode,y_time,y,groups\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# filepath_file = './data/episode_encoded_lagged_data/episode_encoded_'+'lagged_{}_windows_standardized_phenotype_with_episode_cluster_check_2'\n",
    "\n",
    "\n",
    "for n_lag in [15]:\n",
    "    for obs in [30]:\n",
    "        for n_cluster in [4]:\n",
    "            # try:\n",
    "            if not os.path.isfile('./data/output_final_all/result_only_episode_lag_{}_obs_{}_triplet_loss_cluster_{}.p'.format(n_lag,obs,n_cluster)):\n",
    "                print(n_lag,obs,n_cluster,'done')\n",
    "                continue\n",
    "            else:\n",
    "                print(n_lag,obs,n_cluster)\n",
    "                filepath_file = './data/episode_encoded_lagged_data_with_episode/episode_encoded_'+'lagged_'+str(n_lag)+'_obs_{}'.format(obs)+'_windows_with_episode_cluster_check_{}'.format(n_cluster)\n",
    "                use_standardization = True\n",
    "                all_data = []\n",
    "                columns = ['alpha_stress','alpha_smoking','alpha_activity']\n",
    "                amplitude_duration_data = []\n",
    "                amplitude_duration_columns = ['amplitude_stress','duration_stress'] \n",
    "                data_cluster = pickle.load(open(filepath_file.format(n_lag),'rb'))\n",
    "                temp = data_cluster.groupby(['user','cluster_label']).count().index.values\n",
    "                users = np.array([a[0] for a in temp])\n",
    "                labels = np.array([a[1] for a in temp])\n",
    "                cluster_dict = {}\n",
    "                for i,a in enumerate(users):\n",
    "                    cluster_dict[a] = labels[i]\n",
    "                n_groups = len(np.unique(users))\n",
    "                # n_groups = 20\n",
    "                X_feature,X_static,X_stress_episode,X_quit_episode,X_activity_episode,X_smoking_episode,y_time,y,groups = get_X_y_groups(n_lag)\n",
    "                # y[y<0] = 0\n",
    "                X_feature = np.nan_to_num(X_feature)\n",
    "                y = np.float32(y)\n",
    "                indexes = get_train_test_indexes(groups,n_groups_split = 4,n_val_groups=5)\n",
    "                final_y_time = []\n",
    "                final_probs = []\n",
    "                final_y = []\n",
    "                final_groups = []\n",
    "                bias_dict = {}\n",
    "                val_results = {}\n",
    "                for kk,yyyy in enumerate(indexes):\n",
    "                    train_index,test_index,val_index = yyyy\n",
    "                    \n",
    "                    X_feature_train,X_feature_test = X_feature[train_index],X_feature[test_index]\n",
    "                    X_static_train,X_static_test = X_static[train_index],X_static[test_index]\n",
    "                    X_stress_episode_train,X_stress_episode_test = X_stress_episode[train_index], X_stress_episode[test_index]\n",
    "                    X_quit_episode_train,X_quit_episode_test = X_quit_episode[train_index], X_quit_episode[test_index]\n",
    "                    X_activity_episode_train,X_activity_episode_test = X_activity_episode[train_index], X_activity_episode[test_index]\n",
    "                    X_smoking_episode_train,X_smoking_episode_test = X_smoking_episode[train_index], X_smoking_episode[test_index]\n",
    "                    y_train,y_test,groups_train,groups_test,time_train,time_test = y[train_index],y[test_index],groups[train_index],groups[test_index],y_time[train_index],y_time[test_index]\n",
    "                    \n",
    "                    X_feature_val,X_static_val,X_stress_episode_val,X_quit_episode_val,\\\n",
    "                    X_activity_episode_val,X_smoking_episode_val,y_val,groups_val,time_val = X_feature[val_index],X_static[val_index],X_stress_episode[val_index],X_quit_episode[val_index],\\\n",
    "                                                                                            X_activity_episode[val_index],X_smoking_episode[val_index],y[val_index],groups[val_index],y_time[val_index]\n",
    "                    \n",
    "                    # y_train = np.array(y_train).reshape(len(y_train),-1)\n",
    "                    \n",
    "                    X_feature_train,val_feature,\\\n",
    "                    X_static_train,val_static,\\\n",
    "                    X_stress_episode_train,val_stress,\\\n",
    "                    X_smoking_episode_train,val_smoking,\\\n",
    "                    X_quit_episode_train,val_quit,\\\n",
    "                    X_activity_episode_train,val_activity, \\\n",
    "                    y_train,val_y = train_test_split(\n",
    "                                                    X_feature_train,\n",
    "                                                    X_static_train,\n",
    "                                                    X_stress_episode_train,\n",
    "                                                    X_smoking_episode_train,\n",
    "                                                    X_quit_episode_train,\n",
    "                                                    X_activity_episode_train,\n",
    "                                                    y_train,\n",
    "                                                    test_size=.05,\n",
    "                                                    stratify=y_train\n",
    "                                                    )\n",
    "                    val_feature = np.concatenate([val_feature,X_feature_val])\n",
    "                    val_static = np.concatenate([val_static,X_static_val])\n",
    "                    val_stress = np.concatenate([val_stress,X_stress_episode_val])\n",
    "                    val_activity = np.concatenate([val_activity,X_activity_episode_val])\n",
    "                    val_smoking = np.concatenate([val_smoking,X_smoking_episode_val])\n",
    "                    val_quit = np.concatenate([val_quit,X_quit_episode_val])\n",
    "                    val_y = np.array(list(val_y)+list(y_val))\n",
    "                    \n",
    "                    positive_train_index = np.where(y_train==1)[0]\n",
    "                    negative_train_index = np.where(y_train==0)[0]\n",
    "                    len_positive = len(positive_train_index)\n",
    "                    n_iters = 50\n",
    "                    test_preds = []\n",
    "                    bias_pred = []\n",
    "                    for i,n_iter in enumerate(range(n_iters)):\n",
    "                        np.random.seed(np.random.randint(109))\n",
    "                        indexes_sampled = np.array(list(positive_train_index)+list(np.random.choice(negative_train_index,len_positive,replace=False)))\n",
    "                        train_feature = X_feature_train[indexes_sampled]\n",
    "                        train_static = X_static_train[indexes_sampled]\n",
    "                        train_stress = X_stress_episode_train[indexes_sampled]\n",
    "                        train_quit = X_quit_episode_train[indexes_sampled]\n",
    "                        train_activity = X_activity_episode_train[indexes_sampled]\n",
    "                        train_smoking = X_smoking_episode_train[indexes_sampled]\n",
    "                        train_y = y_train[indexes_sampled]\n",
    "                        from keras import backend as K\n",
    "                        K.clear_session()\n",
    "                        model = get_model()\n",
    "                        # model.summary()\n",
    "\n",
    "                        n_epochs = 800\n",
    "                        n_step = 20\n",
    "                        validation_results = [0]\n",
    "                        counter_ = 0\n",
    "                        best_filepath = None\n",
    "                        for iiii in np.arange(0,n_epochs,n_step):\n",
    "                            filepath = './models/lag_'+str(n_lag)+'_iter_'+str(n_iter)+'_temp_with_episode_none_v3_new_loss{}.hdf5'.format(iiii)\n",
    "                            checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min',save_weights_only=False)\n",
    "                            es = EarlyStopping(monitor='loss', mode='min', verbose=1,patience=60)\n",
    "                            callbacks_list = [es,checkpoint]\n",
    "                            model.fit([train_feature,train_static,train_stress,train_activity,train_smoking,train_quit],train_y,\n",
    "                                                    epochs=n_step, batch_size=100,\n",
    "                                                                verbose=0,callbacks=callbacks_list,shuffle=True)\n",
    "                            if os.path.isfile(filepath):\n",
    "                                model.load_weights(filepath)\n",
    "                            val_y_pred = model.predict([val_feature,val_static,val_stress,val_activity,val_smoking,val_quit])[0][:,1]\n",
    "                            val_f1_now = f1Bias_scorer_CV(val_y_pred,val_y)[0]\n",
    "                            # print('<->'*20)\n",
    "                            from sklearn.metrics import roc_auc_score\n",
    "                            print('Validation Result now',val_f1_now,'AUC',roc_auc_score(val_y,val_y_pred),'step',iiii,'stagnant',counter_)\n",
    "                            print('<->'*20)\n",
    "                            if val_f1_now>=max(validation_results):\n",
    "                                best_filepath = filepath\n",
    "                                counter_ = 0\n",
    "                            else:\n",
    "                                counter_ += n_step\n",
    "                                if counter_>100:\n",
    "                                    break\n",
    "                            validation_results.append(val_f1_now)\n",
    "                        if os.path.isfile(best_filepath):\n",
    "                            print(best_filepath[-15:])\n",
    "                            model.load_weights(best_filepath)\n",
    "                          \n",
    "                        test_preds.append(model.predict([X_feature_test,X_static_test,X_stress_episode_test,\n",
    "                                                    X_activity_episode_test,X_smoking_episode_test,X_quit_episode_test])[0][:,1])\n",
    "                        bias_pred.append(model.predict([X_feature_val,X_static_val,X_stress_episode_val,X_activity_episode_val,\n",
    "                                                        X_smoking_episode_val,X_quit_episode_val])[0][:,1])\n",
    "                        \n",
    "                    from sklearn.preprocessing import MinMaxScaler\n",
    "                    y_test_pred = np.concatenate([a.reshape(-1,1) for a in test_preds],axis=1)\n",
    "                    y_test_check = np.int64(np.array(y_test).reshape(-1))\n",
    "                    import seaborn as sns\n",
    "                    for lll in np.arange(y_test_pred.shape[-1]):\n",
    "                        df_check = pd.DataFrame({'original':y_test_check,'predicted':y_test_pred[:,lll]})\n",
    "                        plt.figure()\n",
    "                        sns.kdeplot(data=df_check,x='predicted',hue='original')\n",
    "                        plt.show()\n",
    "                        # df_check['predicted']\n",
    "                        plt.figure()\n",
    "                        plt.hist(y_test_pred[:,lll])\n",
    "                        plt.show()\n",
    "                        distance_80th = np.percentile(y_test_pred[y_test_check==1,lll],20)\n",
    "                        negative_within_circle = y_test_pred[np.where((y_test_check==0)&(y_test_pred[:,lll]>distance_80th))[0],lll]\n",
    "                        print(negative_within_circle.shape[0]/np.where(y_test_check==0)[0].shape[0],'Negative within positive')\n",
    "                        print('-'*10)\n",
    "                    bias_pred = np.concatenate([a.reshape(-1,1) for a in bias_pred],axis=1)\n",
    "                    # print(roc_auc_score(y_test,y_test_pred))\n",
    "                    final_y_time.extend(list(time_test))\n",
    "                    final_probs.extend(list(y_test_pred))\n",
    "                    final_y.extend(list(y_test))\n",
    "                    final_groups.extend(list(groups_test))\n",
    "                    for group_b in np.unique(groups_test):\n",
    "                        bias_dict[group_b] = []\n",
    "                        for kkk in range(bias_pred.shape[1]):\n",
    "                            f1,bias = f1Bias_scorer_CV(bias_pred[:,kkk].reshape(-1),np.int64(y_val.reshape(-1)))\n",
    "                            \n",
    "                            f1_test, bias_test = f1Bias_scorer_CV(y_test_pred[:,kkk],np.int64(y_test.reshape(-1)))\n",
    "                            # plt.hist(y_test_pred[:,kkk],100)\n",
    "                            # plt.show()\n",
    "                            if kkk==0 and group_b==groups_test[0]:\n",
    "                                print(group_b,\n",
    "                                'val results',\n",
    "                                f1,bias,\n",
    "                                'index',kkk,\n",
    "                                'test results',\n",
    "                                f1_test,bias_test,\n",
    "                                'test result with val bias',\n",
    "                                f1_score(y_test,np.array(y_test_pred[:,kkk]>=bias,dtype=np.int64)))\n",
    "                            \n",
    "                            bias_dict[group_b].append(bias)\n",
    "                        val_results[group_b] = [time_val,bias_pred,y_val,groups_val]\n",
    "                    print(len(np.unique(final_groups)))\n",
    "                    # print(bias_dict)\n",
    "                final_y_time,final_probs,final_y,final_groups = np.array(final_y_time),np.array(final_probs),np.array(final_y),np.array(final_groups)\n",
    "                pickle.dump([final_y_time,final_probs,final_y,final_groups,bias_dict,val_results],open('./data/output_final_all/result_only_episode_lag_{}_obs_{}_triplet_loss_cluster___pos_to_neg_none_new_loss_v1{}{}.p'.format(n_lag,obs,n_cluster,.2),'wb'))\n",
    "            # except:\n",
    "            #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "for lll in np.arange(y_test_pred.shape[-1]):\n",
    "    df_check = pd.DataFrame({'original':y_test_check,'predicted':y_test_pred[:,lll]})\n",
    "    plt.figure()\n",
    "    # plt.hist(y_test_pred[:,lll])\n",
    "    sns.kdeplot(data=df_check,x='predicted',hue='original')\n",
    "    plt.show()\n",
    "    distance_80th = np.percentile(y_test_pred[y_test_check==1,lll],20)\n",
    "    negative_within_circle = y_test_pred[np.where((y_test_check==0)&(y_test_pred[:,lll]>=distance_80th))[0],lll]\n",
    "    print(distance_80th,negative_within_circle.shape[0]/np.where(y_test_check==0)[0].shape[0],'Negative within positive')\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check['predicted']\n",
    "# .plot(kind='hist')\n",
    "# plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from tensorflow_addons.losses import metric_learning\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "num_classes = 2\n",
    "feature_dim = 20\n",
    "alpha = .2\n",
    "# centers = K.zeros([num_classes, feature_dim])\n",
    "centers = tf.Variable(\n",
    "            tf.zeros([num_classes, feature_dim]),\n",
    "            name=\"centers\",\n",
    "            trainable=False,\n",
    "            # in a distributed strategy, we want updates to this variable to be summed.\n",
    "            aggregation=tf.VariableAggregation.SUM,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "features = tf.cast(tf.convert_to_tensor(np.random.randn(10,feature_dim)),tf.float32)\n",
    "labels = tf.convert_to_tensor(np.array([0]*5+[1]*5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_for_equal = tf.math.equal(tf.cast(labels,tf.bool),tf.transpose(tf.cast(labels,tf.bool)))\n",
    "        only_positive = tf.repeat(tf.transpose(tf.reshape(tf.cast(labels,tf.bool),[lshape[0],1])),lshape[0],axis=0)\n",
    "        mask_only_positive = tf.math.multiply(tf.cast(mask_for_equal,tf.float32),tf.cast(only_positive,tf.float32))\n",
    "        \n",
    "        only_negative = tf.cast(tf.repeat(tf.transpose(tf.reshape(tf.cast(1-labels,tf.bool),[10,1])),10,axis=0),tf.int32)\n",
    "\n",
    "        mask_negative_to_positive = tf.math.logical_not(tf.cast(tf.math.add(tf.cast(mask_for_equal,tf.int32),only_negative),tf.bool))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow_addons.losses import metric_learning\n",
    "import numpy as np\n",
    "feature_dim = 20\n",
    "\n",
    "precise_embeddings = tf.cast(tf.convert_to_tensor(np.random.randn(10,feature_dim)),tf.float32)\n",
    "labels = tf.convert_to_tensor(np.array([0]*3+[1]*6+[0]))\n",
    "\n",
    "lshape = tf.shape(labels)\n",
    "labels = tf.reshape(labels, [lshape[0], 1])\n",
    "mask_for_equal = tf.math.equal(tf.cast(labels,tf.bool),tf.transpose(tf.cast(labels,tf.bool)))\n",
    "only_positive = tf.repeat(tf.transpose(tf.reshape(tf.cast(labels,tf.bool),[lshape[0],1])),lshape[0],axis=0)\n",
    "mask_only_positive = tf.math.multiply(tf.cast(mask_for_equal,tf.float32),tf.cast(only_positive,tf.float32))\n",
    "\n",
    "only_negative = tf.cast(tf.repeat(tf.transpose(tf.reshape(tf.cast(1-labels,tf.bool),[10,1])),10,axis=0),tf.int32)\n",
    "\n",
    "mask_negative_to_positive = tf.math.logical_not(tf.cast(tf.math.add(tf.cast(mask_for_equal,tf.int32),only_negative),tf.bool))\n",
    "pdist_matrix = metric_learning.pairwise_distance(\n",
    "            precise_embeddings, squared=False\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mask_negative_to_positive = tf.math.logical_and(only_positive,tf.math.logical_not(tf.cast(mask_only_positive,tf.bool)))\n",
    "\n",
    "pdist_matrix = metric_learning.pairwise_distance(\n",
    "            precise_embeddings, squared=False\n",
    "        )\n",
    "# positive_only_dist = tf.math.multiply(pdist_matrix,mask_only_positive)\n",
    "# consistency = tf.reduce_sum(positive_only_dist)/tf.reduce_sum(mask_only_positive)\n",
    "\n",
    "positive_only_dist = tf.boolean_mask(pdist_matrix, tf.cast(mask_only_positive,tf.bool))\n",
    "\n",
    "# distance_95 = tf.reshape(positive_only_dist,(-1,))\n",
    "# samples1 = tf.cast(tf.shape(labels)[0], tf.float32) #batch size\n",
    "# p = 80\n",
    "# p =  2*(100. - p)/100.  #100% will return 0 elements, 0% will return all elements\n",
    "# samples = tf.cast(tf.math.floor(p * samples1), tf.int32)\n",
    "# values, indices = tf.math.top_k(distance_95, samples)\n",
    "# positive_dist_95th = tf.reduce_min(values)\n",
    "\n",
    "# sparse_labels = tf.reshape(labels, (-1,))\n",
    "# centers_batch = tf.gather(centers, tf.cast(sparse_labels,tf.int32))\n",
    "# diff = (1 - .9) * (centers_batch - precise_embeddings)\n",
    "# updates = tf.scatter_nd(tf.cast(sparse_labels[:, None],tf.int32), diff, centers.shape)\n",
    "# centers.assign_add(updates)\n",
    "# centers_batch = tf.gather(centers, tf.ones(sparse_labels.shape,dtype=tf.int32))\n",
    "# distance_to_positive_center = tf.norm(precise_embeddings-centers_batch,2,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_only_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.cast(mask_for_equal,tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_to_positive_center_smaller_than_95 = tf.where(distance_to_positive_center<positive_dist_95th,1,0)\n",
    "negative_only = tf.math.logical_not(tf.cast(sparse_labels,tf.bool))\n",
    "negative_within_circle = tf.reduce_sum(tf.boolean_mask(distance_to_positive_center_smaller_than_95,negative_only))\n",
    "total_negative = tf.shape(sparse_labels)[0] - tf.reduce_sum(sparse_labels)\n",
    "rare_loss = negative_within_circle/(total_negative+K.epsilon())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.norm(distance_to_positive_center,2,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_labels = tf.reshape(labels, (-1,))\n",
    "centers_batch = tf.gather(centers, tf.cast(sparse_labels,tf.int32))\n",
    "diff = (1 - .9) * (centers_batch - precise_embeddings)\n",
    "updates = tf.scatter_nd(tf.cast(sparse_labels[:, None],tf.int32), diff, centers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers.assign_add(updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.math.multiply(pdist_matrix,mask_only_positive)\n",
    "# tf.reduce_sum(mask_only_positive)\n",
    "consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist_matrix = metric_learning.pairwise_distance(\n",
    "            precise_embeddings, squared=False\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency = tf.math.equal(labels, tf.transpose(labels))\n",
    "adjacency_not = tf.math.logical_not(adjacency)\n",
    "batch_size = tf.size(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.math.multiply(adjacency,tf.cast(labels,tf.bool))\n",
    "mask_for_equal = tf.math.equal(tf.cast(labels,tf.bool),tf.transpose(tf.cast(labels,tf.bool)))\n",
    "only_positive = tf.repeat(tf.transpose(tf.reshape(tf.cast(labels,tf.bool),[10,1])),10,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.math.multiply(tf.reshape(tf.cast(labels,tf.bool),[10,1]),mask_for_positive)\n",
    "tf.math.multiply(tf.cast(mask_for_equal,tf.int32),tf.cast(only_positive,tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist_matrix_tile = tf.tile(pdist_matrix, [batch_size, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist_matrix_tile = tf.tile(pdist_matrix, [batch_size, 1])\n",
    "mask = tf.math.logical_and(\n",
    "    tf.tile(adjacency_not, [batch_size, 1]),\n",
    "    tf.math.greater(\n",
    "        pdist_matrix_tile, tf.reshape(tf.transpose(pdist_matrix), [-1, 1])\n",
    "    ),\n",
    ")\n",
    "mask_final = tf.reshape(\n",
    "    tf.math.greater(\n",
    "        tf.math.reduce_sum(\n",
    "            tf.cast(mask, dtype=tf.dtypes.float32), 1, keepdims=True\n",
    "        ),\n",
    "        0.0,\n",
    "    ),\n",
    "    [batch_size, batch_size],\n",
    ")\n",
    "mask_final = tf.transpose(mask_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51b244ab9aca612e739a0539ae1af887c58db9e180d786deb0ab1761def69c1f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('test1': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
